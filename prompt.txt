Directory Structure:

‚îî‚îÄ‚îÄ ./
    ‚îú‚îÄ‚îÄ .env.txt
    ‚îú‚îÄ‚îÄ ai_client_manager.py
    ‚îú‚îÄ‚îÄ ai_trend_researcher.py
    ‚îú‚îÄ‚îÄ config_manager.py
    ‚îú‚îÄ‚îÄ data_processor.py
    ‚îú‚îÄ‚îÄ keyword_manager.py
    ‚îú‚îÄ‚îÄ mcp_client_manager.py
    ‚îú‚îÄ‚îÄ platform_handlers.py
    ‚îú‚îÄ‚îÄ README.md
    ‚îú‚îÄ‚îÄ report_generator.py
    ‚îú‚îÄ‚îÄ requirement.txt
    ‚îú‚îÄ‚îÄ requirements.txt
    ‚îú‚îÄ‚îÄ research_assistant_con_hackers_LLM.py
    ‚îî‚îÄ‚îÄ research_assistant.py



---
File: /.env.txt
---

# --- Claves de API para los servicios de datos externos ---
# Cada una de estas claves es necesaria para que el script pueda autenticarse y solicitar datos de estas plataformas.

# Clave de API para acceder a los datos de YouTube (b√∫squedas, detalles de videos, etc.).
YOUTUBE_API_KEY=your_youtube_api_key_here

# Token de Acceso Personal de GitHub para realizar b√∫squedas en repositorios y c√≥digo.
GITHUB_PERSONAL_ACCESS_TOKEN=your_github_api_key_here

# Clave de API para interactuar con Notion (crear y actualizar p√°ginas).
NOTION_API_KEY=your_notion_api_key_here

# Clave de API para SiliconFlow, que podr√≠a ser usada por el servidor de ArXiv para procesar PDFs.
SILICONFLOW_API_KEY=your_siliconflow_api_key_here

# Token de Acceso para Supabase, para guardar los resultados de la investigaci√≥n en la base de datos.
SUPABASE_ACCESS_TOKEN=your_superbase_api_key_here


# --- Configuraci√≥n Espec√≠fica de Notion ---

# El ID de la p√°gina principal en Notion bajo la cual se crear√°n todas las p√°ginas de informes nuevas.
NOTION_PARENT_PAGE_ID='your_notion_parent_page_id'


# --- Configuraci√≥n de Rutas del Sistema ---
# Define rutas importantes para que la aplicaci√≥n sea portable entre diferentes sistemas.

# Ruta al directorio donde el servidor 'research_hub' descargar√° los papers.
RESEARCH_PAPERS_DIR="/path/to/your/research-papers"

# Ruta completa al binario ejecutable del servidor 'research_hub'.
RESEARCH_HUB_EXECUTABLE="/path/to/your/rust-research-mcp"


# --- Configuraci√≥n del Proveedor de IA ---

# Define qu√© servicio de IA (LLM) se usar√° para tareas como la extracci√≥n de palabras clave o traducciones.
# Opciones v√°lidas: "anthropic", "gemini", "groq", "ollama", "openai".
AI_PROVIDER="openai" 

# Almacena las claves de API para cada uno de los proveedores de IA soportados.
# El script cargar√° la clave correspondiente al proveedor seleccionado en AI_PROVIDER.
ANTHROPIC_API_KEY="tu_clave_de_anthropic"
GOOGLE_API_KEY="tu_clave_de_gemini"
GROQ_API_KEY="tu_clave_de_groq"
OPENAI_API_KEY="tu_clave_de_openai"

# --- Configuraci√≥n Opcional de Modelos de IA ---
# Permite especificar qu√© modelo exacto usar para cada proveedor. Si no se define, se usar√° un modelo por defecto.

# Modelo espec√≠fico para Google Gemini (ej. gemini-1.5-flash).
AI_MODEL_GEMINI="gemini-1.5-flash"
# Modelo espec√≠fico para Groq (ej. llama3-70b-8192).
AI_MODEL_GROQ="llama3-70b-8192"
# Modelo para Ollama, que debe corresponder a un modelo que tengas instalado localmente (ej. llama3).
AI_MODEL_OLLAMA="llama3"
# Modelo espec√≠fico para Anthropic Claude (ej. claude-3-haiku).
AI_MODEL_ANTHROPIC="claude-3-haiku-20240307"
# Modelo espec√≠fico para OpenAI (ej. gpt-4o).
AI_MODEL_OPENAI="gpt-4o"



---
File: /ai_client_manager.py
---

# ai_client_manager.py

import asyncio
# Importa las bibliotecas cliente de cada proveedor de IA soportado.
import google.generativeai as genai  # Para Google Gemini
from anthropic import Anthropic       # Para Anthropic Claude
from groq import Groq                 # Para Groq
import ollama                         # Para Ollama (modelos locales)
from openai import OpenAI             # Para OpenAI
from typing import Callable, Any

class AIClientManager:
    """
    Gestiona la inicializaci√≥n e interacci√≥n con diferentes clientes de IA.
    Act√∫a como una "f√°brica" que crea el cliente correcto seg√∫n la configuraci√≥n
    y proporciona un m√©todo unificado 'chat_completion' para interactuar con √©l de forma no bloqueante.
    """
    def __init__(self, provider: str, api_key: str = None, model: str = None):
        """
        Constructor. Inicializa el cliente de IA basado en el proveedor especificado.
        :param provider: El nombre del proveedor (ej. "openai", "gemini").
        :param api_key: La clave de API para el proveedor.
        :param model: El nombre del modelo espec√≠fico a usar (opcional).
        """
        self.provider = provider.lower()
        self.model = model
        self.client: Any = None
        print(f"Initializing AI client for provider: {self.provider}")

        # L√≥gica condicional para inicializar el cliente correcto.
        if self.provider == 'gemini':
            if not api_key: raise ValueError("Google API Key is required for Gemini provider.")
            genai.configure(api_key=api_key)
            self.client = genai.GenerativeModel(self.model or 'gemini-pro')
        elif self.provider == 'groq':
            if not api_key: raise ValueError("Groq API Key is required for Groq provider.")
            self.client = Groq(api_key=api_key)
        elif self.provider == 'ollama':
            # Para Ollama, el cliente es el propio m√≥dulo de la biblioteca.
            self.client = ollama
        elif self.provider == 'anthropic':
            if not api_key: raise ValueError("Anthropic API Key is required for Anthropic provider.")
            self.client = Anthropic(api_key=api_key)
        elif self.provider == 'openai':
            if not api_key: raise ValueError("OpenAI API Key is required for OpenAI provider.")
            self.client = OpenAI(api_key=api_key)
        else:
            raise ValueError(f"Unsupported AI provider: {self.provider}")

    async def chat_completion(self, prompt: str, max_tokens: int = 1024) -> str:
        """
        Env√≠a un prompt al modelo de IA y devuelve la respuesta de texto.
        Este m√©todo abstrae las diferencias en las llamadas a la API y las ejecuta en un
        hilo separado para no bloquear el bucle de eventos de asyncio.
        """
        try:
            # Selecciona la funci√≥n de llamada a la API correcta basada en el proveedor.
            api_call_function = self._get_api_call_function(prompt, max_tokens)
            
            # Ejecuta la llamada s√≠ncrona de la biblioteca en un hilo separado.
            # Esto es crucial para no bloquear la aplicaci√≥n as√≠ncrona.
            loop = asyncio.get_running_loop()
            response_text = await loop.run_in_executor(
                None,  # Usa el ejecutor de hilos por defecto.
                api_call_function
            )
            return response_text

        except Exception as e:
            print(f"Error calling {self.provider} API: {e}")
            return ""

    def _get_api_call_function(self, prompt: str, max_tokens: int) -> Callable[[], str]:
        """Devuelve la funci√≥n lambda correcta para realizar la llamada a la API s√≠ncrona."""
        
        if self.provider == 'gemini':
            return lambda: self.client.generate_content(prompt).text

        elif self.provider == 'groq':
            return lambda: self.client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model=self.model or "llama3-8b-8192",
            ).choices[0].message.content

        elif self.provider == 'ollama':
            return lambda: self.client.chat(
                model=self.model or 'llama3',
                messages=[{'role': 'user', 'content': prompt}]
            )['message']['content']

        elif self.provider == 'anthropic':
            return lambda: self.client.messages.create(
                model=self.model or "claude-3-sonnet-20240229",
                max_tokens=max_tokens,
                messages=[{"role": "user", "content": prompt}]
            ).content[0].text

        elif self.provider == 'openai':
            return lambda: self.client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model=self.model or "gpt-4o",
            ).choices[0].message.content
            
        else:
            # Esto no deber√≠a ocurrir si el constructor funcion√≥, pero es una salvaguarda.
            raise NotImplementedError(f"API call function not implemented for {self.provider}")



---
File: /ai_trend_researcher.py
---

# ai_trend_researcher.py
# -*- coding: utf-8 -*-

import asyncio
import sys
import os
import signal
import argparse
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Iterable

from dotenv import load_dotenv

from keyword_manager import KeywordManager
from mcp_client_manager import MCPClientManager
from platform_handlers import PlatformHandlerFactory
from data_processor import KeywordExtractor, DataAnalyzer
from report_generator import ReportManager
from config_manager import ServerConfig, AppConfig, PlatformConfig
from ai_client_manager import AIClientManager

load_dotenv()


# ----------------------------- utilidades -----------------------------

def env_int(name: str, default: int) -> int:
    try:
        v = os.getenv(name)
        return int(v) if v is not None else default
    except ValueError:
        return default

def env_float(name: str, default: float) -> float:
    try:
        v = os.getenv(name)
        return float(v) if v is not None else default
    except ValueError:
        return default

def env_bool(name: str, default: bool) -> bool:
    v = (os.getenv(name, str(default)) or "").strip().lower()
    return v in ("1", "true", "t", "yes", "y", "on")

def now_str() -> str:
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def log(msg: str) -> None:
    print(f"[{now_str()}] {msg}", flush=True)


# --------------------------- n√∫cleo investigador ---------------------------

class AITrendResearcher:
    """
    Orquestador principal del flujo de investigaci√≥n de tendencias de IA.
    Mejora: control de concurrencia, timeouts, reintentos y CLI.
    """

    def __init__(
        self,
        platforms_filter: Optional[Iterable[str]] = None,
        exclude_platforms: Optional[Iterable[str]] = None,
        per_task_timeout: float = 35.0,
        retries: int = 1,
        concurrency: int = 4,
        keywords_limit: Optional[int] = None,
    ):
        # Estado configuraci√≥n / validaciones
        AppConfig.print_config_status()
        missing_vars = AppConfig.validate_required_env_vars()
        if missing_vars:
            log("‚úó Faltan variables de entorno requeridas:")
            for var in missing_vars:
                log(f"  - {var}")
            log("Por favor, compl√©talas en tu .env")
            # No abortamos aqu√≠: el sistema puede operar con subset (ej. sin Notion/Supabase)
            # pero si faltan claves cr√≠ticas de proveedor de IA, AIClientManager fallar√°.

        # Cliente de IA
        ai_provider = AppConfig.get_ai_provider()
        api_key = AppConfig.get_api_key(ai_provider)
        ai_model = AppConfig.get_ai_model(ai_provider)
        self.ai_client_manager = AIClientManager(provider=ai_provider, api_key=api_key, model=ai_model)

        # Palabras clave
        self.keyword_manager = KeywordManager()

        # Servidores MCP / plataformas
        server_configs = ServerConfig.get_server_configs()
        supported = set(PlatformConfig.get_supported_platforms())
        wanted = set(p.strip() for p in (platforms_filter or supported))
        excluded = set(p.strip() for p in (exclude_platforms or []))
        self.platforms: List[str] = [p for p in wanted if p in supported and p not in excluded]

        # Gestores
        self.mcp_manager = MCPClientManager(server_configs)
        self.keyword_extractor = KeywordExtractor(self.ai_client_manager)
        self.data_analyzer = DataAnalyzer(self.ai_client_manager)
        self.report_manager: Optional[ReportManager] = None

        # Par√°metros ejecuci√≥n
        self.per_task_timeout = float(per_task_timeout)
        self.retries = int(max(0, retries))
        self.semaphore = asyncio.Semaphore(int(max(1, concurrency)))
        self.keywords_limit = int(keywords_limit) if keywords_limit else None

    async def run_daily_research(self) -> str:
        log(f"üöÄ Starting AI trend research - {now_str()}")

        try:
            # 1) Conectar MCP
            await self.mcp_manager.connect_all_servers()

            # 2) Plataformas activas realmente disponibles
            active_platforms = [p for p in self.platforms if self.mcp_manager.is_platform_available(p)]
            if not active_platforms:
                log("‚ö†Ô∏è No hay servidores MCP activos; nada que hacer.")
                return ""

            log(f"üåê Servidores MCP activos: {active_platforms}")

            # 3) Clientes opcionales para reportes
            notion_client = self.mcp_manager.get_client("notion")
            supabase_client = self.mcp_manager.get_client("supabase")
            notion_parent_id = AppConfig.get_notion_parent_page_id()

            # 4) Gestor de reportes
            self.report_manager = ReportManager(
                reports_dir=AppConfig.get_reports_directory(),
                notion_client=notion_client,
                notion_parent_id=notion_parent_id,
                supabase_client=supabase_client,
            )

            # 5) Cargar keywords activas
            active_keywords = self._load_active_keywords()
            if self.keywords_limit is not None:
                active_keywords = active_keywords[: self.keywords_limit]
            log(f"üîë Investigando {len(active_keywords)} keywords en {len(active_platforms)} plataformas: {active_keywords}")

            # 6) Investigaci√≥n concurrente con




---
File: /config_manager.py
---

# config_manager.py
# -*- coding: utf-8 -*-

# Importa el m√≥dulo 'os' para interactuar con el sistema operativo, principalmente para leer variables de entorno.
import os
# Importa herramientas de 'typing' para a√±adir anotaciones de tipo, mejorando la legibilidad y robustez del c√≥digo.
from typing import Dict, List, Any


class ServerConfig:
    """
    Gestiona las configuraciones de los servidores MCP (Model Context Protocol).
    Define c√≥mo iniciar y conectar con los diferentes servicios externos (YouTube, GitHub, etc.).
    """

    @staticmethod
    def _clean_env(env: Dict[str, Any]) -> Dict[str, str]:
        """
        M√©todo privado para limpiar el diccionario de entorno.
        Elimina claves con valores None o vac√≠os ("") y convierte todos los valores a string.
        Esto es necesario porque algunas bibliotecas (como Pydantic, usada por MCP) no aceptan None en variables de entorno.
        """
        # Si el diccionario de entrada est√° vac√≠o, devuelve uno vac√≠o.
        if not env:
            return {}
        # Devuelve un nuevo diccionario que solo incluye los √≠tems v√°lidos y con valores casteados a string.
        return {k: str(v) for k, v in env.items() if v not in (None, "")}

    @staticmethod
    def get_server_configs() -> Dict[str, Dict[str, Any]]:
        """
        Devuelve un diccionario que contiene la configuraci√≥n detallada para cada servidor MCP.
        Los servidores que requieren credenciales (API keys) se marcan con enabled=False si la clave no est√° presente.
        """
        # Lee todas las claves de API y tokens de las variables de entorno.
        youtube_key   = os.getenv("YOUTUBE_API_KEY")
        gh_token      = os.getenv("GITHUB_PERSONAL_ACCESS_TOKEN")
        notion_key    = os.getenv("NOTION_API_KEY")
        notion_parent = os.getenv("NOTION_PARENT_PAGE_ID")
        supa_token    = os.getenv("SUPABASE_ACCESS_TOKEN")
        silicon_key   = os.getenv("SILICONFLOW_API_KEY")

        # Lee las rutas configurables desde el entorno para mayor portabilidad.
        download_dir = os.getenv("RESEARCH_PAPERS_DIR", "research-papers")
        research_hub_executable = os.getenv("RESEARCH_HUB_EXECUTABLE", "rust-research-mcp")

        # Construye din√°micamente la lista de argumentos para el servidor de Notion.
        notion_args = ["@ramidecodes/mcp-server-notion@latest", "-y"]
        # A√±ade la clave de API a los argumentos solo si existe, para no exponer un argumento vac√≠o.
        if notion_key:
            notion_args.append(f"--api-key={notion_key}")

        # Construye din√°micamente la lista de argumentos para el servidor de Supabase.
        supabase_args = ["-y", "@supabase/mcp-server-supabase@latest"]
        # A√±ade el token de acceso a los argumentos solo si existe.
        if supa_token:
            supabase_args += ["--access-token", supa_token]

        # Define el diccionario principal de configuraciones.
        configs: Dict[str, Dict[str, Any]] = {
            "youtube": {
                "server_name": "npx",  # Comando para ejecutar el servidor (a trav√©s de npx).
                "args": ["-y", "youtube-data-mcp-server"],  # Argumentos para el comando.
                "env": ServerConfig._clean_env({  # Variables de entorno espec√≠ficas para este servidor.
                    "YOUTUBE_API_KEY": youtube_key,
                    "YOUTUBE_TRANSCRIPT_LANG": "ja",  # Configura el idioma de las transcripciones a japon√©s.
                }),
                "tools": ["searchVideos", "getVideoDetails", "getTranscripts"],  # Herramientas que expone el servidor.
                "required_env": ["YOUTUBE_API_KEY"],  # Variables de entorno obligatorias.
                "enabled": True if youtube_key else False,  # Se activa solo si la clave de API est√° presente.
            },
            "github": {
                "server_name": "npx",
                "args": ["-y", "@modelcontextprotocol/server-github"],
                "env": ServerConfig._clean_env({
                    "GITHUB_PERSONAL_ACCESS_TOKEN": gh_token,
                }),
                "tools": ["search_code", "search_repositories", "get_repository"],
                "required_env": ["GITHUB_PERSONAL_ACCESS_TOKEN"],
                "enabled": True if gh_token else False, # Se activa solo si el token de GitHub est√° presente.
            },
            "web": {
                "server_name": "one-search-mcp",  # Este servidor se ejecuta directamente, sin 'npx'.
                "args": [],  # No necesita argumentos adicionales.
                "env": {  # Variables de entorno para estandarizar y silenciar la salida de la consola.
                    "DOTENVX_SILENT": "1",
                    "FORCE_COLOR": "0",
                    "NO_COLOR": "1"
                },
                "tools": ["one_search", "one_extract", "one_scrape"],
                "enabled": True,  # Este servidor siempre est√° habilitado ya que no requiere claves.
            },
            "notion": {
                "server_name": "npx",
                "args": ["-y", *notion_args],  # Usa los argumentos construidos din√°micamente.
                "env": ServerConfig._clean_env({}), # No necesita variables de entorno adicionales.
                "tools": ["create-page", "get-page", "update-page", "query-database", "search"],
                "required_env": ["NOTION_API_KEY"],
                "enabled": True if notion_key else False, # Se activa solo si la clave de Notion est√° presente.
            },
            "arxiv": {
                "server_name": "npx",
                "args": ["-y", "@langgpt/arxiv-mcp-server@latest"],
                "env": {
                    "SILICONFLOW_API_KEY": silicon_key,
                    "WORK_DIR": "./reports",  # Directorio de trabajo para descargar PDFs.
                    "FORCE_COLOR": "0",
                    "NO_COLOR": "1",
                    "DOTENVX_SILENT": "1"
                },
                "tools": [
                    "search_arxiv", "download_arxiv_pdf", "parse_pdf_to_text",
                    "convert_to_wechat_article", "parse_pdf_to_markdown",
                    "process_arxiv_paper", "clear_workdir"
                ],
                "enabled": bool(silicon_key), # Se activa solo si la clave de SiliconFlow est√° presente.
            },
            "hackernews": {
                "server_name": "npx",
                "args": ["-y", "@microagents/server-hackernews"],
                "env": ServerConfig._clean_env({}),
                "tools": ["getStories", "getStory", "getStoryWithComments"],
                "required_env": [],  # No requiere variables de entorno.
                "enabled": True,  # Siempre habilitado.
            },
            "supabase": {
                "server_name": "npx",
                "args": supabase_args,  # Usa los argumentos construidos din√°micamente.
                "env": ServerConfig._clean_env({}),
                "tools": ["execute_sql"],
                "required_env": ["SUPABASE_ACCESS_TOKEN"],
                "enabled": True if supa_token else False, # Se activa solo si el token de Supabase est√° presente.
            },
            "research_hub": {
                "server_name": research_hub_executable,
                "args": [
                    "--download-dir", download_dir,
                    "--log-level", "info"
                ],
                "env": {
                    "RUST_LOG": "info",
                    "DOTENVX_SILENT": "1",
                    "FORCE_COLOR": "0",
                    "NO_COLOR": "1"
                },
                "tools": [
                    "search_papers", 
                    "download_paper", 
                    "extract_metadata",
                    "search_code", 
                    "generate_bibliography"
                ],
                "required_env": ["RESEARCH_HUB_EXECUTABLE", "RESEARCH_PAPERS_DIR"], 
                "enabled": os.path.exists(research_hub_executable), # Se activa si el binario existe.
            },
        }
        # Devuelve el diccionario completo de configuraciones.
        return configs

    @staticmethod
    def get_enabled_platforms() -> List[str]:
        """Devuelve una lista con los nombres de las plataformas que est√°n actualmente habilitadas."""
        configs = ServerConfig.get_server_configs()
        # Crea una lista de plataformas donde el valor de 'enabled' es True.
        return [platform for platform, config in configs.items() if config.get("enabled", False)]


class AppConfig:
    """Clase para gestionar la configuraci√≥n general de la aplicaci√≥n (proveedor de IA, modelos, etc.)."""

    @staticmethod
    def get_ai_provider() -> str:
        """Obtiene el proveedor de IA configurado en .env, con 'openai' como valor por defecto."""
        return os.getenv("AI_PROVIDER", "openai").lower()

    @staticmethod
    def get_api_key(provider: str) -> str:
        """Obtiene la clave de API para un proveedor de IA espec√≠fico."""
        # Mapea el nombre del proveedor a su variable de entorno correspondiente.
        provider_key_map = {
            "anthropic": "ANTHROPIC_API_KEY",
            "gemini": "GOOGLE_API_KEY",
            "groq": "GROQ_API_KEY",
            "openai": "OPENAI_API_KEY",
            # 'ollama' se ejecuta localmente y no requiere clave.
        }
        # Obtiene el nombre de la variable de entorno del mapa.
        env_var_name = provider_key_map.get(provider)
        # Devuelve el valor de la variable de entorno si existe, si no, None.
        return os.getenv(env_var_name) if env_var_name else None

    @staticmethod
    def get_ai_model(provider: str) -> str:
        """Obtiene el nombre del modelo de IA espec√≠fico para un proveedor, si est√° configurado."""
        # Construye el nombre de la variable de entorno (ej. "AI_MODEL_OPENAI").
        env_var_name = f"AI_MODEL_{provider.upper()}"
        # Devuelve el valor de la variable de entorno.
        return os.getenv(env_var_name)

    @staticmethod
    def get_notion_parent_page_id() -> str:
        """Obtiene el ID de la p√°gina padre de Notion desde las variables de entorno."""
        return os.getenv("NOTION_PARENT_PAGE_ID", "")

    @staticmethod
    def get_reports_directory() -> str:
        """Devuelve el nombre del directorio donde se guardan los informes locales."""
        return "reports"

    @staticmethod
    def validate_required_env_vars() -> List[str]:
        """
        Valida que todas las variables de entorno necesarias est√©n definidas.
        Devuelve una lista con las variables que faltan.
        """
        # Define un diccionario de variables requeridas y su descripci√≥n.
        required_vars = {
            "YOUTUBE_API_KEY": "YouTube API key",
            "GITHUB_PERSONAL_ACCESS_TOKEN": "GitHub access token",
            "NOTION_API_KEY": "Notion API key",
            "NOTION_PARENT_PAGE_ID": "Notion parent page ID",
            "SUPABASE_ACCESS_TOKEN": "Supabase access token",
            "RESEARCH_PAPERS_DIR": "Research papers download directory",
            "RESEARCH_HUB_EXECUTABLE": "Path to the Research Hub executable"
        }
        # A√±ade la clave de API del proveedor de IA seleccionado a la lista de requeridos.
        ai_provider = AppConfig.get_ai_provider()
        api_key_env_var = {
            "gemini": "GOOGLE_API_KEY",
            "groq": "GROQ_API_KEY",
            "anthropic": "ANTHROPIC_API_KEY",
            "openai": "OPENAI_API_KEY",
        }.get(ai_provider)
        if api_key_env_var:
            required_vars[api_key_env_var] = f"API Key for {ai_provider.capitalize()}"

        # Crea una lista de las variables que no est√°n definidas.
        missing = [f"{var} ({desc})" for var, desc in required_vars.items() if not os.getenv(var)]
        return missing

    @staticmethod
    def print_config_status():
        """Imprime en la consola un resumen del estado de la configuraci√≥n actual."""
        print("=== Configuration Status ===")
        ai_provider = AppConfig.get_ai_provider()
        print(f"‚úì AI Provider configured: {ai_provider.upper()}")

        api_key = AppConfig.get_api_key(ai_provider)
        if ai_provider not in ["ollama"]: # Ollama no necesita clave.
            print(f"‚úì {ai_provider.capitalize()} API key loaded" if api_key else f"‚úó {ai_provider.capitalize()} API key not found")

        ai_model = AppConfig.get_ai_model(ai_provider)
        print(f"‚úì Using specific model for {ai_provider}: {ai_model}" if ai_model else f"‚úì Using default model for {ai_provider}")
        print("---")

        # Comprueba el estado de otras claves de API importantes.
        other_vars = [
            ("YOUTUBE_API_KEY", "YouTube API key"),
            ("GITHUB_PERSONAL_ACCESS_TOKEN", "GitHub access token"),
            ("NOTION_API_KEY", "Notion API key"),
            ("NOTION_PARENT_PAGE_ID", "Notion parent page ID"),
            ("SUPABASE_ACCESS_TOKEN", "Supabase access token"),
            ("RESEARCH_PAPERS_DIR", "Research papers directory"),
            ("RESEARCH_HUB_EXECUTABLE", "Research Hub executable"),
        ]
        for var, description in other_vars:
            # Imprime un tick (‚úì) si la variable est√° cargada, o una cruz (‚úó) si no.
            print(f"‚úì {description} loaded" if os.getenv(var) else f"‚úó {description} not found")
        
        if os.getenv("RESEARCH_HUB_EXECUTABLE") and not os.path.exists(os.getenv("RESEARCH_HUB_EXECUTABLE")):
            print(f"‚úó WARNING: Research Hub executable not found at specified path.")

        print("============================")


class PlatformConfig:
    """Define las plataformas que la aplicaci√≥n soporta para la investigaci√≥n."""
    # Lista fija de plataformas soportadas en el c√≥digo.
    SUPPORTED_PLATFORMS = ["web", "youtube", "github", "arxiv", "hackernews", "supabase", "research_hub"]

    @staticmethod
    def get_supported_platforms() -> List[str]:
        """Devuelve una copia de la lista de plataformas soportadas."""
        return PlatformConfig.SUPPORTED_PLATFORMS.copy()

    @staticmethod
    def is_platform_supported(platform: str) -> bool:
        """Comprueba si una plataforma dada est√° en la lista de soportadas."""
        return platform in PlatformConfig.SUPPORTED_PLATFORMS



---
File: /data_processor.py
---

# data_processor.py
# -*- coding: utf-8 -*-

# Importa el m√≥dulo 'asyncio' para ejecutar tareas s√≠ncronas en un hilo.
import asyncio
# Importa el m√≥dulo 'json' para trabajar con datos en formato JSON.
import json
# Importa el m√≥dulo 're' para trabajar con expresiones regulares (b√∫squeda de patrones en texto).
import re
# De 'collections', importa 'Counter' para contar f√°cilmente la frecuencia de elementos en una lista.
from collections import Counter
# De 'datetime', importa 'datetime' para obtener la fecha y hora actuales.
from datetime import datetime
# De 'typing', importa herramientas para anotaciones de tipo.
from typing import Dict, List, Any

# Importa el gestor de clientes de IA para que el analizador pueda usar LLMs.
from ai_client_manager import AIClientManager

class KeywordExtractor:
    """
    Extrae nuevas palabras clave a partir de los datos de investigaci√≥n.
    Utiliza un LLM (Modelo Ling√º√≠stico Grande) si est√° disponible para una extracci√≥n m√°s inteligente.
    Si no, recurre a un m√©todo heur√≠stico local basado en frecuencia de palabras.
    """
    def __init__(self, ai_client_manager: AIClientManager = None):
        """
        Constructor. Recibe un gestor de cliente de IA.
        Este gestor debe tener un m√©todo `async chat_completion(prompt, max_tokens=...)`.
        """
        self.ai_client = ai_client_manager

    async def extract_keywords(self, research_data: List[Dict[str, Any]]) -> List[str]:
        """
        M√©todo principal para extraer palabras clave.
        Decide si usar el LLM o el m√©todo heur√≠stico de respaldo.
        """
        # Si no hay datos de investigaci√≥n, no hay nada que hacer.
        if not research_data:
            print("No hay datos de investigaci√≥n para la extracci√≥n de keywords.")
            return []

        # Prepara un resumen compacto del contenido para no enviar demasiada informaci√≥n al LLM.
        content_summary = self._prepare_content_for_analysis(research_data)

        # Si despu√©s de preparar el resumen no hay contenido, usa la heur√≠stica sobre los datos brutos.
        if not content_summary:
            print("No se encontr√≥ contenido utilizable. Usando heur√≠stica sobre corpus completo.")
            corpus = self._concat_corpus_from_raw(research_data)
            return self._heuristic_keywords(corpus)

        # Si hay un cliente de IA disponible, intenta usarlo.
        if self.ai_client:
            try:
                # Crea el prompt (la instrucci√≥n) para el LLM.
                prompt = self._create_extraction_prompt(content_summary)
                # Llama al LLM para obtener una respuesta.
                response = await self.ai_client.chat_completion(prompt, max_tokens=512)
                # Parsea la respuesta del LLM para extraer la lista de palabras clave.
                keywords = self._parse_keywords_from_response(response)
                provider = getattr(self.ai_client, "provider", "ai").capitalize()
                print(f"LLM ({provider}) extrajo {len(keywords)} keywords: {keywords}")
                return keywords
            except Exception as e:
                # Si el LLM falla, informa del error y pasa al m√©todo de respaldo.
                print(f"[KeywordExtractor] Fallo con LLM, usando heur√≠stica. Error: {e}")

        # Si no hay cliente de IA o si fall√≥, usa el m√©todo heur√≠stico local.
        corpus = self._concat_corpus(content_summary)
        return self._heuristic_keywords(corpus)

    # ---------- M√©todos de utilidad internos ----------

    def _prepare_content_for_analysis(self, research_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Compacta los resultados de la investigaci√≥n para crear un resumen manejable."""
        content_summary: List[Dict[str, Any]] = []
        for data in research_data:
            results = data.get("results", [])
            # Toma como m√°ximo los 3 primeros resultados de cada plataforma para ser conciso.
            for result in results[:3]:
                content_summary.append({
                    "platform": data.get("platform", ""),
                    "keyword": data.get("keyword", ""),
                    "title": result.get("title") or result.get("name") or "",
                    # Acorta la descripci√≥n a 200 caracteres.
                    "description": (result.get("description") or result.get("snippet") or result.get("abstract") or "")[:200],
                    "topics": result.get("topics", []),
                })
        return content_summary

    def _create_extraction_prompt(self, content_summary: List[Dict[str, Any]]) -> str:
        """Crea el prompt que se enviar√° al LLM, pidi√©ndole que extraiga keywords en formato JSON."""
        return (
            "Analyze this AI trend research data and extract 5-10 new trending keywords related to AI, "
            "machine learning, or technology.\n\n"
            f"Data: {json.dumps(content_summary, indent=2, ensure_ascii=False)}\n\n"
            "Instructions:\n"
            "1. Focus on AI tools, frameworks, companies, techniques, or emerging technologies\n"
            "2. Return only a JSON array of keywords, like: [\"keyword1\", \"keyword2\", \"keyword3\"]\n"
            "3. Prioritize keywords that appear frequently or have high engagement\n"
            "4. Include both English and Japanese keywords if relevant\n"
            "5. If no relevant keywords are found, return an empty array: []\n"
        )

    def _parse_keywords_from_response(self, response: str) -> List[str]:
        """Parsea la respuesta del LLM. Intenta leer un array JSON, y si falla, lo trata como texto plano."""
        if not response:
            return []
        try:
            # Busca una estructura que parezca un array JSON (empieza con [ y termina con ]).
            m = re.search(r"\[.*?\]", response, re.DOTALL)
            if m:
                # Si lo encuentra, intenta decodificarlo como JSON.
                arr = json.loads(m.group())
                # Limpia y devuelve la lista de strings.
                return [s.strip() for s in arr if isinstance(s, str) and s.strip()]
        except Exception as e:
            # Si el parseo JSON falla, lo informa.
            print(f"Error parseando JSON de keywords: {e}")

        # Si no es JSON, lo trata como texto plano separado por comas.
        parts = response.replace("[", "").replace("]", "").replace('"', "")
        kws = [p.strip() for p in parts.split(",") if p.strip()]
        # Devuelve como m√°ximo las 10 primeras.
        return kws[:10]

    def _concat_corpus(self, content_summary: List[Dict[str, Any]]) -> str:
        """Une todos los textos del resumen (t√≠tulos, descripciones, temas) en un solo bloque de texto (corpus)."""
        parts: List[str] = []
        for item in content_summary:
            parts.append(item.get("title", ""))
            parts.append(item.get("description", ""))
            topics = item.get("topics", [])
            if isinstance(topics, list) and topics:
                parts.extend([str(t) for t in topics])
        return " \n".join([p for p in parts if p])

    def _concat_corpus_from_raw(self, research_data: List[Dict[str, Any]]) -> str:
        """Similar a _concat_corpus, pero trabaja directamente con los datos brutos de investigaci√≥n."""
        parts: List[str] = []
        for d in research_data:
            for r in d.get("results", []):
                parts.append(r.get("title") or r.get("name") or "")
                parts.append(r.get("description") or r.get("snippet") or r.get("abstract") or "")
                topics = r.get("topics", [])
                if isinstance(topics, list):
                    parts.extend([str(t) for t in topics])
        return " \n".join([p for p in parts if p])

    def _heuristic_keywords(self, corpus: str) -> List[str]:
        """
        M√©todo heur√≠stico de respaldo para extraer keywords.
        Se basa en encontrar las palabras y frases (n-gramas) m√°s frecuentes.
        """
        if not corpus:
            return []

        text = corpus.lower()

        # Extrae tokens (palabras) que parecen relevantes.
        tokens = re.findall(r"[a-z0-9][a-z0-9\-_/\.]{2,}", text)
        # Define una lista de palabras comunes (stop words) para ignorar.
        stop = {
            "https", "http", "www", "com", "org", "from", "with", "that", "this", "what", "when",
            "your", "have", "about", "into", "like", "will", "there", "their", "been", "make",
            "only", "some", "more", "over", "also", "than", "which", "were", "after", "before",
            "because", "could", "should", "would"
        }
        tokens = [t for t in tokens if t not in stop]
        # Obtiene las 20 palabras m√°s comunes.
        singles = [w for w, _ in Counter(tokens).most_common(20)]

        # Busca frases de 2 palabras (bigramas) y 3 palabras (trigramas).
        words = re.findall(r"[a-z0-9]+", text)
        bigrams = [" ".join(words[i:i+2]) for i in range(len(words)-1)]
        trigrams = [" ".join(words[i:i+3]) for i in range(len(words)-2)]
        # Cuenta la frecuencia de los n-gramas m√°s relevantes.
        bf = Counter([b for b in bigrams if len(b) > 6])
        tf = Counter([t for t in trigrams if len(t) > 8])

        # Combina las palabras sueltas y los n-gramas m√°s comunes.
        candidates = singles + [w for w, _ in bf.most_common(10)] + [w for w, _ in tf.most_common(10)]
        # Normaliza y limpia la lista final.
        return self._normalize_keywords(candidates)

    def _normalize_keywords(self, kws: List[str]) -> List[str]:
        """Limpia una lista de keywords: convierte a min√∫sculas, quita espacios y duplicados."""
        out: List[str] = []
        for kw in kws:
            k = re.sub(r"\s+", " ", kw.lower()).strip()
            k = k.strip(" .,:;-/\\|\"'()[]{}")
            if len(k) >= 3:
                out.append(k)
        # Elimina duplicados manteniendo el orden.
        seen = set()
        uniq = []
        for k in out:
            if k not in seen:
                seen.add(k)
                uniq.append(k)
        return uniq


class DataAnalyzer:
    """Analiza los datos recolectados para calcular m√©tricas, puntuar keywords y generar recomendaciones."""
    
    def __init__(self, ai_client_manager: AIClientManager = None):
        """Constructor. Recibe el gestor de cliente de IA para generar recomendaciones din√°micas."""
        self.ai_client = ai_client_manager

    def score_keywords(self, new_keywords: List[str], research_data: List[Dict[str, Any]]) -> Dict[str, int]:
        """Asigna una puntuaci√≥n a cada nueva palabra clave basada en su frecuencia en los resultados de la investigaci√≥n."""
        if not new_keywords:
            return {}

        # Crea un gran bloque de texto (corpus) con todos los t√≠tulos, descripciones y temas.
        parts: List[str] = []
        for item in research_data:
            for r in item.get("results", []):
                title = r.get("title") or r.get("name") or ""
                desc = r.get("description") or r.get("snippet") or r.get("abstract") or ""
                topics = r.get("topics") or []
                parts.append(title)
                parts.append(desc)
                if isinstance(topics, list):
                    parts.extend([str(t) for t in topics])
        text = " \n".join([p for p in parts if p]).lower()

        # Cuenta cu√°ntas veces aparece cada nueva palabra clave en el corpus.
        hits_map: Dict[str, int] = {}
        max_hits = 1
        for kw in new_keywords:
            if not kw:
                continue
            hits = len(re.findall(re.escape(kw.lower()), text))
            hits_map[kw] = hits
            if hits > max_hits:
                max_hits = hits

        # Normaliza las puntuaciones en una escala de 0 a 100 usando una escala logar√≠tmica.
        import math
        scores: Dict[str, int] = {}
        for kw, h in hits_map.items():
            norm = math.log1p(h) / math.log1p(max_hits) if max_hits > 0 else 0.0
            scores[kw] = int(round(norm * 100))
        return scores

    def calculate_summary_stats(self, research_data: List[Dict[str, Any]], new_keywords: List[str]) -> Dict[str, Any]:
        """Calcula estad√≠sticas agregadas b√°sicas sobre la ejecuci√≥n de la investigaci√≥n."""
        # Cuenta cu√°ntas ejecuciones se hicieron por plataforma.
        per_platform = Counter([d.get("platform", "unknown") for d in research_data])
        # Suma el total de resultados obtenidos.
        total_results = sum(len(d.get("results", [])) for d in research_data)
        # Cuenta cu√°ntas ejecuciones tuvieron errores.
        runs_with_errors = sum(1 for d in research_data if d.get("error"))

        return {
            "timestamp": datetime.now().isoformat(),
            "platform_breakdown": dict(per_platform),
            "total_items": total_results,
            "new_keywords_count": len(new_keywords),
            "runs_with_errors": runs_with_errors,
        }

    async def generate_recommendations(self, research_data: List[Dict[str, Any]], new_keywords: List[str]) -> List[str]:
        """Genera una lista de recomendaciones de acci√≥n, usando IA si est√° disponible."""
        # Si no hay cliente de IA o no hay datos, usa el m√©todo de respaldo.
        if not self.ai_client or (not research_data and not new_keywords):
            return self._heuristic_recommendations(research_data, new_keywords)

        try:
            prompt = self._create_recommendation_prompt(research_data, new_keywords)
            response = await self.ai_client.chat_completion(prompt, max_tokens=512)
            # Parsea la respuesta en una lista de strings.
            recommendations = [rec.strip("- ").strip() for rec in response.split("\n") if rec.strip("- ").strip()]
            return recommendations if recommendations else ["No specific recommendations generated."]
        except Exception as e:
            print(f"Error generando recomendaciones con IA, usando heur√≠stica. Error: {e}")
            return self._heuristic_recommendations(research_data, new_keywords)

    def _create_recommendation_prompt(self, research_data: List[Dict[str, Any]], new_keywords: List[str]) -> str:
        """Crea el prompt para que el LLM genere recomendaciones."""
        summary_stats = self.calculate_summary_stats(research_data, new_keywords)
        
        # Prepara un resumen de los hallazgos m√°s importantes para el prompt.
        top_findings = []
        for data in research_data:
            platform = data.get("platform")
            if data.get("results"):
                top_result = data["results"][0]
                title = top_result.get("title") or top_result.get("name")
                if title:
                    top_findings.append(f"- From {platform}: Found '{title}' related to '{data.get('keyword')}'.")
        
        return (
            "You are an AI research analyst. Based on the following summary of a trend investigation, "
            "provide 3-5 actionable and insightful recommendations for a research team. "
            "Focus on what to investigate next, what technologies seem promising, and potential content ideas.\n\n"
            f"--- Data Summary ---\n"
            f"Total items found: {summary_stats['total_items']}\n"
            f"Platforms with most results: {', '.join(summary_stats['platform_breakdown'].keys())}\n"
            f"New keywords discovered: {', '.join(new_keywords)}\n"
            f"Top findings:\n{''.join(top_findings[:5])}\n"
            f"--- End of Summary ---\n\n"
            "Generate the recommendations as a bulleted list (e.g., - Recommendation 1). Do not add any introductory text."
        )

    def _heuristic_recommendations(self, research_data: List[Dict[str, Any]], new_keywords: List[str]) -> List[str]:
        """Genera una lista de recomendaciones de acci√≥n simples basadas en los resultados."""
        recs: List[str] = []
        platform_counts = Counter([d.get("platform", "unknown") for d in research_data])

        # A√±ade recomendaciones espec√≠ficas seg√∫n las plataformas que devolvieron datos.
        if platform_counts.get("github", 0) > 0:
            recs.append("Priorizar repositorios con una alta tasa de estrellas (star_rate) y actividad reciente.")
        if platform_counts.get("web", 0) > 0:
            recs.append("Revisar los resultados web en japon√©s para detectar t√©rminos emergentes locales.")
        if platform_counts.get("arxiv", 0) > 0:
            recs.append("Leer los abstracts recientes de arXiv (‚â§ 30 d√≠as) para captar nuevas l√≠neas de investigaci√≥n.")
        
        # Si no hay recomendaciones, sugiere ampliar la b√∫squeda.
        if not recs:
            recs.append("Ampliar las fuentes o las palabras clave para obtener m√°s se√±ales.")
        
        # Si se encontraron nuevas keywords, sugiere explorarlas.
        if new_keywords:
            recs.append(f"Explorar en profundidad las nuevas keywords descubiertas: {', '.join(new_keywords[:5])}...")

        return recs



---
File: /keyword_manager.py
---

import json
import os
from datetime import datetime
from typing import Dict, List, Optional, Any, TypedDict

# Define un tipo para la metadata de las keywords para mejorar la legibilidad y el autocompletado.
class KeywordMetadata(TypedDict, total=False):
    score: int
    status: str
    source: str
    created_date: str
    last_used: Optional[str]
    discovered_from: Optional[str]

MasterKeywords = Dict[str, KeywordMetadata]

class KeywordManager:
    """
    Gestiona el ciclo de vida de las palabras clave con persistencia en archivos JSON.
    Maneja tres archivos principales:
      - keywords/master.json: Un cat√°logo de todas las keywords descubiertas con sus metadatos.
      - keywords/active.json: Una lista simple de las keywords a investigar en la pr√≥xima ejecuci√≥n.
      - keywords/history.json: Un registro de las ejecuciones pasadas.
    """

    def __init__(self, keywords_dir: str = "keywords"):
        """Constructor. Define las rutas a los archivos y se asegura de que existan."""
        self.keywords_dir = keywords_dir
        self.master_file = os.path.join(keywords_dir, "master.json")
        self.active_file = os.path.join(keywords_dir, "active.json")
        self.history_file = os.path.join(keywords_dir, "history.json")

        # Asegura que el directorio 'keywords' exista.
        os.makedirs(self.keywords_dir, exist_ok=True)
        # Si los archivos JSON no existen, los crea con un contenido inicial vac√≠o.
        if not os.path.exists(self.master_file):
            self._atomic_write(self.master_file, {})
        if not os.path.exists(self.active_file):
            self._atomic_write(self.active_file, [])
        if not os.path.exists(self.history_file):
            self._atomic_write(self.history_file, {})

    # ---------------------------------
    # M√©todos para cargar/guardar JSON
    # ---------------------------------
    def load_master_keywords(self) -> MasterKeywords:
        """Carga el cat√°logo maestro de keywords desde master.json."""
        try:
            with open(self.master_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # Devuelve los datos solo si son un diccionario, para evitar errores.
                return data if isinstance(data, dict) else {}
        except (FileNotFoundError, json.JSONDecodeError):
            # Si hay alg√∫n error (archivo no encontrado, JSON mal formado), devuelve un diccionario vac√≠o.
            return {}

    def load_active_keywords(self) -> List[str]:
        """Carga la lista de keywords activas desde active.json."""
        try:
            with open(self.active_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # Devuelve los datos solo si son una lista.
                return data if isinstance(data, list) else []
        except (FileNotFoundError, json.JSONDecodeError):
            # En caso de error, devuelve una lista vac√≠a.
            return []

    def load_history(self) -> Dict[str, Any]:
        """Carga el historial de ejecuciones desde history.json."""
        try:
            with open(self.history_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # Devuelve los datos solo si son un diccionario.
                return data if isinstance(data, dict) else {}
        except (FileNotFoundError, json.JSONDecodeError):
            # En caso de error, devuelve un diccionario vac√≠o.
            return {}

    def save_master_keywords(self, keywords: MasterKeywords):
        """Guarda el cat√°logo maestro de keywords en master.json."""
        self._atomic_write(self.master_file, keywords)

    def save_active_keywords(self, keywords: List[str]):
        """Guarda la lista de keywords activas en active.json."""
        self._atomic_write(self.active_file, keywords)

    def save_history(self, history: Dict[str, Any]):
        """Guarda el historial de ejecuciones en history.json."""
        self._atomic_write(self.history_file, history)

    def _atomic_write(self, path: str, data: Any):
        """
        Realiza una escritura "at√≥mica" para evitar la corrupci√≥n de archivos.
        Primero escribe en un archivo temporal (.tmp) y, si tiene √©xito, lo renombra al archivo final.
        """
        tmp_path = path + ".tmp"
        try:
            with open(tmp_path, "w", encoding="utf-8") as f:
                # Vuelca los datos al archivo JSON con formato legible.
                json.dump(data, f, ensure_ascii=False, indent=2)
            # Reemplaza el archivo original con el nuevo archivo temporal.
            os.replace(tmp_path, path)
        except Exception as e:
            print(f"Error durante la escritura at√≥mica en {path}: {e}")
            # Si hubo un error, intenta eliminar el archivo temporal si existe.
            if os.path.exists(tmp_path):
                os.remove(tmp_path)


    # ---------------------------------
    # API p√∫blica para gestionar keywords
    # ---------------------------------
    def add_new_keyword(
        self,
        keyword: str,
        score: int,
        status: str,
        source: str,
        discovered_from: Optional[str] = None
    ) -> bool:
        """
        A√±ade una nueva palabra clave al cat√°logo maestro si no existe previamente.
        Devuelve True si la keyword fue a√±adida, False si ya exist√≠a.
        """
        # Limpia la keyword de espacios en blanco.
        keyword = (keyword or "").strip()
        if not keyword:
            return False

        master = self.load_master_keywords()
        # Si la keyword ya est√° en el cat√°logo, no hace nada.
        if keyword in master:
            return False

        # Crea la nueva entrada para la palabra clave.
        now_date = datetime.now().strftime("%Y-%m-%d")
        entry: KeywordMetadata = {
            "score": int(score),
            "status": status,
            "source": source,
            "created_date": now_date,
            "last_used": None  # A√∫n no se ha usado para investigar.
        }
        if discovered_from:
            entry["discovered_from"] = discovered_from

        # A√±ade la nueva entrada al cat√°logo y lo guarda.
        master[keyword] = entry
        self.save_master_keywords(master)
        return True

    def update_keyword_score(self, keyword: str, new_score: int) -> bool:
        """Actualiza la puntuaci√≥n de una palabra clave existente."""
        master = self.load_master_keywords()
        if keyword in master:
            master[keyword]["score"] = int(new_score)
            self.save_master_keywords(master)
            return True
        return False

    def mark_keywords_used(self, keywords: List[str]) -> None:
        """Marca una lista de palabras clave como 'usadas' en la fecha actual."""
        if not keywords:
            return

        master = self.load_master_keywords()
        today = datetime.now().strftime("%Y-%m-%d")

        changed = False
        for kw in keywords:
            kw = (kw or "").strip()
            if not kw:
                continue
            # Si la keyword no exist√≠a por alguna raz√≥n, la crea con datos por defecto.
            if kw not in master:
                master[kw] = {
                    "score": 0, "status": "unknown", "source": "runtime",
                    "created_date": today, "last_used": today
                }
                changed = True
            else:
                # Actualiza la fecha del √∫ltimo uso.
                master[kw]["last_used"] = today
                changed = True

        # Guarda los cambios solo si se realiz√≥ alguna modificaci√≥n.
        if changed:
            self.save_master_keywords(master)

    def record_execution(self, keywords: List[str], status: str = "completed", new_keywords_found: int = 0) -> None:
        """Registra un resumen de la ejecuci√≥n actual en el archivo de historial."""
        history = self.load_history()
        today = datetime.now().strftime("%Y-%m-%d")
        # Crea o sobrescribe la entrada para el d√≠a de hoy.
        history[today] = {
            "keywords_used": keywords,
            "execution_time": datetime.now().strftime("%H:%M:%S"),
            "status": status,
            "new_keywords_found": int(new_keywords_found)
        }
        self.save_history(history)

    def get_top_keywords(self, limit: int = 10) -> List[str]:
        """Devuelve una lista de las N mejores keywords seg√∫n su puntuaci√≥n y fecha de √∫ltimo uso."""
        master = self.load_master_keywords()
        
        # Define una funci√≥n de ordenaci√≥n compleja:
        def sort_key(item: tuple[str, KeywordMetadata]):
            kw, meta = item
            score = int(meta.get("score", 0))
            # Trata 'None' o '' como la fecha m√°s antigua para priorizar keywords nunca usadas.
            last_used = meta.get("last_used") or "1970-01-01" 
            # Ordena por puntuaci√≥n descendente (-score) y luego por fecha de √∫ltimo uso ascendente.
            return (-score, last_used)

        # Ordena los √≠tems del cat√°logo usando la clave definida.
        sorted_items = sorted(master.items(), key=sort_key)
        # Devuelve solo los nombres de las keywords del top N.
        return [kw for kw, _ in sorted_items[:limit]]

    def refresh_active_keywords(self, limit: int = 5) -> List[str]:
        """
        Selecciona las mejores keywords del cat√°logo maestro y las guarda en active.json
        para que sean usadas en la pr√≥xima ejecuci√≥n.
        """
        top_keywords = self.get_top_keywords(limit=limit)
        self.save_active_keywords(top_keywords)
        return top_keywords



---
File: /mcp_client_manager.py
---

# mcp_client_manager.py
# -*- coding: utf-8 -*-

# Importa el m√≥dulo 'asyncio' para la programaci√≥n as√≠ncrona.
import asyncio
# Importa el m√≥dulo 'os' para leer variables de entorno.
import os
# Importa herramientas de 'typing' para anotaciones de tipo.
from typing import Dict, List, Any, Optional
# De 'contextlib', importa 'AsyncExitStack' para gestionar m√∫ltiples contextos as√≠ncronos de forma segura.
from contextlib import AsyncExitStack

# Importa las clases necesarias de la biblioteca 'mcp'.
from mcp import ClientSession
from mcp.client.stdio import stdio_client, StdioServerParameters


class RemoteMCPClient:
    """
    Representa un cliente para un √∫nico servidor MCP que se ejecuta como un proceso local (ej. iniciado con npx).
    Gestiona la conexi√≥n, los reintentos, las llamadas a herramientas y el cierre seguro.
    """

    def __init__(self):
        """Constructor. Inicializa el estado del cliente."""
        self.session: Optional[ClientSession] = None  # La sesi√≥n de comunicaci√≥n MCP.
        self.exit_stack: Optional[AsyncExitStack] = None  # Para gestionar recursos as√≠ncronos.
        self._connected: bool = False  # Flag para indicar si la conexi√≥n est√° activa.
        self._cleanup_attempted: bool = False  # Flag para evitar limpiezas duplicadas.
        self._available_tools: List[str] = []  # Lista de herramientas que ofrece el servidor.

    async def connect_to_server_by_name(
        self,
        server_name: str,
        args: List[str] = None,
        env: Dict[str, Any] = None
    ) -> bool:
        """
        Establece una conexi√≥n con un servidor MCP a trav√©s de su entrada/salida est√°ndar (stdio).
        Implementa una l√≥gica de reintentos y timeouts adaptables.
        """
        args = args or []
        joined_args = " ".join(args)

        # Heur√≠stica para definir timeouts de conexi√≥n m√°s largos para servidores que tardan m√°s en arrancar.
        base_timeout = 15.0
        if "one-search-mcp" in server_name or "one-search-mcp" in joined_args:
            init_timeout = 45.0
        elif "@langgpt/arxiv-mcp-server" in joined_args:
            init_timeout = 60.0
        else:
            init_timeout = base_timeout

        # Permite sobrescribir el timeout globalmente mediante una variable de entorno.
        try:
            init_timeout = float(os.getenv("MCP_INIT_TIMEOUT", init_timeout))
        except (ValueError, TypeError):
            pass

        # Prepara el diccionario de entorno, limpi√°ndolo de valores nulos o vac√≠os.
        clean_env: Optional[Dict[str, str]] = None
        if env:
            cleaned = {k: str(v) for k, v in env.items() if v not in (None, "")}
            clean_env = cleaned if cleaned else None

        attempts = 2  # N√∫mero de intentos de conexi√≥n (1 original + 1 reintento).
        last_err: Optional[BaseException] = None
        full_cmd = " ".join([server_name] + args)

        # Bucle de intentos de conexi√≥n.
        for attempt in range(1, attempts + 1):
            try:
                # Prepara un 'AsyncExitStack' para este intento.
                self.exit_stack = AsyncExitStack()

                print(f"[MCP] Conectando a '{os.path.basename(server_name)}' (intento {attempt}/{attempts})")
                if clean_env:
                    print(f"      ‚îñ‚îÄ Entorno: {list(clean_env.keys())}")
                print(f"      ‚îñ‚îÄ Timeout: {int(init_timeout)}s")


                # Define los par√°metros para iniciar el servidor como un subproceso.
                server_params = StdioServerParameters(
                    command=server_name,
                    args=args,
                    env=clean_env
                )

                # Inicia el cliente stdio, que a su vez lanza el proceso del servidor.
                stdio_context = stdio_client(server_params)
                # Entra en el contexto del cliente para obtener los streams de lectura y escritura.
                read_stream, write_stream = await self.exit_stack.enter_async_context(stdio_context)

                # Crea una sesi√≥n MCP usando los streams.
                session_context = ClientSession(read_stream, write_stream)
                self.session = await self.exit_stack.enter_async_context(session_context)

                # Llama al m√©todo 'initialize' del servidor con un tiempo de espera.
                try:
                    await asyncio.wait_for(self.session.initialize(), timeout=init_timeout)
                except asyncio.TimeoutError:
                    raise TimeoutError(f"Timeout en initialize() para '{os.path.basename(server_name)}'")

                # Si la inicializaci√≥n es exitosa, obtiene la lista de herramientas disponibles.
                response = await self.session.list_tools()
                tools = response.tools
                self._available_tools = [tool.name for tool in tools]
                print(f"  ‚úì Conexi√≥n exitosa a '{os.path.basename(server_name)}' | Herramientas: {self._available_tools}")

                self._connected = True
                return True  # Conexi√≥n exitosa, sale del bucle.

            except Exception as e:
                # Si ocurre un error, lo registra y se prepara para el siguiente intento.
                last_err = e
                print(f"  ‚úó Error al conectar '{os.path.basename(server_name)}' (intento {attempt}/{attempts}): {e}")

                # Limpia los recursos del intento fallido.
                try:
                    if self.exit_stack:
                        await asyncio.wait_for(self.exit_stack.aclose(), timeout=5.0)
                except Exception as close_err:
                    print(f"    Aviso: Error durante la limpieza del intento fallido: {close_err}")


                self.session = None
                self.exit_stack = None
                self._connected = False

                # Espera un poco antes de reintentar.
                if attempt < attempts:
                    await asyncio.sleep(2.0)

        print(f"  ‚úó Fallo definitivo conectando a '{os.path.basename(server_name)}': {last_err}")
        return False

    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]):
        """Llama a una herramienta espec√≠fica del servidor MCP conectado."""
        if not self.session or not self._connected:
            raise ConnectionError("No est√° conectado a ning√∫n servidor MCP para llamar a la herramienta.")

        try:
            # Llama a la herramienta y espera la respuesta.
            response = await self.session.call_tool(tool_name, arguments)
            # Devuelve el contenido principal de la respuesta, que puede estar en 'content' o 'result'.
            if hasattr(response, "content"):
                return response.content
            if hasattr(response, "result"):
                return response.result
            return response
        except Exception as e:
            print(f"‚úó Error al llamar a la herramienta '{tool_name}': {e}")
            # Devuelve None o relanza una excepci√≥n m√°s espec√≠fica.
            return None

    def get_available_tools(self) -> List[str]:
        """Devuelve la lista de nombres de herramientas disponibles en el servidor."""
        return self._available_tools

    async def _cleanup(self):
        """M√©todo privado para cerrar y limpiar los recursos del cliente de forma segura."""
        if self._cleanup_attempted:
            return
        self._cleanup_attempted = True

        try:
            # Usa el 'AsyncExitStack' para cerrar todos los contextos abiertos (sesi√≥n, proceso, etc.).
            if self.exit_stack:
                await asyncio.wait_for(self.exit_stack.aclose(), timeout=5.0)
        except asyncio.TimeoutError:
            print("Aviso: Tiempo de espera de limpieza agotado, forzando cierre")
        except asyncio.CancelledError:
            print("Aviso: La limpieza fue cancelada")
        except Exception as e:
            print(f"Aviso: Error durante la limpieza: {e}")
        finally:
            self.exit_stack = None

    async def close(self):
        """M√©todo p√∫blico para cerrar la conexi√≥n con el servidor de forma segura."""
        if not self._connected:
            return
        self._connected = False
        try:
            # Llama al m√©todo de limpieza con un tiempo de espera.
            await asyncio.wait_for(self._cleanup(), timeout=10.0)
        except Exception as e:
            print(f"Aviso: Error durante el cierre: {e}")
        finally:
            # Resetea el estado del cliente.
            self.session = None
            self.exit_stack = None


class MCPClientManager:
    """
    Gestiona un conjunto de m√∫ltiples 'RemoteMCPClient', uno para cada plataforma.
    Orquesta la conexi√≥n y desconexi√≥n de todos ellos.
    """

    def __init__(self, server_configs: Dict[str, Dict]):
        """Constructor. Recibe las configuraciones de todos los servidores."""
        self.server_configs = server_configs
        # Diccionario para almacenar las instancias de cliente, una por plataforma.
        self.clients: Dict[str, Optional[RemoteMCPClient]] = {}

    async def connect_all_servers(self):
        """Intenta conectar a todos los servidores que est√°n marcados como habilitados en la configuraci√≥n."""
        print("\n[MCP] Conectando a todos los servidores habilitados...")
        
        # Crea tareas para conectar a todos los servidores en paralelo.
        tasks = [
            self._connect_single_server(platform, config)
            for platform, config in self.server_configs.items()
            if config.get("enabled", False)
        ]
        
        # Ejecuta las tareas de conexi√≥n concurrentemente.
        await asyncio.gather(*tasks)

        # Imprime los servidores omitidos.
        for platform, config in self.server_configs.items():
            if not config.get("enabled", False):
                print(f"  ‚Ü∑ Omitido '{platform}' (deshabilitado en config)")


    async def _connect_single_server(self, platform: str, config: Dict):
        """Crea un cliente y intenta conectar a un √∫nico servidor MCP."""
        try:
            mcp_client = RemoteMCPClient()
            args = config.get("args", [])
            env = config.get("env", {})
            # Llama al m√©todo de conexi√≥n del cliente.
            success = await mcp_client.connect_to_server_by_name(config["server_name"], args, env)

            # Si la conexi√≥n es exitosa, almacena el cliente. Si no, almacena None.
            self.clients[platform] = mcp_client if success else None
        except Exception as e:
            print(f"  ‚úó Fallo cr√≠tico al inicializar la conexi√≥n para {platform}: {e}")
            self.clients[platform] = None

    def get_client(self, platform: str) -> Optional[RemoteMCPClient]:
        """Devuelve la instancia del cliente para una plataforma, o None si no est√° conectado."""
        return self.clients.get(platform)

    def is_platform_available(self, platform: str) -> bool:
        """Comprueba si el cliente de una plataforma est√° conectado y disponible."""
        client = self.clients.get(platform)
        return client is not None and client._connected

    def get_available_tools(self, platform: str) -> List[str]:
        """Obtiene la lista de herramientas disponibles para una plataforma espec√≠fica."""
        client = self.get_client(platform)
        return client.get_available_tools() if client else []

    async def close_all_clients(self):
        """
        Cierra todos los clientes MCP conectados de forma SECUENCIAL.
        Esto es importante en asyncio para evitar problemas de cancelaci√≥n de tareas.
        """
        print("[MCP] Cerrando todos los clientes...")
        # Itera sobre una copia de los √≠tems para poder modificar el diccionario original.
        for platform, client in list(self.clients.items()):
            if client:
                try:
                    print(f"      ‚îñ‚îÄ Cerrando '{platform}'...")
                    await client.close()  # Espera a que cada cliente se cierre antes de pasar al siguiente.
                except Exception as e:
                    print(f"      ‚îñ‚îÄ Error al cerrar '{platform}': {e}")
        # Limpia el diccionario de clientes.
        self.clients.clear()
        print("[MCP] Todos los clientes cerrados.")



---
File: /platform_handlers.py
---

# platform_handlers.py
# -*- coding: utf-8 -*-

# Este archivo es una versi√≥n consolidada y mejorada que fusiona la l√≥gica de
# platform_manager.py y platform_handlers.py, eliminando la redundancia.

from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from datetime import datetime
import json
import re
from collections import Counter
from urllib.parse import urlparse

# NOTA: No se importa AIClientManager aqu√≠ para evitar una dependencia circular.
# Se pasa como un argumento en el m√©todo de la f√°brica 'create_handler'.

# ---------------- Clase Base ----------------
class BasePlatformHandler(ABC):
    """
    Define la plantilla (interfaz) que todos los manejadores de plataforma deben seguir.
    Garantiza una estructura consistente.
    """
    def __init__(self, platform_name: str):
        """Constructor. Almacena el nombre de la plataforma."""
        self.platform_name = platform_name
    
    @abstractmethod
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        """M√©todo abstracto para la l√≥gica de investigaci√≥n. Debe ser implementado por las subclases."""
        pass
    
    @abstractmethod
    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        """M√©todo abstracto para procesar la respuesta cruda. Debe ser implementado por las subclases."""
        pass
    
    def create_error_result(self, keyword: str, error: str) -> Dict[str, Any]:
        """M√©todo de utilidad para crear un resultado de error estandarizado."""
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": [], "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": {}, "error": str(error)
        }

# ---------------- Manejador de YouTube ----------------
class YouTubeHandler(BasePlatformHandler):
    """Manejador con la l√≥gica espec√≠fica para investigar en YouTube."""
    def __init__(self):
        super().__init__("youtube")
    
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        try:
            params = {"query": keyword, "order": "relevance", "type": "video", "max_results": 10}
            tool_name = config["tools"][0]
            response = await client.call_tool(tool_name, params)
            return self.process_response(response, keyword)
        except Exception as e:
            return self.create_error_result(keyword, str(e))
    
    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        results = []
        data = self._extract_data_from_response(response)
        videos = data.get('videos', data.get('items', [])) if isinstance(data, dict) else (data if isinstance(data, list) else [])
        
        for video in videos:
            snippet = video.get('snippet', {})
            video_id = video.get('id', {}).get('videoId', '')
            if not video_id: continue

            results.append({
                'title': snippet.get('title', ''), 'description': snippet.get('description', ''),
                'published_at': snippet.get('publishedAt', ''), 'channel': snippet.get('channelTitle', ''),
                'video_id': video_id, 'url': f"https://www.youtube.com/watch?v={video_id}",
                'content_type': self._classify_content(snippet.get('title', ''), snippet.get('description', '')),
                'language': self._detect_language(snippet.get('title', '') + ' ' + snippet.get('description', ''))
            })
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": results, "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": self._calculate_engagement_metrics(results)
        }
    
    def _extract_data_from_response(self, response: Any) -> Any:
        if isinstance(response, list) and len(response) > 0 and hasattr(response[0], 'text'):
            try: return json.loads(response[0].text)
            except (json.JSONDecodeError, AttributeError): return {}
        elif hasattr(response, 'content'): return response.content
        return response if isinstance(response, (dict, list)) else {}

    def _classify_content(self, title: str, description: str) -> str:
        text = (title + ' ' + description).lower()
        if any(k in text for k in ['Ëß£Ë™¨', 'Ë™¨Êòé', 'ÂÖ•ÈñÄ', 'Âü∫Á§é', 'Â≠¶Áøí', '„ÉÅ„É•„Éº„Éà„É™„Ç¢„É´']): return "Ëß£Ë™¨ÂãïÁîª"
        elif any(k in text for k in ['„Éá„É¢', '„Éá„É¢„É≥„Çπ„Éà„É¨„Éº„Ç∑„Éß„É≥', 'ÂÆüÊºî', '„Çµ„É≥„Éó„É´']): return "„Éá„É¢"
        elif any(k in text for k in ['„Ç´„É≥„Éï„Ç°„É¨„É≥„Çπ', '„Çª„Éü„Éä„Éº', 'Ë¨õÊºî', 'Áô∫Ë°®', 'talk']): return "„Ç´„É≥„Éï„Ç°„É¨„É≥„Çπ"
        elif any(k in text for k in ['„Éã„É•„Éº„Çπ', 'ÊúÄÊñ∞', '„Ç¢„ÉÉ„Éó„Éá„Éº„Éà', '„É™„É™„Éº„Çπ']): return "„Éã„É•„Éº„Çπ"
        else: return "„Åù„ÅÆ‰ªñ"
    
    def _detect_language(self, text: str) -> str:
        if re.search(r'[\u3040-\u30ff\u3400-\u4dbf\u4e00-\u9fff]', text): return "ja"
        return "en"
    
    def _calculate_engagement_metrics(self, results: List[Dict]) -> Dict:
        return {"total_videos": len(results)}

# ---------------- Manejador de GitHub ----------------
class GitHubHandler(BasePlatformHandler):
    """Manejador robusto para investigar en GitHub."""
    def __init__(self):
        super().__init__("github")
    
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        try:
            params = {
                "query": f"{keyword} stars:>50", "sort": "stars",
                "order": "desc", "per_page": 10
            }
            tool_name = "search_repositories"
            response = await client.call_tool(tool_name, params)
            return self.process_response(response, keyword)
        except Exception as e:
            return self.create_error_result(keyword, str(e))
    
    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        results = []
        repos = self._extract_repositories(response)
        for repo in repos:
            if isinstance(repo, dict):
                stars = repo.get('stargazers_count', repo.get('stars', 0))
                created_at = repo.get('created_at', '')
                star_rate, days_old, is_trending = self._calculate_trend_metrics(stars, created_at)
                results.append({
                    'name': repo.get('name', ''), 'description': repo.get('description', ''),
                    'owner': repo.get('owner', {}).get('login', ''), 'stars': stars, 'language': repo.get('language', ''),
                    'url': repo.get('html_url', ''), 'created_at': created_at, 'topics': repo.get('topics', []),
                    'star_rate': round(star_rate, 2), 'days_old': days_old, 'is_trending': is_trending,
                    'trend_score': self._calculate_trend_score(stars, days_old, star_rate)
                })
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": results, "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": self._calculate_engagement_metrics(results)
        }
    
    def _extract_repositories(self, response: Any) -> List[Dict]:
        if not response: return []
        if isinstance(response, str):
            try: return self._extract_repositories(json.loads(response))
            except json.JSONDecodeError: return []
        if isinstance(response, list) and len(response) > 0 and hasattr(response[0], 'text'):
            try: return self._extract_repositories(json.loads(response[0].text))
            except (json.JSONDecodeError, AttributeError): return []
        if isinstance(response, list): return response
        if isinstance(response, dict): return response.get('items', response.get('repositories', []))
        return []

    def _calculate_trend_metrics(self, stars: int, created_at: str) -> tuple:
        if not created_at or not isinstance(stars, int): return (0.0, 0, False)
        try:
            created_date = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
            days_old = (datetime.now(created_date.tzinfo) - created_date).days
            if days_old <= 0: return (stars, 0, False)
            star_rate = stars / days_old
            is_trending = (stars >= 100 and days_old <= 365 and star_rate > 0.5)
            return (star_rate, days_old, is_trending)
        except (ValueError, TypeError): return (0.0, 0, False)
    
    def _calculate_trend_score(self, stars: int, days_old: int, star_rate: float) -> float:
        if days_old <= 0: return 0.0
        base_score = min(star_rate * 10, 50)
        recency_bonus = max(0, (365 - days_old) / 365 * 30)
        star_bonus = min(stars / 200, 20)
        return round(min(base_score + recency_bonus + star_bonus, 100), 2)
    
    def _calculate_engagement_metrics(self, results: List[Dict]) -> Dict:
        if not results: return {"repo_count": 0}
        languages = [r.get('language') for r in results if r.get('language')]
        return {
            "repo_count": len(results),
            "total_stars": sum(r.get('stars', 0) for r in results),
            "trending_repos_count": sum(1 for r in results if r.get('is_trending')),
            "top_languages": dict(Counter(languages).most_common(3)) if languages else {}
        }

# ---------------- Manejador de Web ----------------
class WebHandler(BasePlatformHandler):
    def __init__(self):
        super().__init__("web")
    
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        try:
            params = {"query": f"{keyword} site:github.com OR site:arxiv.org OR site:huggingface.co", "language": "ja", "region": "jp", "max_results": 10}
            tool_name = config["tools"][0]
            response = await client.call_tool(tool_name, params)
            return self.process_response(response, keyword)
        except Exception as e:
            return self.create_error_result(keyword, str(e))
    
    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        results = self._parse_web_results(response)
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": results, "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": {"search_count": len(results)}
        }
    
    def _parse_web_results(self, response: Any) -> List[Dict]:
        if isinstance(response, list) and len(response) > 0 and hasattr(response[0], 'text'):
            try: response = json.loads(response[0].text)
            except (json.JSONDecodeError, AttributeError): response = {}
        
        results = []
        web_results = response.get('results', []) if isinstance(response, dict) else []
        for result in web_results:
            url = result.get('url', result.get('link', ''))
            results.append({
                'title': result.get('title', ''), 'snippet': result.get('snippet', ''),
                'url': url, 'source': urlparse(url).netloc if url else ''
            })
        return results

# ---------------- Manejador de ArXiv ----------------
class ArxivHandler(BasePlatformHandler):
    def __init__(self, ai_client_manager: Optional[Any] = None):
        super().__init__("arxiv")
        self.ai_client = ai_client_manager
    
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        try:
            english_query = await self._translate_keyword(keyword)
            params = {"query": english_query, "max_results": 10, "sort_by": "relevance"}
            response = await client.call_tool("search_arxiv", params)
            return self.process_response(response, keyword)
        except Exception as e:
            return self.create_error_result(keyword, str(e))

    async def _translate_keyword(self, keyword: str) -> str:
        if not self.ai_client or not re.search(r'[\u3040-\u30ff]', keyword): return keyword
        try:
            prompt = f"Translate the following Japanese technical keyword to English for an ArXiv search. Provide only the English translation, no extra text. Keyword: '{keyword}'"
            translation = await self.ai_client.chat_completion(prompt)
            return translation.strip().replace('"', '') or keyword
        except Exception: return keyword
            
    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        results = []
        papers = self._extract_papers(response)
        for paper in papers:
            if isinstance(paper, dict):
                published_date = paper.get('published', '')
                days_old, is_recent = self._calculate_time_metrics(published_date)
                results.append({
                    'title': paper.get('title', ''), 'abstract': paper.get('summary', ''),
                    'authors': paper.get('authors', []), 'published_date': published_date,
                    'url': paper.get('url', ''), 'days_old': days_old, 'is_recent': is_recent,
                })
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": results, "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": {"paper_count": len(results), "recent_paper_count": sum(1 for r in results if r['is_recent'])}
        }

    def _extract_papers(self, response: Any) -> List[Dict]:
        if isinstance(response, list) and len(response)>0 and hasattr(response[0], 'text'):
            try: response = json.loads(response[0].text)
            except (json.JSONDecodeError, AttributeError): return []
        if isinstance(response, dict): return response.get('results', [])
        if isinstance(response, list): return response
        return []

    def _calculate_time_metrics(self, published_date: str) -> tuple:
        if not published_date: return (9999, False)
        try:
            pub_date = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
            days_old = (datetime.now(pub_date.tzinfo) - pub_date).days
            return (days_old, days_old <= 90)
        except (ValueError, TypeError): return (9999, False)

# ---------------- Manejador de HackerNews ----------------
class HackerNewsHandler(BasePlatformHandler):
    def __init__(self, ai_client_manager: Optional[Any] = None):
        super().__init__("hackernews")
        self.ai_client = ai_client_manager
        
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        try:
            english_keyword = await self._translate_keyword(keyword)
            # HackerNews tool might be named 'search' or similar, adapt as needed.
            # Assuming the tool is 'getStories' for this implementation.
            params = {"query": english_keyword, "max_results": 15}
            response = await client.call_tool(config['tools'][0], params)
            return self.process_response(response, keyword)
        except Exception as e:
            return self.create_error_result(keyword, str(e))
    
    async def _translate_keyword(self, keyword: str) -> str:
        # Same translation logic as Arxiv
        if not self.ai_client or not re.search(r'[\u3040-\u30ff]', keyword): return keyword
        try:
            prompt = f"Translate the following Japanese keyword to a simple English equivalent for a HackerNews search. Provide only the English translation. Keyword: '{keyword}'"
            translation = await self.ai_client.chat_completion(prompt)
            return translation.strip().replace('"', '') or keyword
        except Exception: return keyword

    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        results = []
        posts = self._extract_posts(response)
        for post in posts:
            if isinstance(post, dict):
                results.append({
                    'title': post.get('title', ''), 'url': post.get('url', ''),
                    'score': post.get('score', post.get('points', 0)),
                    'comments_count': post.get('descendants', post.get('num_comments', 0)),
                })
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": results, "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": self._calculate_engagement_metrics(results)
        }
    
    def _extract_posts(self, response: Any) -> List[Dict]:
        if isinstance(response, list): return response
        if isinstance(response, dict): return response.get('hits', [])
        return []

    def _calculate_engagement_metrics(self, results: List[Dict]) -> Dict:
        if not results: return {"post_count": 0}
        post_count = len(results)
        total_score = sum(r.get('score', 0) for r in results)
        return {"post_count": post_count, "avg_score": round(total_score / post_count if post_count > 0 else 0)}

# ---------------- Manejador de Supabase (Placeholder) ----------------
class SupabaseHandler(BasePlatformHandler):
    def __init__(self):
        super().__init__("supabase")
    
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        return self.create_error_result(keyword, "Supabase no se utiliza para la investigaci√≥n de keywords.")
    
    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        return self.create_error_result(keyword, "Supabase no se utiliza para la investigaci√≥n de keywords.")

# ---------------- Manejador de Research Hub ----------------
class ResearchHubHandler(BasePlatformHandler):
    def __init__(self):
        super().__init__("research_hub")

    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        try:
            params = {"query": keyword, "limit": 10}
            response = await client.call_tool("search_papers", params)
            return self.process_response(response, keyword)
        except Exception as e:
            return self.create_error_result(keyword, str(e))

    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        results = []
        papers_data = response
        if isinstance(response, list) and len(response) > 0 and hasattr(response[0], 'content'):
            try: papers_data = json.loads(response[0].content)
            except (json.JSONDecodeError, AttributeError): papers_data = []

        if isinstance(papers_data, list):
            for paper in papers_data:
                results.append({
                    'title': paper.get('title', 'N/A'), 'authors': ", ".join(paper.get('authors', [])),
                    'url': paper.get('url', ''), 'abstract': paper.get('summary', ''),
                    'source': paper.get('source', 'Unknown'), 'year': paper.get('year', 0)
                })
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": results, "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": self._calculate_engagement_metrics(results)
        }

    def _calculate_engagement_metrics(self, results: List[Dict]) -> Dict:
        if not results: return {"paper_count": 0, "recent_papers_count": 0}
        current_year = datetime.now().year
        recent_papers = sum(1 for p in results if p.get('year', 0) >= current_year - 2)
        return {"paper_count": len(results), "recent_papers_count": recent_papers}

# ---------------- F√°brica de Manejadores ----------------
class PlatformHandlerFactory:
    """Utiliza el patr√≥n de dise√±o Factory para crear el manejador de plataforma correcto."""
    
    _handler_classes: Dict[str, Any] = {
        "youtube": YouTubeHandler,
        "github": GitHubHandler,
        "web": WebHandler,
        "arxiv": ArxivHandler,
        "hackernews": HackerNewsHandler,
        "supabase": SupabaseHandler,
        "research_hub": ResearchHubHandler
    }

    @staticmethod
    def create_handler(platform: str, ai_client_manager: Optional[Any] = None) -> BasePlatformHandler:
        handler_class = PlatformHandlerFactory._handler_classes.get(platform)
        if not handler_class:
            raise ValueError(f"No hay un manejador disponible para la plataforma: {platform}")
        
        # Inyecta el cliente de IA si el constructor del manejador lo acepta.
        import inspect
        sig = inspect.signature(handler_class.__init__)
        if 'ai_client_manager' in sig.parameters:
            return handler_class(ai_client_manager=ai_client_manager)
        else:
            return handler_class()



---
File: /README.md
---

# AI Trend Research Engine

Basado en los repositorios:
[EmpowerHerDev/ai-trend-research-system](https://github.com/EmpowerHerDev/ai-trend-research-system) y [Ladvien/research_hub_mcp](https://github.com/Ladvien/research_hub_mcp)

![Python Version](https://img.shields.io/badge/python-3.9%2B-blue)![License](https://img.shields.io/badge/license-MIT-green)![Status](https://img.shields.io/badge/status-activo-brightgreen)

<p align="center">
  <strong>Languages:</strong>
  <br>
  <a href="#-english">English</a> | <a href="#-espa√±ol">Espa√±ol</a> | <a href="#-catal√†">Catal√†</a>
</p>

---

<a name="-english"></a>
## üá¨üáß English

<details>
<summary><strong>Table of Contents</strong></summary>

- [üöÄ Key Features](#-key-features)
- [üèõÔ∏è Project Architecture](#Ô∏è-project-architecture)
- [üõ†Ô∏è Installation and Setup](#Ô∏è-installation-and-setup)
  - [1. Prerequisites](#1-prerequisites)
  - [2. Clone the Repository](#2-clone-the-repository)
  - [3. Install Python Dependencies](#3-install-python-dependencies)
  - [4. Configure Environment Variables](#4-configure-environment-variables)
  - [5. Set Up the Database (Supabase)](#5-set-up-the-database-supabase)
- [‚ñ∂Ô∏è Usage](#Ô∏è-usage)
  - [Daily Trend Research](#daily-trend-research)
  - [Deep Dive Research](#deep-dive-research)
- [ü§ù Contributing](#-contributing)
- [üìÑ License](#-license)

</details>

An automated and modular system for researching trends in Artificial Intelligence. It collects and processes data from multiple platforms (GitHub, YouTube, ArXiv, etc.), uses language models (LLMs) to extract insights, discover new keywords, and generates comprehensive reports in multiple formats (JSON, Notion, Supabase).

**Note:** This project was developed by modifying the [EmpowerHerDev/ai-trend-research-system](https://github.com/EmpowerHerDev/ai-trend-research-system) project and integrating the custom research server from [Ladvien/research_hub_mcp](https://github.com/Ladvien/research_hub_mcp). The result is a flexible and modular system that now supports multiple large language model (LLM) providers, including **OpenAI (GPT), Google (Gemini), Anthropic (Claude), Groq, and Ollama**. This work is also part of a personal learning journey in the field of artificial intelligence agent development.

### üöÄ Key Features

-   **Multi-Platform Research**: Gathers data from YouTube, GitHub, web searches, ArXiv, HackerNews, and a custom paper research engine.
-   **AI-Powered Processing**: Uses LLMs (OpenAI, Gemini, Groq, Anthropic, Ollama) for advanced tasks such as:
    -   Intelligent extraction of new keywords.
    -   Generation of dynamic and contextual recommendations.
    -   Translation of search terms for multilingual queries.
-   **Asynchronous Architecture**: Built with `asyncio` for high concurrency and efficiency in network operations and process handling.
-   **Report Generation**: Automatically creates detailed reports in:
    -   **JSON**: Local files for archiving and analysis.
    -   **Notion**: Structured and easy-to-read pages in your workspace.
    -   **Supabase**: Records in a PostgreSQL database for long-term persistence.
-   **Keyword Management**: Maintains a lifecycle for keywords, with a master catalog, an active list for research, and a history of executions.
-   **Highly Configurable**: All settings (API keys, AI model selection, file paths) are managed through a `.env` file for easy portability and security.
-   **Standard Protocol**: Uses the **Model Context Protocol (MCP)** to communicate with each platform's servers, ensuring a standardized and decoupled interface.

### üèõÔ∏è Project Architecture

The system is designed with a modular and decoupled architecture to facilitate maintenance and extension.

-   `ai_trend_researcher.py`: The main orchestrator that runs the daily research workflow.
-   `config_manager.py`: Centralizes the loading and validation of all configurations from the `.env` file.
-   `mcp_client_manager.py`: Manages the lifecycle (connection, calls, closing) of clients for each platform's MCP servers.
-   `platform_handlers.py`: Contains the specific logic to interact with each platform (e.g., `YouTubeHandler`, `GitHubHandler`), process their data, and standardize it.
-   `data_processor.py`: Handles data analysis, keyword extraction, and recommendation generation using both heuristics and LLMs.
-   `ai_client_manager.py`: Acts as a factory to interact uniformly with different language model providers (OpenAI, Gemini, etc.).
-   `report_generator.py`: Generates the final reports in all supported formats (JSON, Notion, Supabase).
-   `keyword_manager.py`: Manages the keyword database in JSON files.
-   `research_assistant.py`: An advanced script for performing deep research dives using the custom `research_hub` server.

### üõ†Ô∏è Installation and Setup

Follow these steps to get the project running in your local environment.

#### 1. Prerequisites

-   **Python 3.9+**
-   **Node.js and npm** (required for `npx`, which runs the community's MCP servers).
-   **Git**
-   (Optional) **Rust Compiler**, if you want to compile the `research_hub` executable from the source code.

#### 2. Clone the Repository

```bash
git clone https://github.com/your_user/your_repository.git
cd your_repository
```

#### 3. Install Python Dependencies

Create a virtual environment (recommended) and install the required libraries.

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

#### 4. Configure Environment Variables

This is the most important step. The project is controlled via a `.env` file.

1.  Copy the example file:
    ```bash
    cp .env.example .env
    ```
2.  Edit the `.env` file with a text editor and fill in all the necessary API keys and paths.

```env
# .env.example

# --- API Keys (Required for the platforms you use) ---
YOUTUBE_API_KEY=AIzaSy...
GITHUB_PERSONAL_ACCESS_TOKEN=ghp_...
NOTION_API_KEY=secret_...
SUPABASE_ACCESS_TOKEN=eyJhbGciOi...
SILICONFLOW_API_KEY= # Optional, for the ArXiv server

# --- Notion Configuration (Required if using Notion) ---
NOTION_PARENT_PAGE_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# --- System Path Configuration (IMPORTANT!) ---
# Modify these paths to point to directories on your machine.
RESEARCH_PAPERS_DIR="/home/user/documents/research-papers"
RESEARCH_HUB_EXECUTABLE="/home/user/projects/rust-research-mcp/target/release/rust-research-mcp"

# --- AI Provider Configuration ---
# Choose one: "openai", "gemini", "groq", "anthropic", "ollama"
AI_PROVIDER="openai"

# Fill in the API key for your chosen provider.
OPENAI_API_KEY="sk-..."
GOOGLE_API_KEY="AIzaSy..."
GROQ_API_KEY="gsk_..."
ANTHROPIC_API_KEY="sk-ant-..."

# --- (Optional) Specific AI Models ---
# If left blank, default models will be used.
AI_MODEL_OPENAI="gpt-4o"
AI_MODEL_GEMINI="gemini-1.5-flash"
AI_MODEL_GROQ="llama3-70b-8192"
AI_MODEL_ANTHROPIC="claude-3-haiku-20240307"
AI_MODEL_OLLAMA="llama3"
```

#### 5. Set Up the Database (Supabase)

If you plan to use the Supabase integration, make sure your database table matches the schema defined in `supabase_schema.sql`. You can run this script in the SQL editor of your Supabase project.

### ‚ñ∂Ô∏è Usage

Once configured, you can run the two main workflows.

#### Daily Trend Research

This is the main workflow. It will run the research on all enabled platforms, analyze the data, and generate reports.

```bash
python ai_trend_researcher.py
```

The script will print its progress to the console. Upon completion, you will find the JSON report in the `reports/` directory and, if configured, a new page in Notion and a new record in your Supabase table.

#### Deep Dive Research

This script uses the `research_hub` server to perform advanced paper searches, download them, analyze them, and generate bibliographies.

1.  Ensure the `research_hub` executable is correctly configured in your `.env`.
2.  (Optional) Add search terms to the `terminos.txt` file.

```bash
python research_assistant.py
```

The results of this execution (CSVs, JSONs, BibTeX files, and logs) will be saved in subdirectories within `salidas/` to keep each run organized.

### ü§ù Contributing

Contributions are welcome. If you have ideas for improving the project, new platforms to integrate, or find any bugs, please open an issue or submit a pull request.

### üìÑ License

This project is licensed under the MIT License. See the `LICENSE` file for more details.

---

<a name="-espa√±ol"></a>
## üá™üá∏ Espa√±ol

<details>
<summary><strong>Tabla de Contenidos</strong></summary>

- [üöÄ Caracter√≠sticas Principales](#-caracter√≠sticas-principales)
- [üèõÔ∏è Arquitectura del Proyecto](#Ô∏è-arquitectura-del-proyecto)
- [üõ†Ô∏è Instalaci√≥n y Configuraci√≥n](#Ô∏è-instalaci√≥n-y-configuraci√≥n)
  - [1. Prerrequisitos](#1-prerrequisitos)
  - [2. Clonar el Repositorio](#2-clonar-el-repositorio)
  - [3. Instalar Dependencias de Python](#3-instalar-dependencias-de-python)
  - [4. Configurar las Variables de Entorno](#4-configurar-las-variables-de-entorno)
  - [5. Configurar la Base de Datos (Supabase)](#5-configurar-la-base-de-datos-supabase)
- [‚ñ∂Ô∏è Uso](#Ô∏è-uso)
  - [Investigaci√≥n Diaria de Tendencias](#investigaci√≥n-diaria-de-tendencias)
  - [Inmersi√≥n Profunda de Investigaci√≥n](#inmersi√≥n-profunda-de-investigaci√≥n)
- [ü§ù Contribuciones](#-contribuciones)
- [üìÑ Licencia](#-licencia)

</details>

Un sistema automatizado y modular para la investigaci√≥n de tendencias en Inteligencia Artificial. Recopila y procesa datos de m√∫ltiples plataformas (GitHub, YouTube, ArXiv, etc.), utiliza modelos de lenguaje (LLMs) para extraer *insights*, descubrir nuevas palabras clave y genera informes completos en m√∫ltiples formatos (JSON, Notion, Supabase).

**Nota:** Este proyecto ha sido desarrollado modificando el proyecto [EmpowerHerDev/ai-trend-research-system](https://github.com/EmpowerHerDev/ai-trend-research-system) e integrando el servidor de investigaci√≥n personalizado de [Ladvien/research_hub_mcp](https://github.com/Ladvien/research_hub_mcp). El resultado es un sistema flexible y modular que ahora es compatible con m√∫ltiples proveedores de modelos de lenguaje grande (LLM), incluyendo **OpenAI (GPT), Google (Gemini), Anthropic (Claude), Groq y Ollama**. Este trabajo tambi√©n forma parte de un proceso de aprendizaje personal en el campo del desarrollo de agentes de inteligencia artificial.

### üöÄ Caracter√≠sticas Principales

-   **Investigaci√≥n Multiplataforma**: Recopila datos de YouTube, GitHub, b√∫squedas web, ArXiv, HackerNews y un motor de investigaci√≥n de *papers* personalizado.
-   **Procesamiento con IA**: Utiliza LLMs (OpenAI, Gemini, Groq, Anthropic, Ollama) para tareas avanzadas como:
    -   Extracci√≥n inteligente de nuevas palabras clave.
    -   Generaci√≥n de recomendaciones din√°micas y contextuales.
    -   Traducci√≥n de t√©rminos de b√∫squeda para consultas multiling√ºes.
-   **Arquitectura As√≠ncrona**: Construido con `asyncio` para una alta concurrencia y eficiencia en operaciones de red y manejo de procesos.
-   **Generaci√≥n de Informes**: Crea autom√°ticamente informes detallados en:
    -   **JSON**: Archivos locales para archivo y an√°lisis.
    -   **Notion**: P√°ginas estructuradas y f√°ciles de leer en tu *workspace*.
    -   **Supabase**: Registros en una base de datos PostgreSQL para persistencia a largo plazo.
-   **Gesti√≥n de Keywords**: Mantiene un ciclo de vida para las palabras clave, con un cat√°logo maestro, una lista activa para investigar y un historial de ejecuciones.
-   **Altamente Configurable**: Toda la configuraci√≥n (API keys, selecci√≥n de modelos de IA, rutas de archivos) se gestiona a trav√©s de un archivo `.env` para facilitar la portabilidad y seguridad.
-   **Protocolo Est√°ndar**: Utiliza el **Model Context Protocol (MCP)** para comunicarse con los servidores de cada plataforma, asegurando una interfaz estandarizada y desacoplada.

### üèõÔ∏è Arquitectura del Proyecto

El sistema est√° dise√±ado con una arquitectura modular y desacoplada para facilitar su mantenimiento y extensi√≥n.

-   `ai_trend_researcher.py`: El orquestador principal que ejecuta el flujo de investigaci√≥n diario.
-   `config_manager.py`: Centraliza la carga y validaci√≥n de toda la configuraci√≥n desde el archivo `.env`.
-   `mcp_client_manager.py`: Gestiona el ciclo de vida (conexi√≥n, llamadas, cierre) de los clientes para los servidores MCP de cada plataforma.
-   `platform_handlers.py`: Contiene la l√≥gica espec√≠fica para interactuar con cada plataforma (ej. `YouTubeHandler`, `GitHubHandler`), procesar sus datos y estandarizarlos.
-   `data_processor.py`: Se encarga del an√°lisis de datos, la extracci√≥n de keywords y la generaci√≥n de recomendaciones utilizando tanto heur√≠sticas como LLMs.
-   `ai_client_manager.py`: Act√∫a como una f√°brica para interactuar de forma unificada con diferentes proveedores de modelos de lenguaje (OpenAI, Gemini, etc.).
-   `report_generator.py`: Genera los informes finales en todos los formatos soportados (JSON, Notion, Supabase).
-   `keyword_manager.py`: Administra la base de datos de palabras clave en archivos JSON.
-   `research_assistant.py`: Un script avanzado para realizar inmersiones profundas de investigaci√≥n utilizando el servidor personalizado `research_hub`.

### üõ†Ô∏è Instalaci√≥n y Configuraci√≥n

Sigue estos pasos para poner en marcha el proyecto en tu entorno local.

#### 1. Prerrequisitos

-   **Python 3.9+**
-   **Node.js y npm** (necesario para `npx`, que ejecuta los servidores MCP de la comunidad).
-   **Git**
-   (Opcional) **Compilador de Rust**, si deseas compilar el ejecutable de `research_hub` desde el c√≥digo fuente.

#### 2. Clonar el Repositorio

```bash
git clone https://github.com/tu_usuario/tu_repositorio.git
cd tu_repositorio
```

#### 3. Instalar Dependencias de Python

Crea un entorno virtual (recomendado) e instala las bibliotecas necesarias.

```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
pip install -r requirements.txt```

#### 4. Configurar las Variables de Entorno

Este es el paso m√°s importante. El proyecto se controla mediante un archivo `.env`.

1.  Copia el archivo de ejemplo:
    ```bash
    cp .env.example .env
    ```
2.  Edita el archivo `.env` con un editor de texto y rellena todas las claves de API y rutas necesarias.

```env
# .env.example

# --- Claves de API (Obligatorias para las plataformas que uses) ---
YOUTUBE_API_KEY=AIzaSy...
GITHUB_PERSONAL_ACCESS_TOKEN=ghp_...
NOTION_API_KEY=secret_...
SUPABASE_ACCESS_TOKEN=eyJhbGciOi...
SILICONFLOW_API_KEY= # Opcional, para el servidor de ArXiv

# --- Configuraci√≥n de Notion (Obligatoria si se usa Notion) ---
NOTION_PARENT_PAGE_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# --- Configuraci√≥n de Rutas del Sistema (¬°IMPORTANTE!) ---
# Modifica estas rutas para que apunten a directorios en tu m√°quina.
RESEARCH_PAPERS_DIR="/home/user/documentos/research-papers"
RESEARCH_HUB_EXECUTABLE="/home/user/proyectos/rust-research-mcp/target/release/rust-research-mcp"

# --- Configuraci√≥n del Proveedor de IA ---
# Elige uno: "openai", "gemini", "groq", "anthropic", "ollama"
AI_PROVIDER="openai"

# Rellena la clave de API para el proveedor que hayas elegido.
OPENAI_API_KEY="sk-..."
GOOGLE_API_KEY="AIzaSy..."
GROQ_API_KEY="gsk_..."
ANTHROPIC_API_KEY="sk-ant-..."

# --- (Opcional) Modelos Espec√≠ficos de IA ---
# Si se dejan en blanco, se usar√°n los modelos por defecto.
AI_MODEL_OPENAI="gpt-4o"
AI_MODEL_GEMINI="gemini-1.5-flash"
AI_MODEL_GROQ="llama3-70b-8192"
AI_MODEL_ANTHROPIC="claude-3-haiku-20240307"
AI_MODEL_OLLAMA="llama3"
```

#### 5. Configurar la Base de Datos (Supabase)

Si planeas usar la integraci√≥n con Supabase, aseg√∫rate de que tu tabla en la base de datos coincida con el esquema definido en `supabase_schema.sql`. Puedes ejecutar ese script en el editor SQL de tu proyecto de Supabase.

### ‚ñ∂Ô∏è Uso

Una vez configurado, puedes ejecutar los dos flujos de trabajo principales.

#### Investigaci√≥n Diaria de Tendencias

Este es el flujo principal. Ejecutar√° la investigaci√≥n en todas las plataformas habilitadas, analizar√° los datos y generar√° los informes.

```bash
python ai_trend_researcher.py
```

El script imprimir√° su progreso en la consola. Al finalizar, encontrar√°s el informe JSON en el directorio `reports/` y, si est√° configurado, una nueva p√°gina en Notion y un nuevo registro en tu tabla de Supabase.

#### Inmersi√≥n Profunda de Investigaci√≥n

Este script utiliza el servidor `research_hub` para realizar b√∫squedas avanzadas de *papers*, descargarlos, analizarlos y generar bibliograf√≠as.

1.  Aseg√∫rate de que el ejecutable `research_hub` est√© correctamente configurado en tu `.env`.
2.  (Opcional) A√±ade t√©rminos de b√∫squeda al archivo `terminos.txt`.

```bash
python research_assistant.py
```

Los resultados de esta ejecuci√≥n (CSVs, JSONs, archivos BibTeX y logs) se guardar√°n en subdirectorios dentro de `salidas/` para mantener cada ejecuci√≥n organizada.

### ü§ù Contribuciones

Las contribuciones son bienvenidas. Si tienes ideas para mejorar el proyecto, nuevas plataformas para integrar o encuentras alg√∫n error, por favor abre un *issue* o env√≠a un *pull request*.

### üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Consulta el archivo `LICENSE` para m√°s detalles.

---

<a name="-catal√†"></a>
## CAT Catal√†

<details>
<summary><strong>Taula de Continguts</strong></summary>

- [üöÄ Caracter√≠stiques Principals](#-caracter√≠stiques-principals)
- [üèõÔ∏è Arquitectura del Projecte](#Ô∏è-arquitectura-del-projecte)
- [üõ†Ô∏è Instal¬∑laci√≥ i Configuraci√≥](#Ô∏è-installaci√≥-i-configuraci√≥)
  - [1. Prerequisits](#1-prerequisits)
  - [2. Clonar el Repositori](#2-clonar-el-repositori)
  - [3. Instal¬∑lar Depend√®ncies de Python](#3-installar-depend√®ncies-de-python)
  - [4. Configurar les Variables d'Entorn](#4-configurar-les-variables-dentorn)
  - [5. Configurar la Base de Dades (Supabase)](#5-configurar-la-base-de-dades-supabase)
- [‚ñ∂Ô∏è √ös](#Ô∏è-√∫s)
  - [Recerca Di√†ria de Tend√®ncies](#recerca-di√†ria-de-tend√®ncies)
  - [Recerca d'Immersi√≥ Profunda](#recerca-dimmersi√≥-profunda)
- [ü§ù Contribucions](#-contribucions)
- [üìÑ Llic√®ncia](#-llic√®ncia)

</details>

Un sistema automatitzat i modular per a la investigaci√≥ de tend√®ncies en Intel¬∑lig√®ncia Artificial. Recopila i processa dades de m√∫ltiples plataformes (GitHub, YouTube, ArXiv, etc.), utilitza models de llenguatge (LLMs) per extreure *insights*, descobrir noves paraules clau i genera informes complets en m√∫ltiples formats (JSON, Notion, Supabase).

**Nota:** Aquest projecte ha estat desenvolupat modificant el projecte [EmpowerHerDev/ai-trend-research-system](https://github.com/EmpowerHerDev/ai-trend-research-system) i integrant el servidor de recerca personalitzat de [Ladvien/research_hub_mcp](https://github.com/Ladvien/research_hub_mcp). El resultat √©s un sistema flexible i modular que ara √©s compatible amb m√∫ltiples prove√Ødors de models de llenguatge grans (LLM), incloent-hi **OpenAI (GPT), Google (Gemini), Anthropic (Claude), Groq i Ollama**. Aquest treball tamb√© forma part d'un proc√©s d'aprenentatge personal en el camp del desenvolupament d'agents d'intel¬∑lig√®ncia artificial.

### üöÄ Caracter√≠stiques Principals

-   **Recerca Multiplataforma**: Recopila dades de YouTube, GitHub, cerques web, ArXiv, HackerNews i un motor de recerca de *papers* personalitzat.
-   **Processament amb IA**: Utilitza LLMs (OpenAI, Gemini, Groq, Anthropic, Ollama) per a tasques avan√ßades com:
    -   Extracci√≥ intel¬∑ligent de noves paraules clau.
    -   Generaci√≥ de recomanacions din√†miques i contextuals.
    -   Traducci√≥ de termes de cerca per a consultes multiling√ºes.
-   **Arquitectura As√≠ncrona**: Constru√Øt amb `asyncio` per a una alta concurr√®ncia i efici√®ncia en operacions de xarxa i gesti√≥ de processos.
-   **Generaci√≥ d'Informes**: Crea autom√†ticament informes detallats en:
    -   **JSON**: Fitxers locals per a arxiu i an√†lisi.
    -   **Notion**: P√†gines estructurades i f√†cils de llegir al teu *workspace*.
    -   **Supabase**: Registres en una base de dades PostgreSQL per a persist√®ncia a llarg termini.
-   **Gesti√≥ de Keywords**: Mant√© un cicle de vida per a les paraules clau, amb un cat√†leg mestre, una llista activa per investigar i un historial d'execucions.
-   **Altament Configurable**: Tota la configuraci√≥ (claus d'API, selecci√≥ de models d'IA, rutes de fitxers) es gestiona mitjan√ßant un fitxer `.env` per facilitar la portabilitat i seguretat.
-   **Protocol Est√†ndard**: Utilitza el **Model Context Protocol (MCP)** per comunicar-se amb els servidors de cada plataforma, assegurant una interf√≠cie estandarditzada i desacoblada.

### üèõÔ∏è Arquitectura del Projecte

El sistema est√† dissenyat amb una arquitectura modular i desacoblada per facilitar el seu manteniment i extensi√≥.

-   `ai_trend_researcher.py`: L'orquestrador principal que executa el flux de recerca diari.
-   `config_manager.py`: Centralitza la c√†rrega i validaci√≥ de tota la configuraci√≥ des del fitxer `.env`.
-   `mcp_client_manager.py`: Gestiona el cicle de vida (connexi√≥, trucades, tancament) dels clients per als servidors MCP de cada plataforma.
-   `platform_handlers.py`: Cont√© la l√≤gica espec√≠fica per interactuar amb cada plataforma (ex. `YouTubeHandler`, `GitHubHandler`), processar les seves dades i estandarditzar-les.
-   `data_processor.py`: S'encarrega de l'an√†lisi de dades, l'extracci√≥ de paraules clau i la generaci√≥ de recomanacions utilitzant tant heur√≠stiques com LLMs.
-   `ai_client_manager.py`: Actua com una f√†brica per interactuar de manera unificada amb diferents prove√Ødors de models de llenguatge (OpenAI, Gemini, etc.).
-   `report_generator.py`: Genera els informes finals en tots els formats suportats (JSON, Notion, Supabase).
-   `keyword_manager.py`: Administra la base de dades de paraules clau en fitxers JSON.
-   `research_assistant.py`: Un script avan√ßat per realitzar immersions profundes de recerca utilitzant el servidor personalitzat `research_hub`.

### üõ†Ô∏è Instal¬∑laci√≥ i Configuraci√≥

Segueix aquests passos per posar en marxa el projecte al teu entorn local.

#### 1. Prerequisits

-   **Python 3.9+**
-   **Node.js i npm** (necessari per a `npx`, que executa els servidors MCP de la comunitat).
-   **Git**
-   (Opcional) **Compilador de Rust**, si vols compilar l'executable de `research_hub` des del codi font.

#### 2. Clonar el Repositori

```bash
git clone https://github.com/el_teu_usuari/el_teu_repositori.git
cd el_teu_repositori
```

#### 3. Instal¬∑lar Depend√®ncies de Python

Crea un entorn virtual (recomanat) i instal¬∑la les llibreries necess√†ries.

```bash
python -m venv venv
source venv/bin/activate  # A Windows: venv\Scripts\activate
pip install -r requirements.txt
```

#### 4. Configurar les Variables d'Entorn

Aquest √©s el pas m√©s important. El projecte es controla mitjan√ßant un fitxer `.env`.

1.  Copia el fitxer d'exemple:
    ```bash
    cp .env.example .env
    ```
2.  Edita el fitxer `.env` amb un editor de text i omple totes les claus d'API i rutes necess√†ries.

```env
# .env.example

# --- Claus d'API (Obligat√≤ries per a les plataformes que facis servir) ---
YOUTUBE_API_KEY=AIzaSy...
GITHUB_PERSONAL_ACCESS_TOKEN=ghp_...
NOTION_API_KEY=secret_...
SUPABASE_ACCESS_TOKEN=eyJhbGciOi...
SILICONFLOW_API_KEY= # Opcional, per al servidor d'ArXiv

# --- Configuraci√≥ de Notion (Obligat√≤ria si s'usa Notion) ---
NOTION_PARENT_PAGE_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# --- Configuraci√≥ de Rutes del Sistema (IMPORTANT!) ---
# Modifica aquestes rutes perqu√® apuntin a directoris a la teva m√†quina.
RESEARCH_PAPERS_DIR="/home/user/documents/research-papers"
RESEARCH_HUB_EXECUTABLE="/home/user/projectes/rust-research-mcp/target/release/rust-research-mcp"

# --- Configuraci√≥ del Prove√Ødor d'IA ---
# Tria'n un: "openai", "gemini", "groq", "anthropic", "ollama"
AI_PROVIDER="openai"

# Omple la clau d'API per al prove√Ødor que hagis triat.
OPENAI_API_KEY="sk-..."
GOOGLE_API_KEY="AIzaSy..."
GROQ_API_KEY="gsk_..."
ANTHROPIC_API_KEY="sk-ant-..."

# --- (Opcional) Models Espec√≠fics d'IA ---
# Si es deixen en blanc, s'utilitzaran els models per defecte.
AI_MODEL_OPENAI="gpt-4o"
AI_MODEL_GEMINI="gemini-1.5-flash"
AI_MODEL_GROQ="llama3-70b-8192"
AI_MODEL_ANTHROPIC="claude-3-haiku-20240307"
AI_MODEL_OLLAMA="llama3"
```

#### 5. Configurar la Base de Dades (Supabase)

Si planeges fer servir la integraci√≥ amb Supabase, assegura't que la teva taula a la base de dades coincideixi amb l'esquema definit a `supabase_schema.sql`. Pots executar aquest script a l'editor SQL del teu projecte de Supabase.

### ‚ñ∂Ô∏è √ös

Un cop configurat, pots executar els dos fluxos de treball principals.

#### Recerca Di√†ria de Tend√®ncies

Aquest √©s el flux principal. Executar√† la recerca a totes les plataformes habilitades, analitzar√† les dades i generar√† els informes.

```bash
python ai_trend_researcher.py
```

L'script imprimir√† el seu progr√©s a la consola. En finalitzar, trobar√†s l'informe JSON al directori `reports/` i, si est√† configurat, una nova p√†gina a Notion i un nou registre a la teva taula de Supabase.

#### Recerca d'Immersi√≥ Profunda

Aquest script utilitza el servidor `research_hub` per realitzar cerques avan√ßades de *papers*, descarregar-los, analitzar-los i generar bibliografies.

1.  Assegura't que l'executable `research_hub` estigui correctament configurat al teu `.env`.
2.  (Opcional) Afegeix termes de cerca al fitxer `terminos.txt`.

```bash
python research_assistant.py```

Els resultats d'aquesta execuci√≥ (CSVs, JSONs, fitxers BibTeX i logs) es desaran en subdirectoris dins de `salidas/` per mantenir cada execuci√≥ organitzada.

### ü§ù Contribucions

Les contribucions s√≥n benvingudes. Si tens idees per millorar el projecte, noves plataformes per integrar o trobes algun error, si us plau obre un *issue* o envia un *pull request*.

### üìÑ Llic√®ncia

Aquest projecte est√† sota la Llic√®ncia MIT. Consulta el fitxer `LICENSE` per a m√©s detalls.



---
File: /report_generator.py
---

# report_generator.py

# Importa el m√≥dulo 'json' para trabajar con datos JSON.
import json
# Importa el m√≥dulo 'os' para interactuar con el sistema de archivos (crear directorios y archivos).
import os
# Importa 'datetime' para obtener la fecha y hora actuales.
from datetime import datetime
# Importa herramientas de 'typing' para anotaciones de tipo.
from typing import Dict, List, Any, Optional
# Importa la clase 'RemoteMCPClient' para interactuar con los servidores de Notion y Supabase.
from mcp_client_manager import RemoteMCPClient


class JSONReportGenerator:
    """Genera informes de la investigaci√≥n en formato de archivo JSON."""
    
    def __init__(self, reports_dir: str = "reports"):
        """Constructor. Define el directorio donde se guardar√°n los informes."""
        self.reports_dir = reports_dir
    
    def generate_report(self, research_data: List[Dict], new_keywords: List[str], 
                       summary: Dict[str, Any], recommendations: List[str]) -> str:
        """Crea un archivo JSON con todos los datos de la investigaci√≥n."""
        today = datetime.now().strftime("%Y-%m-%d")
        # Define la estructura del informe.
        report = {
            "date": today,
            "summary": summary,
            "new_keywords": new_keywords,
            "recommendations": recommendations,
            "detailed_results": research_data
        }
        
        # Construye la ruta completa del archivo.
        report_file = os.path.join(self.reports_dir, f"ai_trends_{today}.json")
        # Asegura que el directorio de informes exista.
        os.makedirs(self.reports_dir, exist_ok=True)
        
        # Abre el archivo en modo escritura y vuelca el diccionario del informe como JSON.
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2) # indent=2 para formato legible.
        
        return report_file # Devuelve la ruta del archivo creado.


class NotionReportGenerator:
    """Genera informes como una nueva p√°gina en Notion."""
    
    def __init__(self, notion_client: Optional[RemoteMCPClient], parent_page_id: str):
        """Constructor. Necesita el cliente MCP de Notion y el ID de la p√°gina padre."""
        self.notion_client = notion_client
        self.parent_page_id = parent_page_id
    
    async def create_notion_report(self, report: Dict) -> Any:
        """Crea una p√°gina en Notion con el contenido del informe."""
        # Si no hay cliente o ID de p√°gina, no se puede crear el informe.
        if not self.notion_client or not self.parent_page_id:
            print("Aviso: Cliente de Notion o ID de p√°gina padre no disponible. Omitiendo informe de Notion.")
            return None
        
        try:
            today = report.get("date", datetime.now().strftime("%Y-%m-%d"))
            page_title = f"Informe de Tendencias IA - {today}"
            
            print("  - Creando bloques de contenido para Notion...")
            blocks = self._create_notion_blocks(report)
            
            # Valida la estructura de los bloques antes de enviarlos a la API de Notion.
            if not self._validate_blocks_structure(blocks):
                print("Error: La estructura de los bloques de Notion generados es inv√°lida. Omitiendo informe.")
                return None
            
            # Llama a la herramienta 'create-page' del servidor MCP de Notion.
            response = await self.notion_client.call_tool(
                "create-page",
                {
                    "parent_type": "page_id",
                    "parent_id": self.parent_page_id,
                    # Las propiedades (como el t√≠tulo) deben ser un string JSON.
                    "properties": json.dumps({
                        "title": {"title": [{"text": {"content": page_title}}]}
                    }),
                    # El contenido (los bloques) tambi√©n debe ser un string JSON.
                    "children": json.dumps(blocks)
                }
            )
            
            print(f"  ‚úì Informe de Notion creado: '{page_title}'")
            return response
            
        except Exception as e:
            print(f"  ‚úó Error al crear el informe de Notion: {e}")
            return None
    
    def _validate_blocks_structure(self, blocks: List[Dict]) -> bool:
        """Valida que la estructura b√°sica de los bloques sea correcta para la API de Notion."""
        if not isinstance(blocks, list): return False
        for block in blocks:
            if not isinstance(block, dict): return False
            if "object" not in block or block["object"] != "block": return False
            if "type" not in block: return False
            block_type = block["type"]
            if block_type not in block: return False
        return True
    
    def _create_rich_text(self, text: str) -> List[Dict]:
        """Funci√≥n de utilidad para crear un objeto 'rich_text' de Notion."""
        return [{"type": "text", "text": {"content": text}}]

    def _create_notion_blocks(self, report: Dict) -> List[Dict]:
        """Crea una lista detallada de bloques de contenido de Notion a partir de los datos del informe."""
        blocks = []
        
        # --- Resumen ---
        blocks.append({"object": "block", "type": "heading_2", "heading_2": {"rich_text": self._create_rich_text("üìä Resumen")}})
        summary = report.get("summary", {})
        summary_text = (
            f"‚Ä¢ Resultados Totales: {summary.get('total_items', 0)}\n"
            f"‚Ä¢ Nuevas Keywords: {summary.get('new_keywords_count', 0)}\n"
            f"‚Ä¢ Ejecuciones con Errores: {summary.get('runs_with_errors', 0)}"
        )
        blocks.append({"object": "block", "type": "paragraph", "paragraph": {"rich_text": self._create_rich_text(summary_text)}})
        
        # --- Recomendaciones ---
        blocks.append({"object": "block", "type": "heading_2", "heading_2": {"rich_text": self._create_rich_text("üìã Recomendaciones")}})
        recommendations = report.get("recommendations", [])
        if recommendations:
            for rec in recommendations:
                if rec and isinstance(rec, str) and rec.strip():
                    blocks.append({"object": "block", "type": "bulleted_list_item", "bulleted_list_item": {"rich_text": self._create_rich_text(rec.strip())}})
        else:
            blocks.append({"object": "block", "type": "paragraph", "paragraph": {"rich_text": self._create_rich_text("No se generaron recomendaciones.")}})
        
        # --- Nuevas Keywords ---
        blocks.append({"object": "block", "type": "heading_2", "heading_2": {"rich_text": self._create_rich_text("üîç Nuevas Keywords Descubiertas")}})
        new_keywords = report.get("new_keywords", [])
        if new_keywords:
            blocks.append({"object": "block", "type": "paragraph", "paragraph": {"rich_text": self._create_rich_text(", ".join(new_keywords))}})
        else:
            blocks.append({"object": "block", "type": "paragraph", "paragraph": {"rich_text": self._create_rich_text("No se encontraron nuevas keywords.")}})

        # --- Resultados Detallados ---
        blocks.append({"object": "block", "type": "heading_2", "heading_2": {"rich_text": self._create_rich_text("üî¨ Resultados Detallados por Plataforma")}})
        platform_results = self._group_results_by_platform(report.get("detailed_results", []))
        
        for platform, results in platform_results.items():
            if not results: continue
            blocks.append({"object": "block", "type": "heading_3", "heading_3": {"rich_text": self._create_rich_text(f" ‡§™‡•ç‡§≤‡•á‡§ü‡§´‡•â‡§∞‡•ç‡§Æ: {platform.upper()}")}})
            for i, result in enumerate(results[:3], 1): # Limita a los 3 primeros resultados para ser conciso.
                title = str(result.get('title', result.get('name', 'Sin t√≠tulo'))).strip()
                url = result.get('url', '')
                snippet = (result.get('description', result.get('snippet', '')) or "")[:250].strip()
                
                if not title: continue
                
                toggle_block = {
                    "object": "block",
                    "type": "toggle",
                    "toggle": {
                        "rich_text": self._create_rich_text(f"{i}. {title}"),
                        "children": []
                    }
                }
                
                if url:
                    toggle_block["toggle"]["children"].append({"object": "block", "type": "paragraph", "paragraph": {"rich_text": self._create_rich_text(f"üîó URL: {url}")}})
                if snippet:
                     toggle_block["toggle"]["children"].append({"object": "block", "type": "paragraph", "paragraph": {"rich_text": self._create_rich_text(f"üìù Extracto: {snippet}...")}})
                
                blocks.append(toggle_block)
        
        return blocks
    
    def _group_results_by_platform(self, detailed_results: List[Dict]) -> Dict[str, List[Dict]]:
        """Agrupa una lista de resultados en un diccionario por plataforma."""
        platform_results = {}
        for data in detailed_results:
            platform = data.get("platform", "unknown")
            if not data.get("error") and data.get("results"):
                if platform not in platform_results:
                    platform_results[platform] = []
                platform_results[platform].extend(data["results"])
        return platform_results


class SupabaseReportGenerator:
    """Genera informes guardando los datos en una tabla de Supabase."""
    
    def __init__(self, supabase_client: Optional[RemoteMCPClient]):
        """Constructor. Necesita el cliente MCP de Supabase."""
        self.supabase_client = supabase_client
    
    async def create_supabase_report(self, report: Dict) -> Any:
        """Inserta los datos del informe en una tabla de la base de datos Supabase."""
        if not self.supabase_client:
            print("Aviso: Cliente de Supabase no disponible. Omitiendo informe de Supabase.")
            return None
        
        try:
            today = report.get("date", datetime.now().strftime("%Y-%m-%d"))
            
            # Define la consulta SQL para insertar los datos.
            # ADVERTENCIA: Este m√©todo formatea una cadena SQL. Es seguro en este contexto
            # porque los datos son generados por la propia aplicaci√≥n, pero para datos
            # externos, se deben usar consultas parametrizadas si el servidor MCP las soporta.
            sql = """
            INSERT INTO ai_trend_reports (date, summary, detailed_results, new_keywords, recommendations)
            VALUES ('{date}', '{summary}', '{detailed_results}', '{new_keywords}', '{recommendations}')
            RETURNING id;
            """
            
            # Prepara los par√°metros, convirtiendo los diccionarios/listas a strings JSON
            # y escapando comillas simples para evitar errores de sintaxis SQL.
            params = {
                "date": today,
                "summary": json.dumps(report.get("summary", {}), ensure_ascii=False).replace("'", "''"),
                "detailed_results": json.dumps(report.get("detailed_results", []), ensure_ascii=False).replace("'", "''"),
                "new_keywords": json.dumps(report.get("new_keywords", []), ensure_ascii=False).replace("'", "''"),
                "recommendations": json.dumps(report.get("recommendations", []), ensure_ascii=False).replace("'", "''"),
            }
            # Formatea la consulta SQL con los par√°metros.
            query = sql.format(**params)

            # Llama a la herramienta 'execute_sql' del servidor MCP de Supabase.
            response = await self.supabase_client.call_tool("execute_sql", {"query": query})
            
            print(f"  ‚úì Informe de Supabase creado para la fecha: {today}")
            return response
            
        except Exception as e:
            print(f"  ‚úó Error al crear el informe de Supabase: {e}")
            return None


class ReportManager:
    """Clase de alto nivel que gestiona la generaci√≥n de todos los tipos de informes."""
    
    def __init__(self, reports_dir: str = "reports", notion_client: Optional[RemoteMCPClient] = None, 
                 notion_parent_id: Optional[str] = None, supabase_client: Optional[RemoteMCPClient] = None):
        """Constructor. Inicializa todos los generadores de informes necesarios."""
        self.json_generator = JSONReportGenerator(reports_dir)
        self.notion_generator = NotionReportGenerator(notion_client, notion_parent_id)
        self.supabase_generator = SupabaseReportGenerator(supabase_client)
    
    async def generate_all_reports(self, research_data: List[Dict], new_keywords: List[str],
                                 summary: Dict[str, Any], recommendations: List[str]) -> str:
        """Orquesta la generaci√≥n de informes en JSON, Notion y Supabase."""
        # Primero, siempre genera el informe JSON local.
        report_file = self.json_generator.generate_report(
            research_data, new_keywords, summary, recommendations
        )
        
        # Prepara un diccionario unificado con los datos del informe.
        report_data = {
            "date": datetime.now().strftime("%Y-%m-%d"),
            "summary": summary,
            "detailed_results": research_data,
            "new_keywords": new_keywords,
            "recommendations": recommendations
        }
        
        # Genera los informes de Notion y Supabase en paralelo si est√°n disponibles.
        tasks = []
        if self.notion_generator:
            tasks.append(self.notion_generator.create_notion_report(report_data))
        if self.supabase_generator:
            tasks.append(self.supabase_generator.create_supabase_report(report_data))
            
        if tasks:
            await asyncio.gather(*tasks)
        
        return report_file # Devuelve la ruta del archivo JSON como confirmaci√≥n.



---
File: /requirement.txt
---

# Biblioteca oficial para interactuar con los modelos de IA de Anthropic (Claude).
anthropic

# Biblioteca para el protocolo MCP (Model Context Protocol), que estandariza la comunicaci√≥n con herramientas externas.
mcp

# Biblioteca est√°ndar en Python para realizar peticiones HTTP a APIs y servicios web.
requests

# Biblioteca para cargar las variables de entorno definidas en el archivo .env en el script de Python.
python-dotenv

# --- Bibliotecas de IA a√±adidas ---

# Biblioteca oficial de Google para interactuar con los modelos de IA Gemini.
google-generativeai

# Biblioteca oficial para interactuar con la API de Groq, que ofrece inferencia r√°pida de LLMs.
groq

# Biblioteca para interactuar con Ollama, que permite ejecutar modelos de IA localmente.
ollama

# Biblioteca oficial para interactuar con los modelos de IA de OpenAI (GPT).
openai



---
File: /requirements.txt
---

# Biblioteca oficial para interactuar con los modelos de IA de Anthropic (Claude).
anthropic

# Biblioteca para el protocolo MCP (Model Context Protocol), que estandariza la comunicaci√≥n con herramientas externas.
mcp

# Biblioteca est√°ndar en Python para realizar peticiones HTTP a APIs y servicios web.
requests

# Biblioteca para cargar las variables de entorno definidas en el archivo .env en el script de Python.
python-dotenv

# --- Bibliotecas de IA a√±adidas ---

# Biblioteca oficial de Google para interactuar con los modelos de IA Gemini.
google-generativeai

# Biblioteca oficial para interactuar con la API de Groq, que ofrece inferencia r√°pida de LLMs.
groq

# Biblioteca para interactuar con Ollama, que permite ejecutar modelos de IA localmente.
ollama

# Biblioteca oficial para interactuar con los modelos de IA de OpenAI (GPT).
openai



---
File: /research_assistant_con_hackers_LLM.py
---

# research_assistant_con_hackers_LLM.py
# ============================================================
# Flujo de investigaci√≥n con rust-research-mcp (MCP server)
# 1. Lee 'terminos.txt' y construye topics (MAX_TOPICS).
# 2. Busca papers (search_papers) para DOIs y metadatos.
# 3. Descarga PDFs (download_paper) con control de paralelismo.
# 4. Genera bibliograf√≠a (BibTeX) desde los metadatos recolectados.
# 5. Busca discusiones en Hacker News por cada topic y filtra con un LLM.
# ============================================================

from __future__ import annotations
import asyncio
import json
import os
import re
from datetime import datetime
from typing import Any, Iterable, List, Dict, Optional

from dotenv import load_dotenv
import aiofiles
import aiofiles.os

# M√≥dulos del proyecto
from config_manager import ServerConfig, AppConfig
from mcp_client_manager import MCPClientManager, RemoteMCPClient
from ai_client_manager import AIClientManager

load_dotenv()

# ============================================================
# üîß PAR√ÅMETROS CONFIGURABLES (env-first)
# ============================================================

def env_bool(name: str, default: bool) -> bool:
    val = os.getenv(name, str(default)).strip().lower()
    return val in ("1", "true", "t", "yes", "y", "on")

def env_int(name: str, default: int) -> int:
    try:
        v = os.getenv(name)
        return int(v) if v is not None else default
    except ValueError:
        return default

# ‚Äî‚Äî‚Äî Control de salidas ‚Äî‚Äî‚Äî
SEPARATE_RUNS_IN_SUBFOLDER: bool = env_bool("SEPARATE_RUNS_IN_SUBFOLDER", True)
RUN_TAG: Optional[str] = os.getenv("RUN_TAG")

# ‚Äî‚Äî‚Äî B√∫squeda y filtrado ‚Äî‚Äî‚Äî
MAX_TOPICS: int = env_int("MAX_TOPICS", 10)
MAX_RESULTS_PER_TOPIC: int = env_int("MAX_RESULTS_PER_TOPIC", 10)
MAX_HN_RESULTS_PER_TOPIC: int = env_int("MAX_HN_RESULTS_PER_TOPIC", 15)


# ‚Äî‚Äî‚Äî Descarga de PDFs ‚Äî‚Äî‚Äî
DOWNLOAD_ALL_PAPERS: bool = env_bool("DOWNLOAD_ALL_PAPERS", False)
SELECT_TOP_K: int = env_int("SELECT_TOP_K", 5)
MAX_PARALLEL_DOWNLOADS: int = env_int("MAX_PARALLEL_DOWNLOADS", 4)

# ‚Äî‚Äî‚Äî Timeouts/reintentos MCP ‚Äî‚Äî‚Äî
MCP_INIT_RETRIES: int = env_int("MCP_INIT_RETRIES", 1)
RESEARCH_HUB_INIT_TIMEOUT: int = env_int("RESEARCH_HUB_INIT_TIMEOUT", 45)
HACKERNEWS_INIT_TIMEOUT: int = env_int("HACKERNEWS_INIT_TIMEOUT", 15)

# ============================================================
# Constantes y Utilidades
# ============================================================
DOI_REGEX = re.compile(r'^10\.\d{4,9}/.+$')

def slugify(s: str) -> str:
    """Convierte un string en un formato seguro para nombres de archivo."""
    s = str(s).lower().strip()
    s = re.sub(r'[\s\W-]+', '-', s)
    s = s.strip('-')
    return s[:75] if len(s) > 75 else s

def now_str() -> str:
    """Devuelve la fecha y hora actual como un string formateado."""
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def extract_text(blobs: Optional[Iterable[Any]]) -> str:
    """Extrae y une el contenido de texto de una respuesta MCP."""
    if not blobs: 
        return ""
    parts: List[str] = []
    for b in blobs:
        if hasattr(b, "text"):
            parts.append(getattr(b, "text", "") or "")
        elif isinstance(b, str):
            parts.append(b)
    return "\n".join(parts).strip()

def parse_text_response_to_papers(raw_text: str, topic: str) -> List[Dict[str, Any]]:
    """Parsea la respuesta de texto del servidor Rust para la b√∫squeda de papers."""
    papers = []
    if not raw_text:
        return papers

    content = raw_text.split('\n\n', 1)[-1]
    paper_blocks = re.split(r'\n\n(?=\d+\.\s)', content)

    for block in paper_blocks:
        if not block.strip():
            continue
        paper_data = {"topic": topic, "title": None, "doi": None, "source": None, "year": None, "authors": None, "journal": None}
        
        # Extracci√≥n de campos con expresiones regulares
        title_match = re.search(r'^\d+\.\s*(.*?)\s*\(Relevance:', block)
        if title_match: paper_data['title'] = title_match.group(1).strip()
        
        doi_match = re.search(r'üìñ\s*DOI:\s*(.*)', block)
        if doi_match:
            doi_str = doi_match.group(1).strip()
            paper_data['doi'] = doi_str if doi_str and " " not in doi_str else None

        source_match = re.search(r'üîç\s*Source:\s*(.*)', block)
        if source_match: paper_data['source'] = source_match.group(1).strip()
            
        year_match = re.search(r'üìÖ\s*Year:\s*(\d{4})', block)
        if year_match: paper_data['year'] = int(year_match.group(1).strip())

        authors_match = re.search(r'üë§\s*Authors:\s*(.*)', block)
        if authors_match: paper_data['authors'] = authors_match.group(1).strip()

        journal_match = re.search(r'Journal:\s*(.*)', block)
        if journal_match: paper_data['journal'] = journal_match.group(1).strip()
            
        if paper_data.get('title'):
            paper_data['url'] = f"https://doi.org/{paper_data['doi']}" if paper_data.get('doi') else None
            paper_data['is_valid_doi'] = bool(paper_data['doi'] and DOI_REGEX.match(paper_data['doi']))
            papers.append(paper_data)
            
    return papers

class AdvancedResearchAssistant:
    """Orquesta el flujo de investigaci√≥n avanzada, gestionando clientes, archivos y l√≥gica de negocio."""
    
    def __init__(self, topics: List[str]):
        """Inicializa el asistente con los temas de investigaci√≥n."""
        self.topics = topics
        self.mcp_manager: MCPClientManager = None
        self.ai_client: Optional[AIClientManager] = None
        self.rh_client: Optional[RemoteMCPClient] = None
        self.hn_client: Optional[RemoteMCPClient] = None

        # Directorios de salida (inicializados como None)
        self.salidas_dir = self.csv_dir = self.logs_dir = self.bib_dir = self.json_dir = None
        self.downloads_dir = os.getenv("RESEARCH_PAPERS_DIR", os.path.join(os.getcwd(), "ResearchPapers"))

    async def setup_output_dirs(self):
        """Configura los directorios de salida para esta ejecuci√≥n."""
        base = "salidas"
        if SEPARATE_RUNS_IN_SUBFOLDER:
            tag = RUN_TAG or datetime.now().strftime("%Y%m%d_%H%M%S")
            base = os.path.join(base, tag)
        
        self.salidas_dir = base
        self.csv_dir = os.path.join(base, "csv")
        self.logs_dir = os.path.join(base, "logs")
        self.bib_dir = os.path.join(base, "bib")
        self.json_dir = os.path.join(base, "json")
        
        for d in (self.salidas_dir, self.csv_dir, self.logs_dir, self.bib_dir, self.json_dir, self.downloads_dir):
            await aiofiles.os.makedirs(d, exist_ok=True)
        print(f"üìÇ Salidas se guardar√°n en: {self.salidas_dir}")
        print(f"üì• PDFs se guardar√°n en: {self.downloads_dir}")

    async def _save_json(self, name: str, data: Any):
        """Guarda datos en un archivo JSON en el directorio de salida."""
        path = os.path.join(self.json_dir, name)
        async with aiofiles.open(path, "w", encoding="utf-8") as f:
            await f.write(json.dumps(data, ensure_ascii=False, indent=2))
        print(f"üíæ JSON guardado: {os.path.basename(path)}")
    
    async def _save_csv(self, name: str, rows: List[Dict[str, Any]]):
        """Guarda una lista de diccionarios en un archivo CSV."""
        if not rows: return
        path = os.path.join(self.csv_dir, name)
        async with aiofiles.open(path, "w", encoding="utf-8-sig", newline="") as f:
            fields = list(rows[0].keys())
            await f.write(",".join(fields) + "\n")
            for row in rows:
                values = [f"\"{str(row.get(k, '')).replace('\"', '\"\"')}\"" for k in fields]
                await f.write(",".join(values) + "\n")
        print(f"üíæ CSV guardado: {os.path.basename(path)} ({len(rows)} filas)")

    async def _save_bib(self, name: str, content: str):
        """Guarda contenido en un archivo BibTeX."""
        path = os.path.join(self.bib_dir, name)
        async with aiofiles.open(path, "w", encoding="utf-8") as f:
            await f.write(content)
        print(f"üìö Bibliograf√≠a guardada: {os.path.basename(path)}")

    async def _save_log(self, name: str, content: str):
        """Guarda texto en un archivo de registro."""
        path = os.path.join(self.logs_dir, name)
        async with aiofiles.open(path, "w", encoding="utf-8") as f:
            await f.write(content)
        print(f"üìú Log de error guardado: {os.path.basename(path)}")

    async def _connect_with_retries(self, name: str, cfg: Dict[str, Any], timeout: int, retries: int) -> bool:
        """Intenta conectar a un servidor MCP con reintentos."""
        delay = 2.0
        for attempt in range(retries + 1):
            try:
                print(f"[{now_str()}] [MCP] Conectando a '{name}' (intento {attempt+1}/{retries+1}) | timeout={timeout}s")
                await asyncio.wait_for(self.mcp_manager._connect_single_server(name, cfg), timeout=timeout)
                if self.mcp_manager.is_platform_available(name):
                    print(f"  [√âXITO] Conexi√≥n establecida con '{name}'")
                    return True
            except Exception as e:
                err = f"Timeout en initialize() para '{name}'" if isinstance(e, asyncio.TimeoutError) else str(e)
            print(f"  [FALLO] Error conectando a '{name}': {err}")
            if attempt < retries:
                await asyncio.sleep(delay)
                delay *= 2
        print(f"  [FALLO] Conexi√≥n definitiva fallida con '{name}'")
        return False

    async def connect_services(self) -> bool:
        """Inicializa y conecta a todos los servicios externos (MCP, IA)."""
        # 1. Conectar servidores MCP
        server_configs = ServerConfig.get_server_configs()
        self.mcp_manager = MCPClientManager(server_configs)
        
        print("\nüîó Conectando a servidores MCP‚Ä¶")
        rh_cfg = server_configs.get("research_hub", {})
        hn_cfg = server_configs.get("hackernews", {})

        rh_ok, hn_ok = await asyncio.gather(
            self._connect_with_retries("research_hub", rh_cfg, RESEARCH_HUB_INIT_TIMEOUT, MCP_INIT_RETRIES),
            self._connect_with_retries("hackernews", hn_cfg, HACKERNEWS_INIT_TIMEOUT, MCP_INIT_RETRIES),
        )
        self.rh_client = self.mcp_manager.get_client("research_hub") if rh_ok else None
        self.hn_client = self.mcp_manager.get_client("hackernews") if hn_ok else None

        # 2. Inicializar cliente de IA
        try:
            ai_provider = AppConfig.get_ai_provider()
            api_key = AppConfig.get_api_key(ai_provider)
            ai_model = AppConfig.get_ai_model(ai_provider)
            if ai_provider != "ollama" and not api_key:
                print(f"‚ö†Ô∏è  [OMITIDO] No hay API Key para {ai_provider}. Se omitir√° el filtrado con LLM.")
            else:
                self.ai_client = AIClientManager(provider=ai_provider, api_key=api_key, model=ai_model)
                print(f"‚úÖ Cliente IA inicializado: {ai_provider.upper()} (modelo: {ai_model or 'default'})")
        except Exception as e:
            print(f"‚ö†Ô∏è  [FALLO] No se pudo inicializar el cliente de IA: {e}. Se omitir√° el filtrado con LLM.")
        
        return self.rh_client is not None or self.hn_client is not None

    async def _search_papers(self) -> List[Dict[str, Any]]:
        """Paso A: Busca papers para cada topic y los devuelve combinados y deduplicados."""
        print("\n--- PASO A: B√∫squeda de Papers Acad√©micos ---")
        if not self.rh_client:
            print("  [OMITIDO] El cliente de Research Hub no est√° disponible.")
            return []
        
        all_papers: List[Dict[str, Any]] = []
        for topic in self.topics:
            print(f"  -> Buscando topic: '{topic}'...")
            try:
                res = await self.rh_client.call_tool("search_papers", {"query": topic, "limit": MAX_RESULTS_PER_TOPIC})
                papers = parse_text_response_to_papers(extract_text(res), topic)
                all_papers.extend(papers)
                print(f"     Encontrados {len(papers)} resultados.")
            except Exception as e:
                print(f"     [FALLO] Error buscando topic '{topic}': {e}")
        
        # Deduplicado por DOI
        seen_dois = set()
        unique_papers = [p for p in all_papers if p.get("doi") not in seen_dois and not seen_dois.add(p.get("doi"))]
        
        print(f"\n‚ú® [√âXITO] Total de papers √∫nicos encontrados: {len(unique_papers)}")
        return unique_papers

    async def _select_and_download_papers(self, papers: List[Dict[str, Any]]):
        """Pasos B y C: Selecciona DOIs para descargar y ejecuta la descarga en paralelo."""
        print("\n--- PASOS B & C: Selecci√≥n y Descarga de PDFs ---")
        if not self.rh_client:
            print("  [OMITIDO] El cliente de Research Hub no est√° disponible.")
            return

        # Paso B: Selecci√≥n
        selected_dois = [p['doi'] for p in papers if p.get('is_valid_doi')]
        if not DOWNLOAD_ALL_PAPERS:
            selected_dois = selected_dois[:SELECT_TOP_K]
            print(f"  -> Selecci√≥n: TOP {SELECT_TOP_K} papers ({len(selected_dois)} con DOI v√°lido).")
        else:
            print(f"  -> Selecci√≥n: TODOS los papers ({len(selected_dois)} con DOI v√°lido).")
        
        if not selected_dois:
            print("  [AVISO] No hay DOIs v√°lidos para descargar.")
            return

        await self._save_json("02_dois_seleccionados.json", {"dois": selected_dois})

        # Paso C: Descarga
        print(f"  -> Descargando {len(selected_dois)} papers en paralelo (max {MAX_PARALLEL_DOWNLOADS})...")
        semaphore = asyncio.Semaphore(MAX_PARALLEL_DOWNLOADS)
        doi_map = {p['doi']: p for p in papers if p.get('doi')}

        async def _download_one(doi: str):
            async with semaphore:
                title = doi_map.get(doi, {}).get('title', 'untitled')
                filename = f"{slugify(title)}_{slugify(doi)}.pdf"
                try:
                    res = await self.rh_client.call_tool("download_paper", {"doi": doi, "filename": filename})
                    raw_text = extract_text(res)
                    if "Download successful!" in raw_text or "File already exists" in raw_text:
                        print(f"     ‚úì Descargado (o ya exist√≠a): {doi}")
                        return doi, {"status": "ok", "title": title}
                    print(f"     ‚úó Fallo en descarga: {doi}")
                    return doi, {"status": "failed", "reason": raw_text.strip()}
                except Exception as e:
                    print(f"     ‚úó Error grave durante la descarga de {doi}: {e}")
                    return doi, {"status": "error", "reason": str(e)}

        tasks = [_download_one(doi) for doi in selected_dois]
        results = await asyncio.gather(*tasks)
        download_manifest = {doi: result for doi, result in results}
        await self._save_json("03_manifiesto_descarga.json", download_manifest)
        print("‚ú® [√âXITO] Proceso de descarga completado.")

    def _paper_to_bibtex(self, paper: Dict[str, Any]) -> str:
        """Convierte un diccionario de metadatos de un paper a una entrada BibTeX."""
        author_lastname = "unknown"
        if paper.get("authors"):
            try: author_lastname = slugify(paper["authors"].split(',')[0].split(' ')[-1])
            except: pass
        
        year_str = str(paper.get('year', 'nodate'))
        title_slug = slugify(paper.get('title', 'notitle')[:10])
        key = f"{author_lastname}{year_str}{title_slug}"

        entry = f"@article{{{key},\n"
        if paper.get('title'): entry += f"  title     = {{{{{paper['title']}}}}},\n"
        if paper.get('authors'): entry += f"  author    = {{{paper['authors']}}},\n"
        if paper.get('year'): entry += f"  year      = {{{paper['year']}}},\n"
        if paper.get('journal'): entry += f"  journal   = {{{paper['journal']}}},\n"
        if paper.get('doi'): entry += f"  doi       = {{{paper['doi']}}},\n"
        entry += "}"
        return entry
        
    async def _generate_bibliography(self, papers: List[Dict[str, Any]]):
        """Paso D: Genera una bibliograf√≠a completa a partir de los metadatos recolectados."""
        print("\n--- PASO D: Generaci√≥n de Bibliograf√≠a ---")
        papers_with_doi = [p for p in papers if p.get('is_valid_doi')]
        if not papers_with_doi:
            print("  [OMITIDO] No se encontraron papers con DOI v√°lido para generar bibliograf√≠a.")
            return

        print(f"  -> Usando metadatos de {len(papers_with_doi)} papers.")
        bib_entries = [self._paper_to_bibtex(p) for p in papers_with_doi]
        await self._save_bib("bibliografia_final.bib", "\n\n".join(bib_entries))
        print("‚ú® [√âXITO] Bibliograf√≠a generada.")

    async def _search_hackernews(self) -> Dict[str, Any]:
        """
        Paso E: Busca en Hacker News para cada topic.
        MEJORA: Maneja expl√≠citamente errores de JSON y guarda la respuesta cruda.
        """
        print("\n--- PASO E: B√∫squeda en Hacker News ---")
        if not self.hn_client:
            print("  [OMITIDO] El cliente de Hacker News no est√° disponible.")
            return {}

        all_results = {}
        for topic in self.topics:
            print(f"  -> Buscando en HN: '{topic}'...")
            raw_response_text = ""
            try:
                params = {"query": topic, "max_results": MAX_HN_RESULTS_PER_TOPIC}
                response = await self.hn_client.call_tool("getStories", params)
                
                stories = []
                # El problema puede estar aqu√≠: la respuesta puede ser un texto vac√≠o o un error HTML
                raw_response_text = extract_text(response)
                if not raw_response_text.strip():
                     print(f"     [AVISO] Respuesta vac√≠a del servidor para '{topic}'.")
                     all_results[topic] = []
                     continue

                # Intentamos decodificar el JSON
                response_json = json.loads(raw_response_text)
                
                # Asumimos que la respuesta es una lista de historias (o un dict con 'hits')
                story_items = response_json if isinstance(response_json, list) else response_json.get('hits', [])

                all_results[topic] = story_items
                print(f"     Encontrados {len(story_items)} resultados.")
            
            except json.JSONDecodeError as e:
                log_filename = f"hackernews_error_response_{slugify(topic)}.log"
                error_msg = f"     [FALLO] El servidor HN no devolvi√≥ un JSON v√°lido para '{topic}'. Error: {e}"
                print(error_msg)
                await self._save_log(log_filename, f"{error_msg}\n\n--- RESPUESTA RECIBIDA ---\n{raw_response_text}")
                all_results[topic] = []
            except Exception as e:
                print(f"     [FALLO] Error inesperado buscando en HN para '{topic}': {e}")
                all_results[topic] = []
        
        print("‚ú® [√âXITO] B√∫squeda en Hacker News completada.")
        return all_results

    async def _filter_with_llm(self, hn_results: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """Paso F: Usa un LLM para validar la relevancia de los resultados de Hacker News."""
        print("\n--- PASO F: Filtrado de HN con LLM ---")
        if not self.ai_client:
            print("  [OMITIDO] Cliente de IA no disponible.")
            return hn_results
        
        final_results = {}
        for topic, stories in hn_results.items():
            if not stories:
                final_results[topic] = []
                continue

            print(f"  -> Pidiendo al LLM que valide {len(stories)} historias para: '{topic}'...")
            titles_str = "\n".join([f"{i+1}. {s.get('title', 'N/A')}" for i, s in enumerate(stories)])
            
            prompt = (
                f"Identify which of the following Hacker News titles are relevant to the research topic: '{topic}'.\n\n"
                "A title is relevant if it discusses the topic directly, not if it just uses some of the same words in a different context.\n\n"
                f"Candidate Titles:\n{titles_str}\n\n"
                "Return a JSON object with a single key 'relevant_indices', a list of the numbers of relevant titles. Example: {\"relevant_indices\": [1, 4, 5]}.\n"
                "If none are relevant, return an empty list. Respond ONLY with the JSON object."
            )

            try:
                response_text = await self.ai_client.chat_completion(prompt)
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                response_json = json.loads(json_match.group(0)) if json_match else {}
                relevant_indices = response_json.get("relevant_indices", [])
                
                filtered = [stories[i-1] for i in relevant_indices if 0 < i <= len(stories)]
                final_results[topic] = filtered
                print(f"     LLM identific√≥ {len(filtered)} historias relevantes.")
            except Exception as e:
                print(f"     [FALLO] Error con el LLM para '{topic}': {e}. Se mantienen resultados sin filtrar.")
                final_results[topic] = stories
        
        print("‚ú® [√âXITO] Filtrado con LLM completado.")
        return final_results

    async def run(self):
        """Ejecuta el flujo completo de investigaci√≥n."""
        print(f"\n[{now_str()}] üöÄ Iniciando Asistente de Investigaci√≥n‚Ä¶")
        await self.setup_output_dirs()
        
        if not await self.connect_services():
            print("\n‚ùå [ERROR] No se pudo conectar a los servicios necesarios. Abortando.")
            return

        try:
            # Flujo de Research Hub
            found_papers = await self._search_papers()
            if found_papers:
                await self._save_json("01_papers_encontrados.json", found_papers)
                await self._save_csv("01_papers_encontrados.csv", found_papers)
                await self._select_and_download_papers(found_papers)
                await self._generate_bibliography(found_papers)
            
            # Flujo de Hacker News
            hackernews_results = await self._search_hackernews()
            await self._save_json("04_hackernews_raw.json", hackernews_results)
            
            final_hn_results = await self._filter_with_llm(hackernews_results)
            await self._save_json("05_hackernews_filtrado_llm.json", final_hn_results)
            
            print(f"\nüéâ [{now_str()}] Proceso completado con √©xito.")

        finally:
            print("\nüèÅ Finalizando y cerrando conexiones‚Ä¶")
            if self.mcp_manager:
                await self.mcp_manager.close_all_clients()
                print("   [√âXITO] Conexiones MCP cerradas.")

async def main():
    """Punto de entrada principal del script."""
    try:
        async with aiofiles.open("terminos.txt", "r", encoding="utf-8") as f:
            terminos = [t.strip() for t in (await f.read()).splitlines() if t.strip()]
        if not terminos:
            print("‚ö†Ô∏è  'terminos.txt' est√° vac√≠o. No hay nada que procesar.")
            return
    except FileNotFoundError:
        print("‚ùå ERROR: No se encontr√≥ 'terminos.txt'. Por favor, crea el archivo con los temas a investigar.")
        return
    
    topics = terminos[:MAX_TOPICS]
    print(f"‚úÖ Temas a investigar ({len(topics)}): {topics}")
    
    assistant = AdvancedResearchAssistant(topics)
    await assistant.run()

if __name__ == "__main__":
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())


---
File: /research_assistant.py
---

# research_assistant.py
# ============================================================
# Flujo de investigaci√≥n con rust-research-mcp (MCP server)
# 1. Busca papers por temas para descubrir DOIs.
# 2. Descarga los papers seleccionados.
# 3. Vuelve a consultar cada DOI para obtener metadatos enriquecidos (autores, etc.).
# 4. Genera una bibliograf√≠a completa con los datos enriquecidos.
# ============================================================

import asyncio
import json
import os
import re
import csv
from datetime import datetime
from typing import Any, Iterable, List, Dict, Optional

from dotenv import load_dotenv
import aiofiles
import aiofiles.os

# M√≥dulos del proyecto
from config_manager import ServerConfig
from mcp_client_manager import MCPClientManager, RemoteMCPClient

load_dotenv()

# ============================================================
# üîß PAR√ÅMETROS CONFIGURABLES
# ============================================================

def env_bool(name: str, default: bool) -> bool:
    val = os.getenv(name, str(default)).strip().lower()
    return val in ("1", "true", "t", "yes", "y", "on")

def env_int(name: str, default: int) -> int:
    val = os.getenv(name)
    return int(val) if val and val.isdigit() else default

# ‚Äî‚Äî‚Äî Control de salidas ‚Äî‚Äî‚Äî
SEPARATE_RUNS_IN_SUBFOLDER: bool = env_bool("SEPARATE_RUNS_IN_SUBFOLDER", True)
RUN_TAG: Optional[str] = os.getenv("RUN_TAG")

# ‚Äî‚Äî‚Äî B√∫squeda y filtrado ‚Äî‚Äî‚Äî
MAX_TOPICS: int = env_int("MAX_TOPICS", 10)
MAX_RESULTS_PER_TOPIC: int = env_int("MAX_RESULTS_PER_TOPIC", 10)

# ‚Äî‚Äî‚Äî Descarga de PDFs ‚Äî‚Äî‚Äî
DOWNLOAD_ALL_PAPERS: bool = env_bool("DOWNLOAD_ALL_PAPERS", False)
SELECT_TOP_K: int = env_int("SELECT_TOP_K", 5)
MAX_PARALLEL_DOWNLOADS: int = env_int("MAX_PARALLEL_DOWNLOADS", 4)

# ============================================================
# Constantes y Utilidades
# ============================================================
DOI_REGEX = re.compile(r'^10\.\d{4,9}/.+$')

# Directorios de salida globales
SALIDAS_DIR, CSV_DIR, LOGS_DIR, BIB_DIR, JSON_DIR = "", "", "", "", ""
DOWNLOADS_DIR = ""

async def setup_output_dirs() -> None:
    """Configura los directorios de salida y descarga de forma as√≠ncrona."""
    global SALIDAS_DIR, CSV_DIR, LOGS_DIR, BIB_DIR, JSON_DIR, DOWNLOADS_DIR
    
    DOWNLOADS_DIR = os.getenv("RESEARCH_PAPERS_DIR", os.path.join(os.getcwd(), "ResearchPapers"))
    
    base = "salidas"
    if SEPARATE_RUNS_IN_SUBFOLDER:
        tag = RUN_TAG or datetime.now().strftime("%Y%m%d_%H%M%S")
        base = os.path.join(base, tag)
    
    SALIDAS_DIR = base
    CSV_DIR, LOGS_DIR, BIB_DIR, JSON_DIR = (os.path.join(base, d) for d in ["csv", "logs", "bib", "json"])
    
    for d in (base, CSV_DIR, LOGS_DIR, BIB_DIR, JSON_DIR, DOWNLOADS_DIR):
        await aiofiles.os.makedirs(d, exist_ok=True)

def slugify(s: str) -> str:
    s = str(s).lower().strip()
    s = re.sub(r'[\s\W-]+', '-', s)
    s = s.strip('-')
    return s[:75] if len(s) > 75 else s

async def guardar_csv(nombre: str, rows: List[Dict[str, Any]]) -> None:
    path = os.path.join(CSV_DIR, nombre)
    if not rows: return
    async with aiofiles.open(path, "w", encoding="utf-8-sig", newline="") as f:
        fields = list(rows[0].keys())
        await f.write(",".join(fields) + "\n")
        for row in rows:
            values = [f'"{str(row.get(k, "")).replace("\"", "\"\"")}"' for k in fields]
            await f.write(",".join(values) + "\n")
    print(f"üíæ CSV guardado: {path} ({len(rows)} filas)")

async def guardar_json(nombre: str, data: Any) -> None:
    path = os.path.join(JSON_DIR, nombre)
    async with aiofiles.open(path, "w", encoding="utf-8") as f:
        await f.write(json.dumps(data, ensure_ascii=False, indent=2))
    print(f"üíæ JSON guardado: {path}")

async def guardar_bib(nombre: str, contenido: str) -> None:
    path = os.path.join(BIB_DIR, nombre)
    async with aiofiles.open(path, "w", encoding="utf-8") as f:
        await f.write(contenido)
    print(f"üìö Bibliograf√≠a guardada: {path}")

def extract_text(blobs: Optional[Iterable[Any]]) -> str:
    parts: List[str] = []
    if not blobs: return ""
    for b in blobs:
        if hasattr(b, "text"): parts.append(getattr(b, "text", "") or "")
        elif isinstance(b, str): parts.append(b)
    return "\n".join(parts).strip()

def parse_text_response_to_papers(raw_text: str, topic: str) -> List[Dict[str, Any]]:
    """Parsea la respuesta de texto formateada del servidor Rust para la b√∫squeda."""
    papers = []
    content = raw_text.split('\n\n', 1)[-1]
    paper_blocks = re.split(r'\n\n(?=\d+\.\s)', content)

    for block in paper_blocks:
        if not block.strip(): continue
        paper_data = {"topic": topic, "title": None, "doi": None, "source": None, "year": None}
        
        title_match = re.search(r'^\d+\.\s*(.*?)\s*\(Relevance:', block)
        if title_match: paper_data['title'] = title_match.group(1).strip()
        
        doi_match = re.search(r'üìñ\s*DOI:\s*(.*)', block)
        if doi_match:
            doi_str = doi_match.group(1).strip()
            paper_data['doi'] = doi_str if doi_str and " " not in doi_str else None

        source_match = re.search(r'üîç\s*Source:\s*(.*)', block)
        if source_match: paper_data['source'] = source_match.group(1).strip()
            
        year_match = re.search(r'üìÖ\s*Year:\s*(\d{4})', block)
        if year_match: paper_data['year'] = int(year_match.group(1).strip())
            
        if paper_data.get('title'):
            paper_data['url'] = f"https://doi.org/{paper_data['doi']}" if paper_data.get('doi') else None
            paper_data['is_valid_doi'] = bool(paper_data['doi'] and DOI_REGEX.match(paper_data['doi']))
            papers.append(paper_data)
            
    return papers

async def step_a_search_papers(rh_client: RemoteMCPClient, topics: List[str]) -> List[Dict[str, Any]]:
    """Busca papers para cada topic y los devuelve combinados y deduplicados."""
    all_papers = []
    for topic in topics:
        print(f"\n--- üîé Buscando topic: '{topic}' ---")
        try:
            res = await rh_client.call_tool("search_papers", {"query": topic, "limit": MAX_RESULTS_PER_TOPIC})
            raw_text_content = extract_text(res)
            papers = parse_text_response_to_papers(raw_text_content, topic)
            all_papers.extend(papers)
            print(f"  -> Encontrados {len(papers)} resultados para '{topic}'.")
        except Exception as e:
            print(f"  ‚úó Error buscando topic '{topic}': {e}")
    
    seen_dois = set()
    unique_papers = []
    for p in all_papers:
        doi = p.get("doi")
        if doi and doi not in seen_dois:
            seen_dois.add(doi)
            unique_papers.append(p)
        elif not doi:
             unique_papers.append(p)
    
    print(f"\n‚ú® Total de papers √∫nicos encontrados: {len(unique_papers)}")
    return unique_papers

async def step_b_select_papers(papers: List[Dict[str, Any]]) -> List[str]:
    """Filtra y selecciona los DOIs v√°lidos para descargar."""
    print("\n--- üß† Seleccionando papers para descarga ---")
    
    selected_dois = [p['doi'] for p in papers if p.get('is_valid_doi')]
    
    if DOWNLOAD_ALL_PAPERS:
        print(f"  -> Selecci√≥n: TODOS ({len(selected_dois)} papers)")
        return selected_dois
    else:
        print(f"  -> Selecci√≥n: TOP {SELECT_TOP_K} (usando los primeros encontrados)")
        return selected_dois[:SELECT_TOP_K]

async def step_c_download_papers(rh_client: RemoteMCPClient, dois: List[str], papers_metadata: List[Dict]) -> Dict[str, Dict]:
    """Descarga los papers seleccionados en paralelo."""
    if not dois: return {}
    print(f"\n--- üì• Descargando {len(dois)} papers en paralelo (max {MAX_PARALLEL_DOWNLOADS}) ---")
    semaphore = asyncio.Semaphore(MAX_PARALLEL_DOWNLOADS)
    
    doi_map = {p['doi']: p for p in papers_metadata if p.get('doi')}

    async def _download_one(doi: str):
        async with semaphore:
            title = doi_map.get(doi, {}).get('title', 'untitled')
            filename = f"{slugify(title)}_{slugify(doi)}.pdf"
            try:
                res = await rh_client.call_tool("download_paper", {"doi": doi, "filename": filename, "directory": DOWNLOADS_DIR})
                raw_response_text = extract_text(res)
                
                success_match = re.search(r'File:\s*(.*?)\n', raw_response_text)
                
                if ("Download successful!" in raw_response_text or "File already exists" in raw_response_text) and success_match:
                    file_path = success_match.group(1).strip()
                    print(f"  ‚úì Descargado (o ya exist√≠a): {doi}")
                    return doi, {"status": "ok", "path": file_path, "title": title}
                else:
                    print(f"  ‚úó Fallo en descarga: {doi}")
                    return doi, {"status": "failed", "reason": raw_response_text.strip()}

            except Exception as e:
                print(f"  ‚úó Error grave durante la descarga de {doi}: {e}")
                return doi, {"status": "error", "reason": str(e)}

    tasks = [_download_one(doi) for doi in dois]
    results = await asyncio.gather(*tasks)
    return {doi: result for doi, result in results}

def paper_dict_to_bibtex_entry(paper: Dict[str, Any]) -> str:
    """Convierte un diccionario de paper enriquecido en una entrada BibTeX string."""
    # Crea una clave √∫nica a partir del primer autor y a√±o
    author_lastname = "unknown"
    if paper.get("authors"):
        try:
            author_lastname = slugify(paper["authors"].split(',')[0].split(' ')[-1])
        except: # noqa
            pass # Mantener 'unknown' si el formato del autor es inesperado
    
    year_str = str(paper.get('year', 'nodate'))
    key = f"{author_lastname}{year_str}"

    entry = f"@article{{{key},\n"
    if paper.get('title'):
        entry += f"  title     = {{{{{paper['title']}}}}},\n"
    if paper.get('authors'):
        entry += f"  author    = {{{paper['authors']}}},\n"
    if paper.get('year'):
        entry += f"  year      = {{{paper['year']}}},\n"
    if paper.get('journal'):
        entry += f"  journal   = {{{paper['journal']}}},\n"
    if paper.get('doi'):
        entry += f"  doi       = {{{paper['doi']}}},\n"
    entry += "}"
    return entry

async def step_d_generate_bibliography(rh_client: RemoteMCPClient, papers: List[Dict[str, Any]]) -> str:
    """Enriquece los metadatos de los papers y genera una bibliograf√≠a completa."""
    papers_with_doi = [p for p in papers if p.get('is_valid_doi')]
    if not papers_with_doi:
        return "% No se encontraron papers con DOI v√°lido para generar la bibliograf√≠a."

    print(f"\n--- üìö Generando Bibliograf√≠a ---")
    print(f"  -> Enriqueciendo metadatos para {len(papers_with_doi)} papers...")

    enriched_papers = []
    for paper in papers_with_doi:
        try:
            print(f"     - Obteniendo detalles para DOI: {paper['doi']}")
            # Llama a search_papers con el DOI para obtener metadatos ricos
            res = await rh_client.call_tool("search_papers", {"query": paper['doi'], "limit": 1})
            raw_text = extract_text(res)
            
            # Parsea la respuesta rica (puede tener m√°s campos)
            # Usamos un parser simple aqu√≠, asumiendo un formato similar
            enriched_data = paper.copy() # Empezamos con los datos que ya tenemos
            
            authors_match = re.search(r'üë§\s*Authors:\s*(.*)', raw_text)
            if authors_match:
                enriched_data['authors'] = authors_match.group(1).strip()

            journal_match = re.search(r' L Journal:\s*(.*)', raw_text)
            if journal_match:
                enriched_data['journal'] = journal_match.group(1).strip()
            
            enriched_papers.append(enriched_data)
        except Exception as e:
            print(f"  ‚úó Error enriqueciendo {paper.get('doi')}: {e}. Usando datos b√°sicos.")
            enriched_papers.append(paper) # A√±adir con datos b√°sicos si falla

    print(f"  -> Creando entradas BibTeX...")
    bib_entries = [paper_dict_to_bibtex_entry(p) for p in enriched_papers]
        
    print("  -> Bibliograf√≠a generada con √©xito.")
    return "\n\n".join(bib_entries)

def construir_topics_desde_terminos(terminos: List[str], max_topics: int) -> List[str]:
    if not terminos: return ["model context protocol"]
    topics_a_buscar = terminos[:max_topics]
    print(f"‚úÖ Construidos {len(topics_a_buscar)} topics para la b√∫squeda: {topics_a_buscar}")
    return topics_a_buscar

async def main():
    """Flujo principal que orquesta la investigaci√≥n."""
    await setup_output_dirs()
    
    try:
        async with aiofiles.open("terminos.txt", "r", encoding="utf-8") as f:
            contenido = await f.read()
        terminos = [t.strip() for t in contenido.splitlines() if t.strip()]
        if not terminos:
            print("‚ö†Ô∏è  El archivo 'terminos.txt' est√° vac√≠o. No hay nada que procesar.")
            return
    except FileNotFoundError:
        print("‚ùå ERROR: El archivo 'terminos.txt' no se encontr√≥.")
        return

    topics = construir_topics_desde_terminos(terminos, MAX_TOPICS)
    
    print(f"\nIniciando Asistente de Investigaci√≥n...")
    print(f"üìÇ Salidas en: {SALIDAS_DIR}")
    print(f"üì• PDFs se guardar√°n en: {DOWNLOADS_DIR}")
    
    server_configs = ServerConfig.get_server_configs()
    mcp_manager = MCPClientManager(server_configs)
    
    try:
        print("\nüîó Conectando al servidor de Research Hub...")
        await mcp_manager._connect_single_server("research_hub", server_configs["research_hub"])
        rh_client = mcp_manager.get_client("research_hub")
        if not rh_client:
            print("‚ùå No se pudo conectar al servidor de Research Hub. Abortando.")
            return
        print("‚úÖ Conectado.")

        found_papers = await step_a_search_papers(rh_client, topics)
        if not found_papers:
            print("\n‚ö†Ô∏è No se encontraron papers en ninguna de las b√∫squedas. Terminando.")
            return
        
        await guardar_json("00_resultados_completos.json", found_papers)
        await guardar_csv("01_papers_encontrados.csv", found_papers)
        
        selected_dois = await step_b_select_papers(found_papers)
        if not selected_dois:
            print("\n‚ö†Ô∏è No se seleccionaron papers con DOI v√°lido para descargar.")
        else:
            await guardar_json("02_dois_seleccionados.json", {"dois": selected_dois})
            download_manifest = await step_c_download_papers(rh_client, selected_dois, found_papers)
            await guardar_json("03_manifiesto_descarga.json", download_manifest)

        bib_content = await step_d_generate_bibliography(rh_client, found_papers)
        await guardar_bib("bibliografia_final.bib", bib_content)
        
        print("\nüéâ Proceso completado con √©xito.")

    finally:
        print("\nüèÅ Finalizando y cerrando conexiones...")
        await mcp_manager.close_all_clients()

if __name__ == "__main__":
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())
