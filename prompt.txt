Directory Structure:

â””â”€â”€ ./
    â”œâ”€â”€ .env.txt
    â”œâ”€â”€ ai_client_manager.py
    â”œâ”€â”€ ai_trend_researcher.py
    â”œâ”€â”€ config_manager.py
    â”œâ”€â”€ data_processor.py
    â”œâ”€â”€ keyword_manager.py
    â”œâ”€â”€ mcp_client_manager.py
    â”œâ”€â”€ platform_handlers.py
    â”œâ”€â”€ prompt.txt
    â”œâ”€â”€ README.md
    â”œâ”€â”€ report_generator.py
    â”œâ”€â”€ requirement.txt
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ research_assistant_con_hackers_LLM.py
    â””â”€â”€ research_assistant.py



---
File: /.env.txt
---

# --- Claves de API para los servicios de datos externos ---
# Cada una de estas claves es necesaria para que el script pueda autenticarse y solicitar datos de estas plataformas.

# Clave de API para acceder a los datos de YouTube (bÃºsquedas, detalles de videos, etc.).
YOUTUBE_API_KEY=your_youtube_api_key_here

# Token de Acceso Personal de GitHub para realizar bÃºsquedas en repositorios y cÃ³digo.
GITHUB_PERSONAL_ACCESS_TOKEN=your_github_api_key_here

# Clave de API para interactuar con Notion (crear y actualizar pÃ¡ginas).
NOTION_API_KEY=your_notion_api_key_here

# Clave de API para SiliconFlow, que podrÃ­a ser usada por el servidor de ArXiv para procesar PDFs.
SILICONFLOW_API_KEY=your_siliconflow_api_key_here

# Token de Acceso para Supabase, para guardar los resultados de la investigaciÃ³n en la base de datos.
SUPABASE_ACCESS_TOKEN=your_superbase_api_key_here


# --- ConfiguraciÃ³n EspecÃ­fica de Notion ---

# El ID de la pÃ¡gina principal en Notion bajo la cual se crearÃ¡n todas las pÃ¡ginas de informes nuevas.
NOTION_PARENT_PAGE_ID='your_notion_parent_page_id'


# --- ConfiguraciÃ³n de Rutas del Sistema ---
# Define rutas importantes para que la aplicaciÃ³n sea portable entre diferentes sistemas.

# Ruta al directorio donde el servidor 'research_hub' descargarÃ¡ los papers.
RESEARCH_PAPERS_DIR="/path/to/your/research-papers"

# Ruta completa al binario ejecutable del servidor 'research_hub'.
RESEARCH_HUB_EXECUTABLE="/path/to/your/rust-research-mcp"


# --- ConfiguraciÃ³n del Proveedor de IA ---

# Define quÃ© servicio de IA (LLM) se usarÃ¡ para tareas como la extracciÃ³n de palabras clave o traducciones.
# Opciones vÃ¡lidas: "anthropic", "gemini", "groq", "ollama", "openai".
AI_PROVIDER="openai" 

# Almacena las claves de API para cada uno de los proveedores de IA soportados.
# El script cargarÃ¡ la clave correspondiente al proveedor seleccionado en AI_PROVIDER.
ANTHROPIC_API_KEY="tu_clave_de_anthropic"
GOOGLE_API_KEY="tu_clave_de_gemini"
GROQ_API_KEY="tu_clave_de_groq"
OPENAI_API_KEY="tu_clave_de_openai"

# --- ConfiguraciÃ³n Opcional de Modelos de IA ---
# Permite especificar quÃ© modelo exacto usar para cada proveedor. Si no se define, se usarÃ¡ un modelo por defecto.

# Modelo especÃ­fico para Google Gemini (ej. gemini-1.5-flash).
AI_MODEL_GEMINI="gemini-1.5-flash"
# Modelo especÃ­fico para Groq (ej. llama3-70b-8192).
AI_MODEL_GROQ="llama3-70b-8192"
# Modelo para Ollama, que debe corresponder a un modelo que tengas instalado localmente (ej. llama3).
AI_MODEL_OLLAMA="llama3"
# Modelo especÃ­fico para Anthropic Claude (ej. claude-3-haiku).
AI_MODEL_ANTHROPIC="claude-3-haiku-20240307"
# Modelo especÃ­fico para OpenAI (ej. gpt-4o).
AI_MODEL_OPENAI="gpt-4o"



---
File: /ai_client_manager.py
---

# ai_client_manager.py

import asyncio
# Importa las bibliotecas cliente de cada proveedor de IA soportado.
import google.generativeai as genai  # Para Google Gemini
from anthropic import Anthropic       # Para Anthropic Claude
from groq import Groq                 # Para Groq
import ollama                         # Para Ollama (modelos locales)
from openai import OpenAI             # Para OpenAI
from typing import Callable, Any

class AIClientManager:
    """
    Gestiona la inicializaciÃ³n e interacciÃ³n con diferentes clientes de IA.
    ActÃºa como una "fÃ¡brica" que crea el cliente correcto segÃºn la configuraciÃ³n
    y proporciona un mÃ©todo unificado 'chat_completion' para interactuar con Ã©l de forma no bloqueante.
    """
    def __init__(self, provider: str, api_key: str = None, model: str = None):
        """
        Constructor. Inicializa el cliente de IA basado en el proveedor especificado.
        :param provider: El nombre del proveedor (ej. "openai", "gemini").
        :param api_key: La clave de API para el proveedor.
        :param model: El nombre del modelo especÃ­fico a usar (opcional).
        """
        self.provider = provider.lower()
        self.model = model
        self.client: Any = None
        print(f"Initializing AI client for provider: {self.provider}")

        # LÃ³gica condicional para inicializar el cliente correcto.
        if self.provider == 'gemini':
            if not api_key: raise ValueError("Google API Key is required for Gemini provider.")
            genai.configure(api_key=api_key)
            self.client = genai.GenerativeModel(self.model or 'gemini-pro')
        elif self.provider == 'groq':
            if not api_key: raise ValueError("Groq API Key is required for Groq provider.")
            self.client = Groq(api_key=api_key)
        elif self.provider == 'ollama':
            # Para Ollama, el cliente es el propio mÃ³dulo de la biblioteca.
            self.client = ollama
        elif self.provider == 'anthropic':
            if not api_key: raise ValueError("Anthropic API Key is required for Anthropic provider.")
            self.client = Anthropic(api_key=api_key)
        elif self.provider == 'openai':
            if not api_key: raise ValueError("OpenAI API Key is required for OpenAI provider.")
            self.client = OpenAI(api_key=api_key)
        else:
            raise ValueError(f"Unsupported AI provider: {self.provider}")

    async def chat_completion(self, prompt: str, max_tokens: int = 1024) -> str:
        """
        EnvÃ­a un prompt al modelo de IA y devuelve la respuesta de texto.
        Este mÃ©todo abstrae las diferencias en las llamadas a la API y las ejecuta en un
        hilo separado para no bloquear el bucle de eventos de asyncio.
        """
        try:
            # Selecciona la funciÃ³n de llamada a la API correcta basada en el proveedor.
            api_call_function = self._get_api_call_function(prompt, max_tokens)
            
            # Ejecuta la llamada sÃ­ncrona de la biblioteca en un hilo separado.
            # Esto es crucial para no bloquear la aplicaciÃ³n asÃ­ncrona.
            loop = asyncio.get_running_loop()
            response_text = await loop.run_in_executor(
                None,  # Usa el ejecutor de hilos por defecto.
                api_call_function
            )
            return response_text

        except Exception as e:
            print(f"Error calling {self.provider} API: {e}")
            return ""

    def _get_api_call_function(self, prompt: str, max_tokens: int) -> Callable[[], str]:
        """Devuelve la funciÃ³n lambda correcta para realizar la llamada a la API sÃ­ncrona."""
        
        if self.provider == 'gemini':
            return lambda: self.client.generate_content(prompt).text

        elif self.provider == 'groq':
            return lambda: self.client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model=self.model or "llama3-8b-8192",
            ).choices[0].message.content

        elif self.provider == 'ollama':
            return lambda: self.client.chat(
                model=self.model or 'llama3',
                messages=[{'role': 'user', 'content': prompt}]
            )['message']['content']

        elif self.provider == 'anthropic':
            return lambda: self.client.messages.create(
                model=self.model or "claude-3-sonnet-20240229",
                max_tokens=max_tokens,
                messages=[{"role": "user", "content": prompt}]
            ).content[0].text

        elif self.provider == 'openai':
            return lambda: self.client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model=self.model or "gpt-4o",
            ).choices[0].message.content
            
        else:
            # Esto no deberÃ­a ocurrir si el constructor funcionÃ³, pero es una salvaguarda.
            raise NotImplementedError(f"API call function not implemented for {self.provider}")



---
File: /ai_trend_researcher.py
---

# ai_trend_researcher.py
# -*- coding: utf-8 -*-

import asyncio
import sys
import os
import signal
import argparse
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Iterable

from dotenv import load_dotenv

from keyword_manager import KeywordManager
from mcp_client_manager import MCPClientManager
from platform_handlers import PlatformHandlerFactory
from data_processor import KeywordExtractor, DataAnalyzer
from report_generator import ReportManager
from config_manager import ServerConfig, AppConfig, PlatformConfig
from ai_client_manager import AIClientManager

load_dotenv()


# ----------------------------- utilidades -----------------------------

def env_int(name: str, default: int) -> int:
    try:
        v = os.getenv(name)
        return int(v) if v is not None else default
    except ValueError:
        return default

def env_float(name: str, default: float) -> float:
    try:
        v = os.getenv(name)
        return float(v) if v is not None else default
    except ValueError:
        return default

def env_bool(name: str, default: bool) -> bool:
    v = (os.getenv(name, str(default)) or "").strip().lower()
    return v in ("1", "true", "t", "yes", "y", "on")

def now_str() -> str:
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def log(msg: str) -> None:
    print(f"[{now_str()}] {msg}", flush=True)


# --------------------------- nÃºcleo investigador ---------------------------

class AITrendResearcher:
    """
    Orquestador principal del flujo de investigaciÃ³n de tendencias de IA.
    Mejora: control de concurrencia, timeouts, reintentos y CLI.
    """

    def __init__(
        self,
        platforms_filter: Optional[Iterable[str]] = None,
        exclude_platforms: Optional[Iterable[str]] = None,
        per_task_timeout: float = 35.0,
        retries: int = 1,
        concurrency: int = 4,
        keywords_limit: Optional[int] = None,
    ):
        # Estado configuraciÃ³n / validaciones
        AppConfig.print_config_status()
        missing_vars = AppConfig.validate_required_env_vars()
        if missing_vars:
            log("âœ— Faltan variables de entorno requeridas:")
            for var in missing_vars:
                log(f"  - {var}")
            log("Por favor, complÃ©talas en tu .env o deshabilita las plataformas asociadas.")
            # No abortamos aquÃ­: el sistema puede operar con subset (ej. sin Notion/Supabase)
            # pero si faltan claves crÃ­ticas de proveedor de IA, AIClientManager fallarÃ¡.

        # Cliente de IA
        ai_provider = AppConfig.get_ai_provider()
        api_key = AppConfig.get_api_key(ai_provider)
        ai_model = AppConfig.get_ai_model(ai_provider)
        self.ai_client_manager = AIClientManager(provider=ai_provider, api_key=api_key, model=ai_model)

        # Palabras clave
        self.keyword_manager = KeywordManager()

        # Servidores MCP / plataformas
        self.server_configs = ServerConfig.get_server_configs()
        supported = set(PlatformConfig.get_supported_platforms())
        wanted = set(p.strip() for p in (platforms_filter or supported))
        excluded = set(p.strip() for p in (exclude_platforms or []))
        self.platforms: List[str] = [p for p in wanted if p in supported and p not in excluded]

        # Gestores
        self.mcp_manager = MCPClientManager(self.server_configs)
        self.keyword_extractor = KeywordExtractor(self.ai_client_manager)
        self.data_analyzer = DataAnalyzer(self.ai_client_manager)
        self.report_manager: Optional[ReportManager] = None

        # ParÃ¡metros ejecuciÃ³n
        self.per_task_timeout = float(per_task_timeout)
        self.retries = int(max(0, retries))
        self.semaphore = asyncio.Semaphore(int(max(1, concurrency)))
        self.keywords_limit = int(keywords_limit) if keywords_limit else None

    # =======================================================================
    # BLOQUE DE CÃ“DIGO CORREGIDO Y COMPLETADO
    # =======================================================================
    async def run_daily_research(self) -> str:
        log(f"ðŸš€ Starting AI trend research - {now_str()}")
        report_path = ""
        try:
            # 1) Conectar MCP
            await self.mcp_manager.connect_all_servers()

            # 2) Plataformas activas realmente disponibles
            active_platforms = [p for p in self.platforms if self.mcp_manager.is_platform_available(p)]
            if not active_platforms:
                log("âš ï¸ No hay servidores MCP activos; nada que hacer.")
                return ""

            log(f"ðŸŒ Servidores MCP activos: {', '.join(active_platforms)}")

            # 3) Clientes opcionales para reportes
            notion_client = self.mcp_manager.get_client("notion")
            supabase_client = self.mcp_manager.get_client("supabase")
            notion_parent_id = AppConfig.get_notion_parent_page_id()

            # 4) Gestor de reportes
            self.report_manager = ReportManager(
                reports_dir=AppConfig.get_reports_directory(),
                notion_client=notion_client,
                notion_parent_id=notion_parent_id,
                supabase_client=supabase_client,
            )

            # 5) Cargar keywords activas desde terminos.txt
            active_keywords = self._load_active_keywords()
            if self.keywords_limit is not None:
                active_keywords = active_keywords[: self.keywords_limit]
            
            if not active_keywords:
                log("âš ï¸ No se encontraron keywords en 'terminos.txt'. El archivo estÃ¡ vacÃ­o o no existe.")
                return ""
                
            log(f"ðŸ”‘ Investigando {len(active_keywords)} keywords en {len(active_platforms)} plataformas: {active_keywords}")

            # 6) InvestigaciÃ³n concurrente
            tasks = []
            for keyword in active_keywords:
                for platform in active_platforms:
                    task = asyncio.create_task(self._research_single_keyword(keyword, platform))
                    tasks.append(task)
            
            log(f"ðŸ”„ Lanzando {len(tasks)} tareas de investigaciÃ³n...")
            research_results: List[Dict] = await asyncio.gather(*tasks)

            # 7) Procesar y analizar resultados
            log("ðŸ“Š Analizando resultados y extrayendo insights...")
            valid_results = [r for r in research_results if r and not r.get("error")]
            
            new_keywords_list = await self.keyword_extractor.extract_keywords(valid_results)
            scored_keywords = self.data_analyzer.score_keywords(new_keywords_list, valid_results)
            summary_stats = self.data_analyzer.calculate_summary_stats(research_results, new_keywords_list)
            recommendations = await self.data_analyzer.generate_recommendations(valid_results, new_keywords_list)

            # 8) Actualizar base de datos de keywords (Opcional, se mantiene la lÃ³gica)
            log("ðŸ’¾ Actualizando el catÃ¡logo de keywords...")
            for kw, score in scored_keywords.items():
                self.keyword_manager.add_new_keyword(kw, score, "discovered", "llm_extraction")
            
            self.keyword_manager.mark_keywords_used(active_keywords)
            self.keyword_manager.record_execution(active_keywords, "completed", len(new_keywords_list))
            
            # 9) Generar informes
            log("ðŸ“„ Generando informes...")
            report_path = await self.report_manager.generate_all_reports(
                research_data=research_results,
                new_keywords=new_keywords_list,
                summary=summary_stats,
                recommendations=recommendations,
            )
            log(f"ðŸŽ‰ InvestigaciÃ³n completada con Ã©xito. Informe local: {report_path}")

        except Exception as e:
            log(f"âŒ Error catastrÃ³fico en el flujo principal: {e}")
            import traceback
            traceback.print_exc()

        finally:
            # Este bloque se asegura de que las conexiones se cierren siempre
            log("ðŸ”Œ Cerrando todas las conexiones MCP...")
            await self.mcp_manager.close_all_clients()
            return report_path

    # =======================================================================
    # FUNCIÃ“N MODIFICADA PARA LEER DESDE terminos.txt
    # =======================================================================
    def _load_active_keywords(self) -> List[str]:
        """Carga las keywords directamente desde el archivo 'terminos.txt'."""
        keywords_file = "terminos.txt"
        log(f"Cargando keywords desde '{keywords_file}'...")
        if not os.path.exists(keywords_file):
            log(f"ðŸ”¥ El archivo '{keywords_file}' no se encuentra en el directorio.")
            return []
        
        try:
            with open(keywords_file, 'r', encoding='utf-8') as f:
                # Lee cada lÃ­nea, quita espacios/saltos de lÃ­nea y filtra las que queden vacÃ­as
                keywords = [line.strip() for line in f if line.strip()]
            return keywords
        except Exception as e:
            log(f"ðŸ”¥ Error al leer el archivo de keywords '{keywords_file}': {e}")
            return []

    async def _research_single_keyword(self, keyword: str, platform: str) -> Dict[str, Any]:
        """Ejecuta la investigaciÃ³n para una Ãºnica combinaciÃ³n de keyword y plataforma con reintentos."""
        async with self.semaphore:
            for attempt in range(self.retries + 1):
                try:
                    handler = PlatformHandlerFactory.create_handler(platform, self.ai_client_manager)
                    client = self.mcp_manager.get_client(platform)
                    config = self.server_configs.get(platform, {})
                    
                    if not client:
                        raise ConnectionError(f"Cliente para {platform} no estÃ¡ disponible.")

                    result = await asyncio.wait_for(
                        handler.research_keyword(client, keyword, config),
                        timeout=self.per_task_timeout
                    )
                    return result

                except asyncio.TimeoutError:
                    log(f"â³ Timeout investigando '{keyword}' en '{platform}' (intento {attempt+1})")
                    if attempt >= self.retries:
                        return {"platform": platform, "keyword": keyword, "error": "Timeout after all retries"}
                except Exception as e:
                    log(f"ðŸ”¥ Error investigando '{keyword}' en '{platform}' (intento {attempt+1}): {e}")
                    if attempt >= self.retries:
                        return {"platform": platform, "keyword": keyword, "error": str(e)}
                
                if attempt < self.retries:
                    await asyncio.sleep(2.0 * (attempt + 1)) # Backoff exponencial simple
            
            return {"platform": platform, "keyword": keyword, "error": "Unknown error after all retries"}

# =======================================================================
# FIN DEL BLOQUE MODIFICADO
# =======================================================================

async def main(args):
    """Punto de entrada principal para la ejecuciÃ³n del script."""
    researcher = AITrendResearcher(
        platforms_filter=args.platforms,
        exclude_platforms=args.exclude,
        concurrency=args.concurrency,
        per_task_timeout=args.timeout,
        retries=args.retries,
        keywords_limit=args.limit_keywords
    )
    
    # Manejo de cierre gradual
    loop = asyncio.get_running_loop()
    stop_event = asyncio.Event()

    def _shutdown_handler():
        log("SeÃ±al de interrupciÃ³n recibida, iniciando cierre gradual...")
        stop_event.set()

    if sys.platform != "win32":
        loop.add_signal_handler(signal.SIGINT, _shutdown_handler)
        loop.add_signal_handler(signal.SIGTERM, _shutdown_handler)

    try:
        research_task = asyncio.create_task(researcher.run_daily_research())
        stop_wait_task = asyncio.create_task(stop_event.wait())
        
        done, pending = await asyncio.wait(
            {research_task, stop_wait_task},
            return_when=asyncio.FIRST_COMPLETED
        )

        if stop_wait_task in done:
            log("Cierre solicitado. Cancelando tareas pendientes...")
            research_task.cancel()
            await asyncio.sleep(1) # Dar tiempo para que la cancelaciÃ³n se propague
        
        for task in pending:
            task.cancel()
        await asyncio.gather(*pending, return_exceptions=True)

    except asyncio.CancelledError:
        log("Tareas principales canceladas.")
    finally:
        if sys.platform != "win32":
            loop.remove_signal_handler(signal.SIGINT)
            loop.remove_signal_handler(signal.SIGTERM)
        log("EjecuciÃ³n finalizada.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Motor de InvestigaciÃ³n de Tendencias de IA.")
    parser.add_argument("-p", "--platforms", nargs='+', help="Lista de plataformas a investigar (ej. youtube github). Por defecto todas.")
    parser.add_argument("-e", "--exclude", nargs='+', help="Lista de plataformas a excluir (ej. web arxiv).")
    parser.add_argument("-c", "--concurrency", type=int, default=4, help="NÃºmero de tareas de investigaciÃ³n concurrentes.")
    parser.add_argument("-t", "--timeout", type=float, default=45.0, help="Timeout en segundos para cada tarea individual.")
    parser.add_argument("-r", "--retries", type=int, default=1, help="NÃºmero de reintentos por tarea en caso de fallo.")
    parser.add_argument("-l", "--limit-keywords", type=int, help="Limita el nÃºmero de keywords a investigar.")
    
    cli_args = parser.parse_args()
    
    try:
        asyncio.run(main(cli_args))
    except KeyboardInterrupt:
        log("InterrupciÃ³n por teclado detectada. Saliendo.")




---
File: /config_manager.py
---

# config_manager.py
# -*- coding: utf-8 -*-

# Importa el mÃ³dulo 'os' para interactuar con el sistema operativo, principalmente para leer variables de entorno.
import os
# Importa herramientas de 'typing' para aÃ±adir anotaciones de tipo, mejorando la legibilidad y robustez del cÃ³digo.
from typing import Dict, List, Any


class ServerConfig:
    """
    Gestiona las configuraciones de los servidores MCP (Model Context Protocol).
    Define cÃ³mo iniciar y conectar con los diferentes servicios externos (YouTube, GitHub, etc.).
    """

    @staticmethod
    def _clean_env(env: Dict[str, Any]) -> Dict[str, str]:
        """
        MÃ©todo privado para limpiar el diccionario de entorno.
        Elimina claves con valores None o vacÃ­os ("") y convierte todos los valores a string.
        Esto es necesario porque algunas bibliotecas (como Pydantic, usada por MCP) no aceptan None en variables de entorno.
        """
        # Si el diccionario de entrada estÃ¡ vacÃ­o, devuelve uno vacÃ­o.
        if not env:
            return {}
        # Devuelve un nuevo diccionario que solo incluye los Ã­tems vÃ¡lidos y con valores casteados a string.
        return {k: str(v) for k, v in env.items() if v not in (None, "")}

    @staticmethod
    def get_server_configs() -> Dict[str, Dict[str, Any]]:
        """
        Devuelve un diccionario que contiene la configuraciÃ³n detallada para cada servidor MCP.
        Los servidores que requieren credenciales (API keys) se marcan con enabled=False si la clave no estÃ¡ presente.
        """
        # Lee todas las claves de API y tokens de las variables de entorno.
        youtube_key   = os.getenv("YOUTUBE_API_KEY")
        gh_token      = os.getenv("GITHUB_PERSONAL_ACCESS_TOKEN")
        notion_key    = os.getenv("NOTION_API_KEY")
        notion_parent = os.getenv("NOTION_PARENT_PAGE_ID")
        supa_token    = os.getenv("SUPABASE_ACCESS_TOKEN")
        silicon_key   = os.getenv("SILICONFLOW_API_KEY")

        # Lee las rutas configurables desde el entorno para mayor portabilidad.
        download_dir = os.getenv("RESEARCH_PAPERS_DIR", "research-papers")
        research_hub_executable = os.getenv("RESEARCH_HUB_EXECUTABLE", "rust-research-mcp")

        # Construye dinÃ¡micamente la lista de argumentos para el servidor de Notion.
        notion_args = ["@ramidecodes/mcp-server-notion@latest", "-y"]
        # AÃ±ade la clave de API a los argumentos solo si existe, para no exponer un argumento vacÃ­o.
        if notion_key:
            notion_args.append(f"--api-key={notion_key}")

        # Construye dinÃ¡micamente la lista de argumentos para el servidor de Supabase.
        supabase_args = ["-y", "@supabase/mcp-server-supabase@latest"]
        # AÃ±ade el token de acceso a los argumentos solo si existe.
        if supa_token:
            supabase_args += ["--access-token", supa_token]

        # Define el diccionario principal de configuraciones.
        configs: Dict[str, Dict[str, Any]] = {
            "youtube": {
                "server_name": "npx",  # Comando para ejecutar el servidor (a travÃ©s de npx).
                "args": ["-y", "youtube-data-mcp-server"],  # Argumentos para el comando.
                "env": ServerConfig._clean_env({  # Variables de entorno especÃ­ficas para este servidor.
                    "YOUTUBE_API_KEY": youtube_key,
                    "YOUTUBE_TRANSCRIPT_LANG": "ja",  # Configura el idioma de las transcripciones a japonÃ©s.
                }),
                "tools": ["searchVideos", "getVideoDetails", "getTranscripts"],  # Herramientas que expone el servidor.
                "required_env": ["YOUTUBE_API_KEY"],  # Variables de entorno obligatorias.
                "enabled": True if youtube_key else False,  # Se activa solo si la clave de API estÃ¡ presente.
            },
            "github": {
                "server_name": "npx",
                "args": ["-y", "@modelcontextprotocol/server-github"],
                "env": ServerConfig._clean_env({
                    "GITHUB_PERSONAL_ACCESS_TOKEN": gh_token,
                }),
                "tools": ["search_code", "search_repositories", "get_repository"],
                "required_env": ["GITHUB_PERSONAL_ACCESS_TOKEN"],
                "enabled": True if gh_token else False, # Se activa solo si el token de GitHub estÃ¡ presente.
            },
            "web": {
                "server_name": "one-search-mcp",  # Este servidor se ejecuta directamente, sin 'npx'.
                "args": [],  # No necesita argumentos adicionales.
                "env": {  # Variables de entorno para estandarizar y silenciar la salida de la consola.
                    "DOTENVX_SILENT": "1",
                    "FORCE_COLOR": "0",
                    "NO_COLOR": "1"
                },
                "tools": ["one_search", "one_extract", "one_scrape"],
                "enabled": True,  # Este servidor siempre estÃ¡ habilitado ya que no requiere claves.
            },
            "notion": {
                "server_name": "npx",
                "args": ["-y", *notion_args],  # Usa los argumentos construidos dinÃ¡micamente.
                "env": ServerConfig._clean_env({}), # No necesita variables de entorno adicionales.
                "tools": ["create-page", "get-page", "update-page", "query-database", "search"],
                "required_env": ["NOTION_API_KEY"],
                "enabled": True if notion_key else False, # Se activa solo si la clave de Notion estÃ¡ presente.
            },
            "arxiv": {
                "server_name": "npx",
                "args": ["-y", "@langgpt/arxiv-mcp-server@latest"],
                "env": {
                    "SILICONFLOW_API_KEY": silicon_key,
                    "WORK_DIR": "./reports",  # Directorio de trabajo para descargar PDFs.
                    "FORCE_COLOR": "0",
                    "NO_COLOR": "1",
                    "DOTENVX_SILENT": "1"
                },
                "tools": [
                    "search_arxiv", "download_arxiv_pdf", "parse_pdf_to_text",
                    "convert_to_wechat_article", "parse_pdf_to_markdown",
                    "process_arxiv_paper", "clear_workdir"
                ],
                "enabled": bool(silicon_key), # Se activa solo si la clave de SiliconFlow estÃ¡ presente.
            },
            "hackernews": {
                "server_name": "npx",
                "args": ["-y", "@microagents/server-hackernews"],
                "env": ServerConfig._clean_env({}),
                "tools": ["getStories", "getStory", "getStoryWithComments"],
                "required_env": [],  # No requiere variables de entorno.
                "enabled": True,  # Siempre habilitado.
            },
            "supabase": {
                "server_name": "npx",
                "args": supabase_args,  # Usa los argumentos construidos dinÃ¡micamente.
                "env": ServerConfig._clean_env({}),
                "tools": ["execute_sql"],
                "required_env": ["SUPABASE_ACCESS_TOKEN"],
                "enabled": True if supa_token else False, # Se activa solo si el token de Supabase estÃ¡ presente.
            },
            "research_hub": {
                "server_name": research_hub_executable,
                "args": [
                    "--download-dir", download_dir,
                    "--log-level", "info"
                ],
                "env": {
                    "RUST_LOG": "info",
                    "DOTENVX_SILENT": "1",
                    "FORCE_COLOR": "0",
                    "NO_COLOR": "1"
                },
                "tools": [
                    "search_papers", 
                    "download_paper", 
                    "extract_metadata",
                    "search_code", 
                    "generate_bibliography"
                ],
                "required_env": ["RESEARCH_HUB_EXECUTABLE", "RESEARCH_PAPERS_DIR"], 
                "enabled": os.path.exists(research_hub_executable), # Se activa si el binario existe.
            },
        }
        # Devuelve el diccionario completo de configuraciones.
        return configs

    @staticmethod
    def get_enabled_platforms() -> List[str]:
        """Devuelve una lista con los nombres de las plataformas que estÃ¡n actualmente habilitadas."""
        configs = ServerConfig.get_server_configs()
        # Crea una lista de plataformas donde el valor de 'enabled' es True.
        return [platform for platform, config in configs.items() if config.get("enabled", False)]


class AppConfig:
    """Clase para gestionar la configuraciÃ³n general de la aplicaciÃ³n (proveedor de IA, modelos, etc.)."""

    @staticmethod
    def get_ai_provider() -> str:
        """Obtiene el proveedor de IA configurado en .env, con 'openai' como valor por defecto."""
        return os.getenv("AI_PROVIDER", "openai").lower()

    @staticmethod
    def get_api_key(provider: str) -> str:
        """Obtiene la clave de API para un proveedor de IA especÃ­fico."""
        # Mapea el nombre del proveedor a su variable de entorno correspondiente.
        provider_key_map = {
            "anthropic": "ANTHROPIC_API_KEY",
            "gemini": "GOOGLE_API_KEY",
            "groq": "GROQ_API_KEY",
            "openai": "OPENAI_API_KEY",
            # 'ollama' se ejecuta localmente y no requiere clave.
        }
        # Obtiene el nombre de la variable de entorno del mapa.
        env_var_name = provider_key_map.get(provider)
        # Devuelve el valor de la variable de entorno si existe, si no, None.
        return os.getenv(env_var_name) if env_var_name else None

    @staticmethod
    def get_ai_model(provider: str) -> str:
        """Obtiene el nombre del modelo de IA especÃ­fico para un proveedor, si estÃ¡ configurado."""
        # Construye el nombre de la variable de entorno (ej. "AI_MODEL_OPENAI").
        env_var_name = f"AI_MODEL_{provider.upper()}"
        # Devuelve el valor de la variable de entorno.
        return os.getenv(env_var_name)

    @staticmethod
    def get_notion_parent_page_id() -> str:
        """Obtiene el ID de la pÃ¡gina padre de Notion desde las variables de entorno."""
        return os.getenv("NOTION_PARENT_PAGE_ID", "")

    @staticmethod
    def get_reports_directory() -> str:
        """Devuelve el nombre del directorio donde se guardan los informes locales."""
        return "reports"

    @staticmethod
    def validate_required_env_vars() -> List[str]:
        """
        Valida que todas las variables de entorno necesarias estÃ©n definidas.
        Devuelve una lista con las variables que faltan.
        """
        # Define un diccionario de variables requeridas y su descripciÃ³n.
        required_vars = {
            "YOUTUBE_API_KEY": "YouTube API key",
            "GITHUB_PERSONAL_ACCESS_TOKEN": "GitHub access token",
            "NOTION_API_KEY": "Notion API key",
            "NOTION_PARENT_PAGE_ID": "Notion parent page ID",
            "SUPABASE_ACCESS_TOKEN": "Supabase access token",
            "RESEARCH_PAPERS_DIR": "Research papers download directory",
            "RESEARCH_HUB_EXECUTABLE": "Path to the Research Hub executable"
        }
        # AÃ±ade la clave de API del proveedor de IA seleccionado a la lista de requeridos.
        ai_provider = AppConfig.get_ai_provider()
        api_key_env_var = {
            "gemini": "GOOGLE_API_KEY",
            "groq": "GROQ_API_KEY",
            "anthropic": "ANTHROPIC_API_KEY",
            "openai": "OPENAI_API_KEY",
        }.get(ai_provider)
        if api_key_env_var:
            required_vars[api_key_env_var] = f"API Key for {ai_provider.capitalize()}"

        # Crea una lista de las variables que no estÃ¡n definidas.
        missing = [f"{var} ({desc})" for var, desc in required_vars.items() if not os.getenv(var)]
        return missing

    @staticmethod
    def print_config_status():
        """Imprime en la consola un resumen del estado de la configuraciÃ³n actual."""
        print("=== Configuration Status ===")
        ai_provider = AppConfig.get_ai_provider()
        print(f"âœ“ AI Provider configured: {ai_provider.upper()}")

        api_key = AppConfig.get_api_key(ai_provider)
        if ai_provider not in ["ollama"]: # Ollama no necesita clave.
            print(f"âœ“ {ai_provider.capitalize()} API key loaded" if api_key else f"âœ— {ai_provider.capitalize()} API key not found")

        ai_model = AppConfig.get_ai_model(ai_provider)
        print(f"âœ“ Using specific model for {ai_provider}: {ai_model}" if ai_model else f"âœ“ Using default model for {ai_provider}")
        print("---")

        # Comprueba el estado de otras claves de API importantes.
        other_vars = [
            ("YOUTUBE_API_KEY", "YouTube API key"),
            ("GITHUB_PERSONAL_ACCESS_TOKEN", "GitHub access token"),
            ("NOTION_API_KEY", "Notion API key"),
            ("NOTION_PARENT_PAGE_ID", "Notion parent page ID"),
            ("SUPABASE_ACCESS_TOKEN", "Supabase access token"),
            ("RESEARCH_PAPERS_DIR", "Research papers directory"),
            ("RESEARCH_HUB_EXECUTABLE", "Research Hub executable"),
        ]
        for var, description in other_vars:
            # Imprime un tick (âœ“) si la variable estÃ¡ cargada, o una cruz (âœ—) si no.
            print(f"âœ“ {description} loaded" if os.getenv(var) else f"âœ— {description} not found")
        
        if os.getenv("RESEARCH_HUB_EXECUTABLE") and not os.path.exists(os.getenv("RESEARCH_HUB_EXECUTABLE")):
            print(f"âœ— WARNING: Research Hub executable not found at specified path.")

        print("============================")


class PlatformConfig:
    """Define las plataformas que la aplicaciÃ³n soporta para la investigaciÃ³n."""
    # Lista fija de plataformas soportadas en el cÃ³digo.
    SUPPORTED_PLATFORMS = ["web", "youtube", "github", "arxiv", "hackernews", "supabase", "research_hub"]

    @staticmethod
    def get_supported_platforms() -> List[str]:
        """Devuelve una copia de la lista de plataformas soportadas."""
        return PlatformConfig.SUPPORTED_PLATFORMS.copy()

    @staticmethod
    def is_platform_supported(platform: str) -> bool:
        """Comprueba si una plataforma dada estÃ¡ en la lista de soportadas."""
        return platform in PlatformConfig.SUPPORTED_PLATFORMS



---
File: /data_processor.py
---

# data_processor.py
# -*- coding: utf-8 -*-

# Importa el mÃ³dulo 'asyncio' para ejecutar tareas sÃ­ncronas en un hilo.
import asyncio
# Importa el mÃ³dulo 'json' para trabajar con datos en formato JSON.
import json
# Importa el mÃ³dulo 're' para trabajar con expresiones regulares (bÃºsqueda de patrones en texto).
import re
# De 'collections', importa 'Counter' para contar fÃ¡cilmente la frecuencia de elementos en una lista.
from collections import Counter
# De 'datetime', importa 'datetime' para obtener la fecha y hora actuales.
from datetime import datetime
# De 'typing', importa herramientas para anotaciones de tipo.
from typing import Dict, List, Any

# Importa el gestor de clientes de IA para que el analizador pueda usar LLMs.
from ai_client_manager import AIClientManager

class KeywordExtractor:
    """
    Extrae nuevas palabras clave a partir de los datos de investigaciÃ³n.
    Utiliza un LLM (Modelo LingÃ¼Ã­stico Grande) si estÃ¡ disponible para una extracciÃ³n mÃ¡s inteligente.
    Si no, recurre a un mÃ©todo heurÃ­stico local basado en frecuencia de palabras.
    """
    def __init__(self, ai_client_manager: AIClientManager = None):
        """
        Constructor. Recibe un gestor de cliente de IA.
        Este gestor debe tener un mÃ©todo `async chat_completion(prompt, max_tokens=...)`.
        """
        self.ai_client = ai_client_manager

    async def extract_keywords(self, research_data: List[Dict[str, Any]]) -> List[str]:
        """
        MÃ©todo principal para extraer palabras clave.
        Decide si usar el LLM o el mÃ©todo heurÃ­stico de respaldo.
        """
        # Si no hay datos de investigaciÃ³n, no hay nada que hacer.
        if not research_data:
            print("No hay datos de investigaciÃ³n para la extracciÃ³n de keywords.")
            return []

        # Prepara un resumen compacto del contenido para no enviar demasiada informaciÃ³n al LLM.
        content_summary = self._prepare_content_for_analysis(research_data)

        # Si despuÃ©s de preparar el resumen no hay contenido, usa la heurÃ­stica sobre los datos brutos.
        if not content_summary:
            print("No se encontrÃ³ contenido utilizable. Usando heurÃ­stica sobre corpus completo.")
            corpus = self._concat_corpus_from_raw(research_data)
            return self._heuristic_keywords(corpus)

        # Si hay un cliente de IA disponible, intenta usarlo.
        if self.ai_client:
            try:
                # Crea el prompt (la instrucciÃ³n) para el LLM.
                prompt = self._create_extraction_prompt(content_summary)
                # Llama al LLM para obtener una respuesta.
                response = await self.ai_client.chat_completion(prompt, max_tokens=512)
                # Parsea la respuesta del LLM para extraer la lista de palabras clave.
                keywords = self._parse_keywords_from_response(response)
                provider = getattr(self.ai_client, "provider", "ai").capitalize()
                print(f"LLM ({provider}) extrajo {len(keywords)} keywords: {keywords}")
                return keywords
            except Exception as e:
                # Si el LLM falla, informa del error y pasa al mÃ©todo de respaldo.
                print(f"[KeywordExtractor] Fallo con LLM, usando heurÃ­stica. Error: {e}")

        # Si no hay cliente de IA o si fallÃ³, usa el mÃ©todo heurÃ­stico local.
        corpus = self._concat_corpus(content_summary)
        return self._heuristic_keywords(corpus)

    # ---------- MÃ©todos de utilidad internos ----------

    def _prepare_content_for_analysis(self, research_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Compacta los resultados de la investigaciÃ³n para crear un resumen manejable."""
        content_summary: List[Dict[str, Any]] = []
        for data in research_data:
            results = data.get("results", [])
            # Toma como mÃ¡ximo los 3 primeros resultados de cada plataforma para ser conciso.
            for result in results[:3]:
                content_summary.append({
                    "platform": data.get("platform", ""),
                    "keyword": data.get("keyword", ""),
                    "title": result.get("title") or result.get("name") or "",
                    # Acorta la descripciÃ³n a 200 caracteres.
                    "description": (result.get("description") or result.get("snippet") or result.get("abstract") or "")[:200],
                    "topics": result.get("topics", []),
                })
        return content_summary

    def _create_extraction_prompt(self, content_summary: List[Dict[str, Any]]) -> str:
        """Crea el prompt que se enviarÃ¡ al LLM, pidiÃ©ndole que extraiga keywords en formato JSON."""
        return (
            "Analyze this AI trend research data and extract 5-10 new trending keywords related to AI, "
            "machine learning, or technology.\n\n"
            f"Data: {json.dumps(content_summary, indent=2, ensure_ascii=False)}\n\n"
            "Instructions:\n"
            "1. Focus on AI tools, frameworks, companies, techniques, or emerging technologies\n"
            "2. Return only a JSON array of keywords, like: [\"keyword1\", \"keyword2\", \"keyword3\"]\n"
            "3. Prioritize keywords that appear frequently or have high engagement\n"
            "4. Include both English and Japanese keywords if relevant\n"
            "5. If no relevant keywords are found, return an empty array: []\n"
        )

    def _parse_keywords_from_response(self, response: str) -> List[str]:
        """Parsea la respuesta del LLM. Intenta leer un array JSON, y si falla, lo trata como texto plano."""
        if not response:
            return []
        try:
            # Busca una estructura que parezca un array JSON (empieza con [ y termina con ]).
            m = re.search(r"\[.*?\]", response, re.DOTALL)
            if m:
                # Si lo encuentra, intenta decodificarlo como JSON.
                arr = json.loads(m.group())
                # Limpia y devuelve la lista de strings.
                return [s.strip() for s in arr if isinstance(s, str) and s.strip()]
        except Exception as e:
            # Si el parseo JSON falla, lo informa.
            print(f"Error parseando JSON de keywords: {e}")

        # Si no es JSON, lo trata como texto plano separado por comas.
        parts = response.replace("[", "").replace("]", "").replace('"', "")
        kws = [p.strip() for p in parts.split(",") if p.strip()]
        # Devuelve como mÃ¡ximo las 10 primeras.
        return kws[:10]

    def _concat_corpus(self, content_summary: List[Dict[str, Any]]) -> str:
        """Une todos los textos del resumen (tÃ­tulos, descripciones, temas) en un solo bloque de texto (corpus)."""
        parts: List[str] = []
        for item in content_summary:
            parts.append(item.get("title", ""))
            parts.append(item.get("description", ""))
            topics = item.get("topics", [])
            if isinstance(topics, list) and topics:
                parts.extend([str(t) for t in topics])
        return " \n".join([p for p in parts if p])

    def _concat_corpus_from_raw(self, research_data: List[Dict[str, Any]]) -> str:
        """Similar a _concat_corpus, pero trabaja directamente con los datos brutos de investigaciÃ³n."""
        parts: List[str] = []
        for d in research_data:
            for r in d.get("results", []):
                parts.append(r.get("title") or r.get("name") or "")
                parts.append(r.get("description") or r.get("snippet") or r.get("abstract") or "")
                topics = r.get("topics", [])
                if isinstance(topics, list):
                    parts.extend([str(t) for t in topics])
        return " \n".join([p for p in parts if p])

    def _heuristic_keywords(self, corpus: str) -> List[str]:
        """
        MÃ©todo heurÃ­stico de respaldo para extraer keywords.
        Se basa en encontrar las palabras y frases (n-gramas) mÃ¡s frecuentes.
        """
        if not corpus:
            return []

        text = corpus.lower()

        # Extrae tokens (palabras) que parecen relevantes.
        tokens = re.findall(r"[a-z0-9][a-z0-9\-_/\.]{2,}", text)
        # Define una lista de palabras comunes (stop words) para ignorar.
        stop = {
            "https", "http", "www", "com", "org", "from", "with", "that", "this", "what", "when",
            "your", "have", "about", "into", "like", "will", "there", "their", "been", "make",
            "only", "some", "more", "over", "also", "than", "which", "were", "after", "before",
            "because", "could", "should", "would"
        }
        tokens = [t for t in tokens if t not in stop]
        # Obtiene las 20 palabras mÃ¡s comunes.
        singles = [w for w, _ in Counter(tokens).most_common(20)]

        # Busca frases de 2 palabras (bigramas) y 3 palabras (trigramas).
        words = re.findall(r"[a-z0-9]+", text)
        bigrams = [" ".join(words[i:i+2]) for i in range(len(words)-1)]
        trigrams = [" ".join(words[i:i+3]) for i in range(len(words)-2)]
        # Cuenta la frecuencia de los n-gramas mÃ¡s relevantes.
        bf = Counter([b for b in bigrams if len(b) > 6])
        tf = Counter([t for t in trigrams if len(t) > 8])

        # Combina las palabras sueltas y los n-gramas mÃ¡s comunes.
        candidates = singles + [w for w, _ in bf.most_common(10)] + [w for w, _ in tf.most_common(10)]
        # Normaliza y limpia la lista final.
        return self._normalize_keywords(candidates)

    def _normalize_keywords(self, kws: List[str]) -> List[str]:
        """Limpia una lista de keywords: convierte a minÃºsculas, quita espacios y duplicados."""
        out: List[str] = []
        for kw in kws:
            k = re.sub(r"\s+", " ", kw.lower()).strip()
            k = k.strip(" .,:;-/\\|\"'()[]{}")
            if len(k) >= 3:
                out.append(k)
        # Elimina duplicados manteniendo el orden.
        seen = set()
        uniq = []
        for k in out:
            if k not in seen:
                seen.add(k)
                uniq.append(k)
        return uniq


class DataAnalyzer:
    """Analiza los datos recolectados para calcular mÃ©tricas, puntuar keywords y generar recomendaciones."""
    
    def __init__(self, ai_client_manager: AIClientManager = None):
        """Constructor. Recibe el gestor de cliente de IA para generar recomendaciones dinÃ¡micas."""
        self.ai_client = ai_client_manager

    def score_keywords(self, new_keywords: List[str], research_data: List[Dict[str, Any]]) -> Dict[str, int]:
        """Asigna una puntuaciÃ³n a cada nueva palabra clave basada en su frecuencia en los resultados de la investigaciÃ³n."""
        if not new_keywords:
            return {}

        # Crea un gran bloque de texto (corpus) con todos los tÃ­tulos, descripciones y temas.
        parts: List[str] = []
        for item in research_data:
            for r in item.get("results", []):
                title = r.get("title") or r.get("name") or ""
                desc = r.get("description") or r.get("snippet") or r.get("abstract") or ""
                topics = r.get("topics") or []
                parts.append(title)
                parts.append(desc)
                if isinstance(topics, list):
                    parts.extend([str(t) for t in topics])
        text = " \n".join([p for p in parts if p]).lower()

        # Cuenta cuÃ¡ntas veces aparece cada nueva palabra clave en el corpus.
        hits_map: Dict[str, int] = {}
        max_hits = 1
        for kw in new_keywords:
            if not kw:
                continue
            hits = len(re.findall(re.escape(kw.lower()), text))
            hits_map[kw] = hits
            if hits > max_hits:
                max_hits = hits

        # Normaliza las puntuaciones en una escala de 0 a 100 usando una escala logarÃ­tmica.
        import math
        scores: Dict[str, int] = {}
        for kw, h in hits_map.items():
            norm = math.log1p(h) / math.log1p(max_hits) if max_hits > 0 else 0.0
            scores[kw] = int(round(norm * 100))
        return scores

    def calculate_summary_stats(self, research_data: List[Dict[str, Any]], new_keywords: List[str]) -> Dict[str, Any]:
        """Calcula estadÃ­sticas agregadas bÃ¡sicas sobre la ejecuciÃ³n de la investigaciÃ³n."""
        # Cuenta cuÃ¡ntas ejecuciones se hicieron por plataforma.
        per_platform = Counter([d.get("platform", "unknown") for d in research_data])
        # Suma el total de resultados obtenidos.
        total_results = sum(len(d.get("results", [])) for d in research_data)
        # Cuenta cuÃ¡ntas ejecuciones tuvieron errores.
        runs_with_errors = sum(1 for d in research_data if d.get("error"))

        return {
            "timestamp": datetime.now().isoformat(),
            "platform_breakdown": dict(per_platform),
            "total_items": total_results,
            "new_keywords_count": len(new_keywords),
            "runs_with_errors": runs_with_errors,
        }

    async def generate_recommendations(self, research_data: List[Dict[str, Any]], new_keywords: List[str]) -> List[str]:
        """Genera una lista de recomendaciones de acciÃ³n, usando IA si estÃ¡ disponible."""
        # Si no hay cliente de IA o no hay datos, usa el mÃ©todo de respaldo.
        if not self.ai_client or (not research_data and not new_keywords):
            return self._heuristic_recommendations(research_data, new_keywords)

        try:
            prompt = self._create_recommendation_prompt(research_data, new_keywords)
            response = await self.ai_client.chat_completion(prompt, max_tokens=512)
            # Parsea la respuesta en una lista de strings.
            recommendations = [rec.strip("- ").strip() for rec in response.split("\n") if rec.strip("- ").strip()]
            return recommendations if recommendations else ["No specific recommendations generated."]
        except Exception as e:
            print(f"Error generando recomendaciones con IA, usando heurÃ­stica. Error: {e}")
            return self._heuristic_recommendations(research_data, new_keywords)

    def _create_recommendation_prompt(self, research_data: List[Dict[str, Any]], new_keywords: List[str]) -> str:
        """Crea el prompt para que el LLM genere recomendaciones."""
        summary_stats = self.calculate_summary_stats(research_data, new_keywords)
        
        # Prepara un resumen de los hallazgos mÃ¡s importantes para el prompt.
        top_findings = []
        for data in research_data:
            platform = data.get("platform")
            if data.get("results"):
                top_result = data["results"][0]
                title = top_result.get("title") or top_result.get("name")
                if title:
                    top_findings.append(f"- From {platform}: Found '{title}' related to '{data.get('keyword')}'.")
        
        return (
            "You are an AI research analyst. Based on the following summary of a trend investigation, "
            "provide 3-5 actionable and insightful recommendations for a research team. "
            "Focus on what to investigate next, what technologies seem promising, and potential content ideas.\n\n"
            f"--- Data Summary ---\n"
            f"Total items found: {summary_stats['total_items']}\n"
            f"Platforms with most results: {', '.join(summary_stats['platform_breakdown'].keys())}\n"
            f"New keywords discovered: {', '.join(new_keywords)}\n"
            f"Top findings:\n{''.join(top_findings[:5])}\n"
            f"--- End of Summary ---\n\n"
            "Generate the recommendations as a bulleted list (e.g., - Recommendation 1). Do not add any introductory text."
        )

    def _heuristic_recommendations(self, research_data: List[Dict[str, Any]], new_keywords: List[str]) -> List[str]:
        """Genera una lista de recomendaciones de acciÃ³n simples basadas en los resultados."""
        recs: List[str] = []
        platform_counts = Counter([d.get("platform", "unknown") for d in research_data])

        # AÃ±ade recomendaciones especÃ­ficas segÃºn las plataformas que devolvieron datos.
        if platform_counts.get("github", 0) > 0:
            recs.append("Priorizar repositorios con una alta tasa de estrellas (star_rate) y actividad reciente.")
        if platform_counts.get("web", 0) > 0:
            recs.append("Revisar los resultados web en japonÃ©s para detectar tÃ©rminos emergentes locales.")
        if platform_counts.get("arxiv", 0) > 0:
            recs.append("Leer los abstracts recientes de arXiv (â‰¤ 30 dÃ­as) para captar nuevas lÃ­neas de investigaciÃ³n.")
        
        # Si no hay recomendaciones, sugiere ampliar la bÃºsqueda.
        if not recs:
            recs.append("Ampliar las fuentes o las palabras clave para obtener mÃ¡s seÃ±ales.")
        
        # Si se encontraron nuevas keywords, sugiere explorarlas.
        if new_keywords:
            recs.append(f"Explorar en profundidad las nuevas keywords descubiertas: {', '.join(new_keywords[:5])}...")

        return recs



---
File: /keyword_manager.py
---

import json
import os
from datetime import datetime
from typing import Dict, List, Optional, Any, TypedDict

# Define un tipo para la metadata de las keywords para mejorar la legibilidad y el autocompletado.
class KeywordMetadata(TypedDict, total=False):
    score: int
    status: str
    source: str
    created_date: str
    last_used: Optional[str]
    discovered_from: Optional[str]

MasterKeywords = Dict[str, KeywordMetadata]

class KeywordManager:
    """
    Gestiona el ciclo de vida de las palabras clave con persistencia en archivos JSON.
    Maneja tres archivos principales:
      - keywords/master.json: Un catÃ¡logo de todas las keywords descubiertas con sus metadatos.
      - keywords/active.json: Una lista simple de las keywords a investigar en la prÃ³xima ejecuciÃ³n.
      - keywords/history.json: Un registro de las ejecuciones pasadas.
    """

    def __init__(self, keywords_dir: str = "keywords"):
        """Constructor. Define las rutas a los archivos y se asegura de que existan."""
        self.keywords_dir = keywords_dir
        self.master_file = os.path.join(keywords_dir, "master.json")
        self.active_file = os.path.join(keywords_dir, "active.json")
        self.history_file = os.path.join(keywords_dir, "history.json")

        # Asegura que el directorio 'keywords' exista.
        os.makedirs(self.keywords_dir, exist_ok=True)
        # Si los archivos JSON no existen, los crea con un contenido inicial vacÃ­o.
        if not os.path.exists(self.master_file):
            self._atomic_write(self.master_file, {})
        if not os.path.exists(self.active_file):
            self._atomic_write(self.active_file, [])
        if not os.path.exists(self.history_file):
            self._atomic_write(self.history_file, {})

    # ---------------------------------
    # MÃ©todos para cargar/guardar JSON
    # ---------------------------------
    def load_master_keywords(self) -> MasterKeywords:
        """Carga el catÃ¡logo maestro de keywords desde master.json."""
        try:
            with open(self.master_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # Devuelve los datos solo si son un diccionario, para evitar errores.
                return data if isinstance(data, dict) else {}
        except (FileNotFoundError, json.JSONDecodeError):
            # Si hay algÃºn error (archivo no encontrado, JSON mal formado), devuelve un diccionario vacÃ­o.
            return {}

    def load_active_keywords(self) -> List[str]:
        """Carga la lista de keywords activas desde active.json."""
        try:
            with open(self.active_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # Devuelve los datos solo si son una lista.
                return data if isinstance(data, list) else []
        except (FileNotFoundError, json.JSONDecodeError):
            # En caso de error, devuelve una lista vacÃ­a.
            return []

    def load_history(self) -> Dict[str, Any]:
        """Carga el historial de ejecuciones desde history.json."""
        try:
            with open(self.history_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # Devuelve los datos solo si son un diccionario.
                return data if isinstance(data, dict) else {}
        except (FileNotFoundError, json.JSONDecodeError):
            # En caso de error, devuelve un diccionario vacÃ­o.
            return {}

    def save_master_keywords(self, keywords: MasterKeywords):
        """Guarda el catÃ¡logo maestro de keywords en master.json."""
        self._atomic_write(self.master_file, keywords)

    def save_active_keywords(self, keywords: List[str]):
        """Guarda la lista de keywords activas en active.json."""
        self._atomic_write(self.active_file, keywords)

    def save_history(self, history: Dict[str, Any]):
        """Guarda el historial de ejecuciones en history.json."""
        self._atomic_write(self.history_file, history)

    def _atomic_write(self, path: str, data: Any):
        """
        Realiza una escritura "atÃ³mica" para evitar la corrupciÃ³n de archivos.
        Primero escribe en un archivo temporal (.tmp) y, si tiene Ã©xito, lo renombra al archivo final.
        """
        tmp_path = path + ".tmp"
        try:
            with open(tmp_path, "w", encoding="utf-8") as f:
                # Vuelca los datos al archivo JSON con formato legible.
                json.dump(data, f, ensure_ascii=False, indent=2)
            # Reemplaza el archivo original con el nuevo archivo temporal.
            os.replace(tmp_path, path)
        except Exception as e:
            print(f"Error durante la escritura atÃ³mica en {path}: {e}")
            # Si hubo un error, intenta eliminar el archivo temporal si existe.
            if os.path.exists(tmp_path):
                os.remove(tmp_path)


    # ---------------------------------
    # API pÃºblica para gestionar keywords
    # ---------------------------------
    def add_new_keyword(
        self,
        keyword: str,
        score: int,
        status: str,
        source: str,
        discovered_from: Optional[str] = None
    ) -> bool:
        """
        AÃ±ade una nueva palabra clave al catÃ¡logo maestro si no existe previamente.
        Devuelve True si la keyword fue aÃ±adida, False si ya existÃ­a.
        """
        # Limpia la keyword de espacios en blanco.
        keyword = (keyword or "").strip()
        if not keyword:
            return False

        master = self.load_master_keywords()
        # Si la keyword ya estÃ¡ en el catÃ¡logo, no hace nada.
        if keyword in master:
            return False

        # Crea la nueva entrada para la palabra clave.
        now_date = datetime.now().strftime("%Y-%m-%d")
        entry: KeywordMetadata = {
            "score": int(score),
            "status": status,
            "source": source,
            "created_date": now_date,
            "last_used": None  # AÃºn no se ha usado para investigar.
        }
        if discovered_from:
            entry["discovered_from"] = discovered_from

        # AÃ±ade la nueva entrada al catÃ¡logo y lo guarda.
        master[keyword] = entry
        self.save_master_keywords(master)
        return True

    def update_keyword_score(self, keyword: str, new_score: int) -> bool:
        """Actualiza la puntuaciÃ³n de una palabra clave existente."""
        master = self.load_master_keywords()
        if keyword in master:
            master[keyword]["score"] = int(new_score)
            self.save_master_keywords(master)
            return True
        return False

    def mark_keywords_used(self, keywords: List[str]) -> None:
        """Marca una lista de palabras clave como 'usadas' en la fecha actual."""
        if not keywords:
            return

        master = self.load_master_keywords()
        today = datetime.now().strftime("%Y-%m-%d")

        changed = False
        for kw in keywords:
            kw = (kw or "").strip()
            if not kw:
                continue
            # Si la keyword no existÃ­a por alguna razÃ³n, la crea con datos por defecto.
            if kw not in master:
                master[kw] = {
                    "score": 0, "status": "unknown", "source": "runtime",
                    "created_date": today, "last_used": today
                }
                changed = True
            else:
                # Actualiza la fecha del Ãºltimo uso.
                master[kw]["last_used"] = today
                changed = True

        # Guarda los cambios solo si se realizÃ³ alguna modificaciÃ³n.
        if changed:
            self.save_master_keywords(master)

    def record_execution(self, keywords: List[str], status: str = "completed", new_keywords_found: int = 0) -> None:
        """Registra un resumen de la ejecuciÃ³n actual en el archivo de historial."""
        history = self.load_history()
        today = datetime.now().strftime("%Y-%m-%d")
        # Crea o sobrescribe la entrada para el dÃ­a de hoy.
        history[today] = {
            "keywords_used": keywords,
            "execution_time": datetime.now().strftime("%H:%M:%S"),
            "status": status,
            "new_keywords_found": int(new_keywords_found)
        }
        self.save_history(history)

    def get_top_keywords(self, limit: int = 10) -> List[str]:
        """Devuelve una lista de las N mejores keywords segÃºn su puntuaciÃ³n y fecha de Ãºltimo uso."""
        master = self.load_master_keywords()
        
        # Define una funciÃ³n de ordenaciÃ³n compleja:
        def sort_key(item: tuple[str, KeywordMetadata]):
            kw, meta = item
            score = int(meta.get("score", 0))
            # Trata 'None' o '' como la fecha mÃ¡s antigua para priorizar keywords nunca usadas.
            last_used = meta.get("last_used") or "1970-01-01" 
            # Ordena por puntuaciÃ³n descendente (-score) y luego por fecha de Ãºltimo uso ascendente.
            return (-score, last_used)

        # Ordena los Ã­tems del catÃ¡logo usando la clave definida.
        sorted_items = sorted(master.items(), key=sort_key)
        # Devuelve solo los nombres de las keywords del top N.
        return [kw for kw, _ in sorted_items[:limit]]

    def refresh_active_keywords(self, limit: int = 5) -> List[str]:
        """
        Selecciona las mejores keywords del catÃ¡logo maestro y las guarda en active.json
        para que sean usadas en la prÃ³xima ejecuciÃ³n.
        """
        top_keywords = self.get_top_keywords(limit=limit)
        self.save_active_keywords(top_keywords)
        return top_keywords



---
File: /mcp_client_manager.py
---

# mcp_client_manager.py
# -*- coding: utf-8 -*-

# Importa el mÃ³dulo 'asyncio' para la programaciÃ³n asÃ­ncrona.
import asyncio
# Importa el mÃ³dulo 'os' para leer variables de entorno.
import os
# Importa herramientas de 'typing' para anotaciones de tipo.
from typing import Dict, List, Any, Optional
# De 'contextlib', importa 'AsyncExitStack' para gestionar mÃºltiples contextos asÃ­ncronos de forma segura.
from contextlib import AsyncExitStack

# Importa las clases necesarias de la biblioteca 'mcp'.
from mcp import ClientSession
from mcp.client.stdio import stdio_client, StdioServerParameters


class RemoteMCPClient:
    """
    Representa un cliente para un Ãºnico servidor MCP que se ejecuta como un proceso local (ej. iniciado con npx).
    Gestiona la conexiÃ³n, los reintentos, las llamadas a herramientas y el cierre seguro.
    """

    def __init__(self):
        """Constructor. Inicializa el estado del cliente."""
        self.session: Optional[ClientSession] = None  # La sesiÃ³n de comunicaciÃ³n MCP.
        self.exit_stack: Optional[AsyncExitStack] = None  # Para gestionar recursos asÃ­ncronos.
        self._connected: bool = False  # Flag para indicar si la conexiÃ³n estÃ¡ activa.
        self._cleanup_attempted: bool = False  # Flag para evitar limpiezas duplicadas.
        self._available_tools: List[str] = []  # Lista de herramientas que ofrece el servidor.

    async def connect_to_server_by_name(
        self,
        server_name: str,
        args: List[str] = None,
        env: Dict[str, Any] = None
    ) -> bool:
        """
        Establece una conexiÃ³n con un servidor MCP a travÃ©s de su entrada/salida estÃ¡ndar (stdio).
        Implementa una lÃ³gica de reintentos y timeouts adaptables.
        """
        args = args or []
        joined_args = " ".join(args)

        # HeurÃ­stica para definir timeouts de conexiÃ³n mÃ¡s largos para servidores que tardan mÃ¡s en arrancar.
        base_timeout = 15.0
        if "one-search-mcp" in server_name or "one-search-mcp" in joined_args:
            init_timeout = 45.0
        elif "@langgpt/arxiv-mcp-server" in joined_args:
            init_timeout = 60.0
        else:
            init_timeout = base_timeout

        # Permite sobrescribir el timeout globalmente mediante una variable de entorno.
        try:
            init_timeout = float(os.getenv("MCP_INIT_TIMEOUT", init_timeout))
        except (ValueError, TypeError):
            pass

        # Prepara el diccionario de entorno, limpiÃ¡ndolo de valores nulos o vacÃ­os.
        clean_env: Optional[Dict[str, str]] = None
        if env:
            cleaned = {k: str(v) for k, v in env.items() if v not in (None, "")}
            clean_env = cleaned if cleaned else None

        attempts = 2  # NÃºmero de intentos de conexiÃ³n (1 original + 1 reintento).
        last_err: Optional[BaseException] = None
        full_cmd = " ".join([server_name] + args)

        # Bucle de intentos de conexiÃ³n.
        for attempt in range(1, attempts + 1):
            try:
                # Prepara un 'AsyncExitStack' para este intento.
                self.exit_stack = AsyncExitStack()

                print(f"[MCP] Conectando a '{os.path.basename(server_name)}' (intento {attempt}/{attempts})")
                if clean_env:
                    print(f"      â”–â”€ Entorno: {list(clean_env.keys())}")
                print(f"      â”–â”€ Timeout: {int(init_timeout)}s")


                # Define los parÃ¡metros para iniciar el servidor como un subproceso.
                server_params = StdioServerParameters(
                    command=server_name,
                    args=args,
                    env=clean_env
                )

                # Inicia el cliente stdio, que a su vez lanza el proceso del servidor.
                stdio_context = stdio_client(server_params)
                # Entra en el contexto del cliente para obtener los streams de lectura y escritura.
                read_stream, write_stream = await self.exit_stack.enter_async_context(stdio_context)

                # Crea una sesiÃ³n MCP usando los streams.
                session_context = ClientSession(read_stream, write_stream)
                self.session = await self.exit_stack.enter_async_context(session_context)

                # Llama al mÃ©todo 'initialize' del servidor con un tiempo de espera.
                try:
                    await asyncio.wait_for(self.session.initialize(), timeout=init_timeout)
                except asyncio.TimeoutError:
                    raise TimeoutError(f"Timeout en initialize() para '{os.path.basename(server_name)}'")

                # Si la inicializaciÃ³n es exitosa, obtiene la lista de herramientas disponibles.
                response = await self.session.list_tools()
                tools = response.tools
                self._available_tools = [tool.name for tool in tools]
                print(f"  âœ“ ConexiÃ³n exitosa a '{os.path.basename(server_name)}' | Herramientas: {self._available_tools}")

                self._connected = True
                return True  # ConexiÃ³n exitosa, sale del bucle.

            except Exception as e:
                # Si ocurre un error, lo registra y se prepara para el siguiente intento.
                last_err = e
                print(f"  âœ— Error al conectar '{os.path.basename(server_name)}' (intento {attempt}/{attempts}): {e}")

                # Limpia los recursos del intento fallido.
                try:
                    if self.exit_stack:
                        await asyncio.wait_for(self.exit_stack.aclose(), timeout=5.0)
                except Exception as close_err:
                    print(f"    Aviso: Error durante la limpieza del intento fallido: {close_err}")


                self.session = None
                self.exit_stack = None
                self._connected = False

                # Espera un poco antes de reintentar.
                if attempt < attempts:
                    await asyncio.sleep(2.0)

        print(f"  âœ— Fallo definitivo conectando a '{os.path.basename(server_name)}': {last_err}")
        return False

    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]):
        """Llama a una herramienta especÃ­fica del servidor MCP conectado."""
        if not self.session or not self._connected:
            raise ConnectionError("No estÃ¡ conectado a ningÃºn servidor MCP para llamar a la herramienta.")

        try:
            # Llama a la herramienta y espera la respuesta.
            response = await self.session.call_tool(tool_name, arguments)
            # Devuelve el contenido principal de la respuesta, que puede estar en 'content' o 'result'.
            if hasattr(response, "content"):
                return response.content
            if hasattr(response, "result"):
                return response.result
            return response
        except Exception as e:
            print(f"âœ— Error al llamar a la herramienta '{tool_name}': {e}")
            # Devuelve None o relanza una excepciÃ³n mÃ¡s especÃ­fica.
            return None

    def get_available_tools(self) -> List[str]:
        """Devuelve la lista de nombres de herramientas disponibles en el servidor."""
        return self._available_tools

    async def _cleanup(self):
        """MÃ©todo privado para cerrar y limpiar los recursos del cliente de forma segura."""
        if self._cleanup_attempted:
            return
        self._cleanup_attempted = True

        try:
            # Usa el 'AsyncExitStack' para cerrar todos los contextos abiertos (sesiÃ³n, proceso, etc.).
            if self.exit_stack:
                await asyncio.wait_for(self.exit_stack.aclose(), timeout=5.0)
        except asyncio.TimeoutError:
            print("Aviso: Tiempo de espera de limpieza agotado, forzando cierre")
        except asyncio.CancelledError:
            print("Aviso: La limpieza fue cancelada")
        except Exception as e:
            print(f"Aviso: Error durante la limpieza: {e}")
        finally:
            self.exit_stack = None

    async def close(self):
        """MÃ©todo pÃºblico para cerrar la conexiÃ³n con el servidor de forma segura."""
        if not self._connected:
            return
        self._connected = False
        try:
            # Llama al mÃ©todo de limpieza con un tiempo de espera.
            await asyncio.wait_for(self._cleanup(), timeout=10.0)
        except Exception as e:
            print(f"Aviso: Error durante el cierre: {e}")
        finally:
            # Resetea el estado del cliente.
            self.session = None
            self.exit_stack = None


class MCPClientManager:
    """
    Gestiona un conjunto de mÃºltiples 'RemoteMCPClient', uno para cada plataforma.
    Orquesta la conexiÃ³n y desconexiÃ³n de todos ellos.
    """

    def __init__(self, server_configs: Dict[str, Dict]):
        """Constructor. Recibe las configuraciones de todos los servidores."""
        self.server_configs = server_configs
        # Diccionario para almacenar las instancias de cliente, una por plataforma.
        self.clients: Dict[str, Optional[RemoteMCPClient]] = {}

    async def connect_all_servers(self):
        """Intenta conectar a todos los servidores que estÃ¡n marcados como habilitados en la configuraciÃ³n."""
        print("\n[MCP] Conectando a todos los servidores habilitados...")
        
        # Crea tareas para conectar a todos los servidores en paralelo.
        tasks = [
            self._connect_single_server(platform, config)
            for platform, config in self.server_configs.items()
            if config.get("enabled", False)
        ]
        
        # Ejecuta las tareas de conexiÃ³n concurrentemente.
        await asyncio.gather(*tasks)

        # Imprime los servidores omitidos.
        for platform, config in self.server_configs.items():
            if not config.get("enabled", False):
                print(f"  â†· Omitido '{platform}' (deshabilitado en config)")


    async def _connect_single_server(self, platform: str, config: Dict):
        """Crea un cliente y intenta conectar a un Ãºnico servidor MCP."""
        try:
            mcp_client = RemoteMCPClient()
            args = config.get("args", [])
            env = config.get("env", {})
            # Llama al mÃ©todo de conexiÃ³n del cliente.
            success = await mcp_client.connect_to_server_by_name(config["server_name"], args, env)

            # Si la conexiÃ³n es exitosa, almacena el cliente. Si no, almacena None.
            self.clients[platform] = mcp_client if success else None
        except Exception as e:
            print(f"  âœ— Fallo crÃ­tico al inicializar la conexiÃ³n para {platform}: {e}")
            self.clients[platform] = None

    def get_client(self, platform: str) -> Optional[RemoteMCPClient]:
        """Devuelve la instancia del cliente para una plataforma, o None si no estÃ¡ conectado."""
        return self.clients.get(platform)

    def is_platform_available(self, platform: str) -> bool:
        """Comprueba si el cliente de una plataforma estÃ¡ conectado y disponible."""
        client = self.clients.get(platform)
        return client is not None and client._connected

    def get_available_tools(self, platform: str) -> List[str]:
        """Obtiene la lista de herramientas disponibles para una plataforma especÃ­fica."""
        client = self.get_client(platform)
        return client.get_available_tools() if client else []

    async def close_all_clients(self):
        """
        Cierra todos los clientes MCP conectados de forma SECUENCIAL.
        Esto es importante en asyncio para evitar problemas de cancelaciÃ³n de tareas.
        """
        print("[MCP] Cerrando todos los clientes...")
        # Itera sobre una copia de los Ã­tems para poder modificar el diccionario original.
        for platform, client in list(self.clients.items()):
            if client:
                try:
                    print(f"      â”–â”€ Cerrando '{platform}'...")
                    await client.close()  # Espera a que cada cliente se cierre antes de pasar al siguiente.
                except Exception as e:
                    print(f"      â”–â”€ Error al cerrar '{platform}': {e}")
        # Limpia el diccionario de clientes.
        self.clients.clear()
        print("[MCP] Todos los clientes cerrados.")



---
File: /platform_handlers.py
---

# platform_handlers.py
# -*- coding: utf-8 -*-

# Este archivo es una versiÃ³n consolidada y mejorada que fusiona la lÃ³gica de
# platform_manager.py y platform_handlers.py, eliminando la redundancia.

from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from datetime import datetime
import json
import re
from collections import Counter
from urllib.parse import urlparse

# NOTA: No se importa AIClientManager aquÃ­ para evitar una dependencia circular.
# Se pasa como un argumento en el mÃ©todo de la fÃ¡brica 'create_handler'.

# ---------------- Clase Base ----------------
class BasePlatformHandler(ABC):
    """
    Define la plantilla (interfaz) que todos los manejadores de plataforma deben seguir.
    Garantiza una estructura consistente.
    """
    def __init__(self, platform_name: str):
        """Constructor. Almacena el nombre de la plataforma."""
        self.platform_name = platform_name
    
    @abstractmethod
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        """MÃ©todo abstracto para la lÃ³gica de investigaciÃ³n. Debe ser implementado por las subclases."""
        pass
    
    @abstractmethod
    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        """MÃ©todo abstracto para procesar la respuesta cruda. Debe ser implementado por las subclases."""
        pass
    
    def create_error_result(self, keyword: str, error: str) -> Dict[str, Any]:
        """MÃ©todo de utilidad para crear un resultado de error estandarizado."""
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": [], "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": {}, "error": str(error)
        }

# ---------------- Manejador de YouTube ----------------
class YouTubeHandler(BasePlatformHandler):
    """Manejador con la lÃ³gica especÃ­fica para investigar en YouTube."""
    def __init__(self):
        super().__init__("youtube")
    
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        try:
            params = {"query": keyword, "order": "relevance", "type": "video", "max_results": 10}
            tool_name = config["tools"][0]
            response = await client.call_tool(tool_name, params)
            return self.process_response(response, keyword)
        except Exception as e:
            return self.create_error_result(keyword, str(e))
    
    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        results = []
        data = self._extract_data_from_response(response)
        videos = data.get('videos', data.get('items', [])) if isinstance(data, dict) else (data if isinstance(data, list) else [])
        
        for video in videos:
            snippet = video.get('snippet', {})
            video_id = video.get('id', {}).get('videoId', '')
            if not video_id: continue

            results.append({
                'title': snippet.get('title', ''), 'description': snippet.get('description', ''),
                'published_at': snippet.get('publishedAt', ''), 'channel': snippet.get('channelTitle', ''),
                'video_id': video_id, 'url': f"https://www.youtube.com/watch?v={video_id}",
                'content_type': self._classify_content(snippet.get('title', ''), snippet.get('description', '')),
                'language': self._detect_language(snippet.get('title', '') + ' ' + snippet.get('description', ''))
            })
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": results, "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": self._calculate_engagement_metrics(results)
        }
    
    def _extract_data_from_response(self, response: Any) -> Any:
        if isinstance(response, list) and len(response) > 0 and hasattr(response[0], 'text'):
            try: return json.loads(response[0].text)
            except (json.JSONDecodeError, AttributeError): return {}
        elif hasattr(response, 'content'): return response.content
        return response if isinstance(response, (dict, list)) else {}

    def _classify_content(self, title: str, description: str) -> str:
        text = (title + ' ' + description).lower()
        if any(k in text for k in ['è§£èª¬', 'èª¬æ˜Ž', 'å…¥é–€', 'åŸºç¤Ž', 'å­¦ç¿’', 'ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«']): return "è§£èª¬å‹•ç”»"
        elif any(k in text for k in ['ãƒ‡ãƒ¢', 'ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³', 'å®Ÿæ¼”', 'ã‚µãƒ³ãƒ—ãƒ«']): return "ãƒ‡ãƒ¢"
        elif any(k in text for k in ['ã‚«ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹', 'ã‚»ãƒŸãƒŠãƒ¼', 'è¬›æ¼”', 'ç™ºè¡¨', 'talk']): return "ã‚«ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹"
        elif any(k in text for k in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ€æ–°', 'ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ', 'ãƒªãƒªãƒ¼ã‚¹']): return "ãƒ‹ãƒ¥ãƒ¼ã‚¹"
        else: return "ãã®ä»–"
    
    def _detect_language(self, text: str) -> str:
        if re.search(r'[\u3040-\u30ff\u3400-\u4dbf\u4e00-\u9fff]', text): return "ja"
        return "en"
    
    def _calculate_engagement_metrics(self, results: List[Dict]) -> Dict:
        return {"total_videos": len(results)}

# ---------------- Manejador de GitHub ----------------
class GitHubHandler(BasePlatformHandler):
    """Manejador robusto para investigar en GitHub."""
    def __init__(self):
        super().__init__("github")
    
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        try:
            params = {
                "query": f"{keyword} stars:>50", "sort": "stars",
                "order": "desc", "per_page": 10
            }
            tool_name = "search_repositories"
            response = await client.call_tool(tool_name, params)
            return self.process_response(response, keyword)
        except Exception as e:
            return self.create_error_result(keyword, str(e))
    
    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        results = []
        repos = self._extract_repositories(response)
        for repo in repos:
            if isinstance(repo, dict):
                stars = repo.get('stargazers_count', repo.get('stars', 0))
                created_at = repo.get('created_at', '')
                star_rate, days_old, is_trending = self._calculate_trend_metrics(stars, created_at)
                results.append({
                    'name': repo.get('name', ''), 'description': repo.get('description', ''),
                    'owner': repo.get('owner', {}).get('login', ''), 'stars': stars, 'language': repo.get('language', ''),
                    'url': repo.get('html_url', ''), 'created_at': created_at, 'topics': repo.get('topics', []),
                    'star_rate': round(star_rate, 2), 'days_old': days_old, 'is_trending': is_trending,
                    'trend_score': self._calculate_trend_score(stars, days_old, star_rate)
                })
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": results, "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": self._calculate_engagement_metrics(results)
        }
    
    def _extract_repositories(self, response: Any) -> List[Dict]:
        if not response: return []
        if isinstance(response, str):
            try: return self._extract_repositories(json.loads(response))
            except json.JSONDecodeError: return []
        if isinstance(response, list) and len(response) > 0 and hasattr(response[0], 'text'):
            try: return self._extract_repositories(json.loads(response[0].text))
            except (json.JSONDecodeError, AttributeError): return []
        if isinstance(response, list): return response
        if isinstance(response, dict): return response.get('items', response.get('repositories', []))
        return []

    def _calculate_trend_metrics(self, stars: int, created_at: str) -> tuple:
        if not created_at or not isinstance(stars, int): return (0.0, 0, False)
        try:
            created_date = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
            days_old = (datetime.now(created_date.tzinfo) - created_date).days
            if days_old <= 0: return (stars, 0, False)
            star_rate = stars / days_old
            is_trending = (stars >= 100 and days_old <= 365 and star_rate > 0.5)
            return (star_rate, days_old, is_trending)
        except (ValueError, TypeError): return (0.0, 0, False)
    
    def _calculate_trend_score(self, stars: int, days_old: int, star_rate: float) -> float:
        if days_old <= 0: return 0.0
        base_score = min(star_rate * 10, 50)
        recency_bonus = max(0, (365 - days_old) / 365 * 30)
        star_bonus = min(stars / 200, 20)
        return round(min(base_score + recency_bonus + star_bonus, 100), 2)
    
    def _calculate_engagement_metrics(self, results: List[Dict]) -> Dict:
        if not results: return {"repo_count": 0}
        languages = [r.get('language') for r in results if r.get('language')]
        return {
            "repo_count": len(results),
            "total_stars": sum(r.get('stars', 0) for r in results),
            "trending_repos_count": sum(1 for r in results if r.get('is_trending')),
            "top_languages": dict(Counter(languages).most_common(3)) if languages else {}
        }

# ---------------- Manejador de Web ----------------
class WebHandler(BasePlatformHandler):
    def __init__(self):
        super().__init__("web")
    
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        try:
            params = {"query": f"{keyword} site:github.com OR site:arxiv.org OR site:huggingface.co", "language": "ja", "region": "jp", "max_results": 10}
            tool_name = config["tools"][0]
            response = await client.call_tool(tool_name, params)
            return self.process_response(response, keyword)
        except Exception as e:
            return self.create_error_result(keyword, str(e))
    
    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        results = self._parse_web_results(response)
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": results, "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": {"search_count": len(results)}
        }
    
    def _parse_web_results(self, response: Any) -> List[Dict]:
        if isinstance(response, list) and len(response) > 0 and hasattr(response[0], 'text'):
            try: response = json.loads(response[0].text)
            except (json.JSONDecodeError, AttributeError): response = {}
        
        results = []
        web_results = response.get('results', []) if isinstance(response, dict) else []
        for result in web_results:
            url = result.get('url', result.get('link', ''))
            results.append({
                'title': result.get('title', ''), 'snippet': result.get('snippet', ''),
                'url': url, 'source': urlparse(url).netloc if url else ''
            })
        return results

# ---------------- Manejador de ArXiv ----------------
class ArxivHandler(BasePlatformHandler):
    def __init__(self, ai_client_manager: Optional[Any] = None):
        super().__init__("arxiv")
        self.ai_client = ai_client_manager
    
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        try:
            english_query = await self._translate_keyword(keyword)
            params = {"query": english_query, "max_results": 10, "sort_by": "relevance"}
            response = await client.call_tool("search_arxiv", params)
            return self.process_response(response, keyword)
        except Exception as e:
            return self.create_error_result(keyword, str(e))

    async def _translate_keyword(self, keyword: str) -> str:
        if not self.ai_client or not re.search(r'[\u3040-\u30ff]', keyword): return keyword
        try:
            prompt = f"Translate the following Japanese technical keyword to English for an ArXiv search. Provide only the English translation, no extra text. Keyword: '{keyword}'"
            translation = await self.ai_client.chat_completion(prompt)
            return translation.strip().replace('"', '') or keyword
        except Exception: return keyword
            
    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        results = []
        papers = self._extract_papers(response)
        for paper in papers:
            if isinstance(paper, dict):
                published_date = paper.get('published', '')
                days_old, is_recent = self._calculate_time_metrics(published_date)
                results.append({
                    'title': paper.get('title', ''), 'abstract': paper.get('summary', ''),
                    'authors': paper.get('authors', []), 'published_date': published_date,
                    'url': paper.get('url', ''), 'days_old': days_old, 'is_recent': is_recent,
                })
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": results, "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": {"paper_count": len(results), "recent_paper_count": sum(1 for r in results if r['is_recent'])}
        }

    def _extract_papers(self, response: Any) -> List[Dict]:
        if isinstance(response, list) and len(response)>0 and hasattr(response[0], 'text'):
            try: response = json.loads(response[0].text)
            except (json.JSONDecodeError, AttributeError): return []
        if isinstance(response, dict): return response.get('results', [])
        if isinstance(response, list): return response
        return []

    def _calculate_time_metrics(self, published_date: str) -> tuple:
        if not published_date: return (9999, False)
        try:
            pub_date = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
            days_old = (datetime.now(pub_date.tzinfo) - pub_date).days
            return (days_old, days_old <= 90)
        except (ValueError, TypeError): return (9999, False)

# ---------------- Manejador de HackerNews ----------------
class HackerNewsHandler(BasePlatformHandler):
    def __init__(self, ai_client_manager: Optional[Any] = None):
        super().__init__("hackernews")
        self.ai_client = ai_client_manager
        
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        try:
            english_keyword = await self._translate_keyword(keyword)
            # HackerNews tool might be named 'search' or similar, adapt as needed.
            # Assuming the tool is 'getStories' for this implementation.
            params = {"query": english_keyword, "max_results": 15}
            response = await client.call_tool(config['tools'][0], params)
            return self.process_response(response, keyword)
        except Exception as e:
            return self.create_error_result(keyword, str(e))
    
    async def _translate_keyword(self, keyword: str) -> str:
        # Same translation logic as Arxiv
        if not self.ai_client or not re.search(r'[\u3040-\u30ff]', keyword): return keyword
        try:
            prompt = f"Translate the following Japanese keyword to a simple English equivalent for a HackerNews search. Provide only the English translation. Keyword: '{keyword}'"
            translation = await self.ai_client.chat_completion(prompt)
            return translation.strip().replace('"', '') or keyword
        except Exception: return keyword

    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        results = []
        posts = self._extract_posts(response)
        for post in posts:
            if isinstance(post, dict):
                results.append({
                    'title': post.get('title', ''), 'url': post.get('url', ''),
                    'score': post.get('score', post.get('points', 0)),
                    'comments_count': post.get('descendants', post.get('num_comments', 0)),
                })
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": results, "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": self._calculate_engagement_metrics(results)
        }
    
    def _extract_posts(self, response: Any) -> List[Dict]:
        if isinstance(response, list): return response
        if isinstance(response, dict): return response.get('hits', [])
        return []

    def _calculate_engagement_metrics(self, results: List[Dict]) -> Dict:
        if not results: return {"post_count": 0}
        post_count = len(results)
        total_score = sum(r.get('score', 0) for r in results)
        return {"post_count": post_count, "avg_score": round(total_score / post_count if post_count > 0 else 0)}

# ---------------- Manejador de Supabase (Placeholder) ----------------
class SupabaseHandler(BasePlatformHandler):
    def __init__(self):
        super().__init__("supabase")
    
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        return self.create_error_result(keyword, "Supabase no se utiliza para la investigaciÃ³n de keywords.")
    
    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        return self.create_error_result(keyword, "Supabase no se utiliza para la investigaciÃ³n de keywords.")

# ---------------- Manejador de Research Hub ----------------
class ResearchHubHandler(BasePlatformHandler):
    def __init__(self):
        super().__init__("research_hub")

    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        try:
            params = {"query": keyword, "limit": 10}
            response = await client.call_tool("search_papers", params)
            return self.process_response(response, keyword)
        except Exception as e:
            return self.create_error_result(keyword, str(e))

    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        results = []
        papers_data = response
        if isinstance(response, list) and len(response) > 0 and hasattr(response[0], 'content'):
            try: papers_data = json.loads(response[0].content)
            except (json.JSONDecodeError, AttributeError): papers_data = []

        if isinstance(papers_data, list):
            for paper in papers_data:
                results.append({
                    'title': paper.get('title', 'N/A'), 'authors': ", ".join(paper.get('authors', [])),
                    'url': paper.get('url', ''), 'abstract': paper.get('summary', ''),
                    'source': paper.get('source', 'Unknown'), 'year': paper.get('year', 0)
                })
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": results, "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": self._calculate_engagement_metrics(results)
        }

    def _calculate_engagement_metrics(self, results: List[Dict]) -> Dict:
        if not results: return {"paper_count": 0, "recent_papers_count": 0}
        current_year = datetime.now().year
        recent_papers = sum(1 for p in results if p.get('year', 0) >= current_year - 2)
        return {"paper_count": len(results), "recent_papers_count": recent_papers}

# ---------------- FÃ¡brica de Manejadores ----------------
class PlatformHandlerFactory:
    """Utiliza el patrÃ³n de diseÃ±o Factory para crear el manejador de plataforma correcto."""
    
    _handler_classes: Dict[str, Any] = {
        "youtube": YouTubeHandler,
        "github": GitHubHandler,
        "web": WebHandler,
        "arxiv": ArxivHandler,
        "hackernews": HackerNewsHandler,
        "supabase": SupabaseHandler,
        "research_hub": ResearchHubHandler
    }

    @staticmethod
    def create_handler(platform: str, ai_client_manager: Optional[Any] = None) -> BasePlatformHandler:
        handler_class = PlatformHandlerFactory._handler_classes.get(platform)
        if not handler_class:
            raise ValueError(f"No hay un manejador disponible para la plataforma: {platform}")
        
        # Inyecta el cliente de IA si el constructor del manejador lo acepta.
        import inspect
        sig = inspect.signature(handler_class.__init__)
        if 'ai_client_manager' in sig.parameters:
            return handler_class(ai_client_manager=ai_client_manager)
        else:
            return handler_class()



---
File: /prompt.txt
---

Directory Structure:

â””â”€â”€ ./
    â”œâ”€â”€ .env.txt
    â”œâ”€â”€ ai_client_manager.py
    â”œâ”€â”€ ai_trend_researcher.py
    â”œâ”€â”€ config_manager.py
    â”œâ”€â”€ data_processor.py
    â”œâ”€â”€ keyword_manager.py
    â”œâ”€â”€ mcp_client_manager.py
    â”œâ”€â”€ platform_handlers.py
    â”œâ”€â”€ README.md
    â”œâ”€â”€ report_generator.py
    â”œâ”€â”€ requirement.txt
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ research_assistant_con_hackers_LLM.py
    â””â”€â”€ research_assistant.py



---
File: /.env.txt
---

# --- Claves de API para los servicios de datos externos ---
# Cada una de estas claves es necesaria para que el script pueda autenticarse y solicitar datos de estas plataformas.

# Clave de API para acceder a los datos de YouTube (bÃºsquedas, detalles de videos, etc.).
YOUTUBE_API_KEY=your_youtube_api_key_here

# Token de Acceso Personal de GitHub para realizar bÃºsquedas en repositorios y cÃ³digo.
GITHUB_PERSONAL_ACCESS_TOKEN=your_github_api_key_here

# Clave de API para interactuar con Notion (crear y actualizar pÃ¡ginas).
NOTION_API_KEY=your_notion_api_key_here

# Clave de API para SiliconFlow, que podrÃ­a ser usada por el servidor de ArXiv para procesar PDFs.
SILICONFLOW_API_KEY=your_siliconflow_api_key_here

# Token de Acceso para Supabase, para guardar los resultados de la investigaciÃ³n en la base de datos.
SUPABASE_ACCESS_TOKEN=your_superbase_api_key_here


# --- ConfiguraciÃ³n EspecÃ­fica de Notion ---

# El ID de la pÃ¡gina principal en Notion bajo la cual se crearÃ¡n todas las pÃ¡ginas de informes nuevas.
NOTION_PARENT_PAGE_ID='your_notion_parent_page_id'


# --- ConfiguraciÃ³n de Rutas del Sistema ---
# Define rutas importantes para que la aplicaciÃ³n sea portable entre diferentes sistemas.

# Ruta al directorio donde el servidor 'research_hub' descargarÃ¡ los papers.
RESEARCH_PAPERS_DIR="/path/to/your/research-papers"

# Ruta completa al binario ejecutable del servidor 'research_hub'.
RESEARCH_HUB_EXECUTABLE="/path/to/your/rust-research-mcp"


# --- ConfiguraciÃ³n del Proveedor de IA ---

# Define quÃ© servicio de IA (LLM) se usarÃ¡ para tareas como la extracciÃ³n de palabras clave o traducciones.
# Opciones vÃ¡lidas: "anthropic", "gemini", "groq", "ollama", "openai".
AI_PROVIDER="openai" 

# Almacena las claves de API para cada uno de los proveedores de IA soportados.
# El script cargarÃ¡ la clave correspondiente al proveedor seleccionado en AI_PROVIDER.
ANTHROPIC_API_KEY="tu_clave_de_anthropic"
GOOGLE_API_KEY="tu_clave_de_gemini"
GROQ_API_KEY="tu_clave_de_groq"
OPENAI_API_KEY="tu_clave_de_openai"

# --- ConfiguraciÃ³n Opcional de Modelos de IA ---
# Permite especificar quÃ© modelo exacto usar para cada proveedor. Si no se define, se usarÃ¡ un modelo por defecto.

# Modelo especÃ­fico para Google Gemini (ej. gemini-1.5-flash).
AI_MODEL_GEMINI="gemini-1.5-flash"
# Modelo especÃ­fico para Groq (ej. llama3-70b-8192).
AI_MODEL_GROQ="llama3-70b-8192"
# Modelo para Ollama, que debe corresponder a un modelo que tengas instalado localmente (ej. llama3).
AI_MODEL_OLLAMA="llama3"
# Modelo especÃ­fico para Anthropic Claude (ej. claude-3-haiku).
AI_MODEL_ANTHROPIC="claude-3-haiku-20240307"
# Modelo especÃ­fico para OpenAI (ej. gpt-4o).
AI_MODEL_OPENAI="gpt-4o"



---
File: /ai_client_manager.py
---

# ai_client_manager.py

import asyncio
# Importa las bibliotecas cliente de cada proveedor de IA soportado.
import google.generativeai as genai  # Para Google Gemini
from anthropic import Anthropic       # Para Anthropic Claude
from groq import Groq                 # Para Groq
import ollama                         # Para Ollama (modelos locales)
from openai import OpenAI             # Para OpenAI
from typing import Callable, Any

class AIClientManager:
    """
    Gestiona la inicializaciÃ³n e interacciÃ³n con diferentes clientes de IA.
    ActÃºa como una "fÃ¡brica" que crea el cliente correcto segÃºn la configuraciÃ³n
    y proporciona un mÃ©todo unificado 'chat_completion' para interactuar con Ã©l de forma no bloqueante.
    """
    def __init__(self, provider: str, api_key: str = None, model: str = None):
        """
        Constructor. Inicializa el cliente de IA basado en el proveedor especificado.
        :param provider: El nombre del proveedor (ej. "openai", "gemini").
        :param api_key: La clave de API para el proveedor.
        :param model: El nombre del modelo especÃ­fico a usar (opcional).
        """
        self.provider = provider.lower()
        self.model = model
        self.client: Any = None
        print(f"Initializing AI client for provider: {self.provider}")

        # LÃ³gica condicional para inicializar el cliente correcto.
        if self.provider == 'gemini':
            if not api_key: raise ValueError("Google API Key is required for Gemini provider.")
            genai.configure(api_key=api_key)
            self.client = genai.GenerativeModel(self.model or 'gemini-pro')
        elif self.provider == 'groq':
            if not api_key: raise ValueError("Groq API Key is required for Groq provider.")
            self.client = Groq(api_key=api_key)
        elif self.provider == 'ollama':
            # Para Ollama, el cliente es el propio mÃ³dulo de la biblioteca.
            self.client = ollama
        elif self.provider == 'anthropic':
            if not api_key: raise ValueError("Anthropic API Key is required for Anthropic provider.")
            self.client = Anthropic(api_key=api_key)
        elif self.provider == 'openai':
            if not api_key: raise ValueError("OpenAI API Key is required for OpenAI provider.")
            self.client = OpenAI(api_key=api_key)
        else:
            raise ValueError(f"Unsupported AI provider: {self.provider}")

    async def chat_completion(self, prompt: str, max_tokens: int = 1024) -> str:
        """
        EnvÃ­a un prompt al modelo de IA y devuelve la respuesta de texto.
        Este mÃ©todo abstrae las diferencias en las llamadas a la API y las ejecuta en un
        hilo separado para no bloquear el bucle de eventos de asyncio.
        """
        try:
            # Selecciona la funciÃ³n de llamada a la API correcta basada en el proveedor.
            api_call_function = self._get_api_call_function(prompt, max_tokens)
            
            # Ejecuta la llamada sÃ­ncrona de la biblioteca en un hilo separado.
            # Esto es crucial para no bloquear la aplicaciÃ³n asÃ­ncrona.
            loop = asyncio.get_running_loop()
            response_text = await loop.run_in_executor(
                None,  # Usa el ejecutor de hilos por defecto.
                api_call_function
            )
            return response_text

        except Exception as e:
            print(f"Error calling {self.provider} API: {e}")
            return ""

    def _get_api_call_function(self, prompt: str, max_tokens: int) -> Callable[[], str]:
        """Devuelve la funciÃ³n lambda correcta para realizar la llamada a la API sÃ­ncrona."""
        
        if self.provider == 'gemini':
            return lambda: self.client.generate_content(prompt).text

        elif self.provider == 'groq':
            return lambda: self.client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model=self.model or "llama3-8b-8192",
            ).choices[0].message.content

        elif self.provider == 'ollama':
            return lambda: self.client.chat(
                model=self.model or 'llama3',
                messages=[{'role': 'user', 'content': prompt}]
            )['message']['content']

        elif self.provider == 'anthropic':
            return lambda: self.client.messages.create(
                model=self.model or "claude-3-sonnet-20240229",
                max_tokens=max_tokens,
                messages=[{"role": "user", "content": prompt}]
            ).content[0].text

        elif self.provider == 'openai':
            return lambda: self.client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model=self.model or "gpt-4o",
            ).choices[0].message.content
            
        else:
            # Esto no deberÃ­a ocurrir si el constructor funcionÃ³, pero es una salvaguarda.
            raise NotImplementedError(f"API call function not implemented for {self.provider}")



---
File: /ai_trend_researcher.py
---

# ai_trend_researcher.py
# -*- coding: utf-8 -*-

import asyncio
import sys
import os
import signal
import argparse
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Iterable

from dotenv import load_dotenv

from keyword_manager import KeywordManager
from mcp_client_manager import MCPClientManager
from platform_handlers import PlatformHandlerFactory
from data_processor import KeywordExtractor, DataAnalyzer
from report_generator import ReportManager
from config_manager import ServerConfig, AppConfig, PlatformConfig
from ai_client_manager import AIClientManager

load_dotenv()


# ----------------------------- utilidades -----------------------------

def env_int(name: str, default: int) -> int:
    try:
        v = os.getenv(name)
        return int(v) if v is not None else default
    except ValueError:
        return default

def env_float(name: str, default: float) -> float:
    try:
        v = os.getenv(name)
        return float(v) if v is not None else default
    except ValueError:
        return default

def env_bool(name: str, default: bool) -> bool:
    v = (os.getenv(name, str(default)) or "").strip().lower()
    return v in ("1", "true", "t", "yes", "y", "on")

def now_str() -> str:
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def log(msg: str) -> None:
    print(f"[{now_str()}] {msg}", flush=True)


# --------------------------- nÃºcleo investigador ---------------------------

class AITrendResearcher:
    """
    Orquestador principal del flujo de investigaciÃ³n de tendencias de IA.
    Mejora: control de concurrencia, timeouts, reintentos y CLI.
    """

    def __init__(
        self,
        platforms_filter: Optional[Iterable[str]] = None,
        exclude_platforms: Optional[Iterable[str]] = None,
        per_task_timeout: float = 35.0,
        retries: int = 1,
        concurrency: int = 4,
        keywords_limit: Optional[int] = None,
    ):
        # Estado configuraciÃ³n / validaciones
        AppConfig.print_config_status()
        missing_vars = AppConfig.validate_required_env_vars()
        if missing_vars:
            log("âœ— Faltan variables de entorno requeridas:")
            for var in missing_vars:
                log(f"  - {var}")
            log("Por favor, complÃ©talas en tu .env")
            # No abortamos aquÃ­: el sistema puede operar con subset (ej. sin Notion/Supabase)
            # pero si faltan claves crÃ­ticas de proveedor de IA, AIClientManager fallarÃ¡.

        # Cliente de IA
        ai_provider = AppConfig.get_ai_provider()
        api_key = AppConfig.get_api_key(ai_provider)
        ai_model = AppConfig.get_ai_model(ai_provider)
        self.ai_client_manager = AIClientManager(provider=ai_provider, api_key=api_key, model=ai_model)

        # Palabras clave
        self.keyword_manager = KeywordManager()

        # Servidores MCP / plataformas
        server_configs = ServerConfig.get_server_configs()
        supported = set(PlatformConfig.get_supported_platforms())
        wanted = set(p.strip() for p in (platforms_filter or supported))
        excluded = set(p.strip() for p in (exclude_platforms or []))
        self.platforms: List[str] = [p for p in wanted if p in supported and p not in excluded]

        # Gestores
        self.mcp_manager = MCPClientManager(server_configs)
        self.keyword_extractor = KeywordExtractor(self.ai_client_manager)
        self.data_analyzer = DataAnalyzer(self.ai_client_manager)
        self.report_manager: Optional[ReportManager] = None

        # ParÃ¡metros ejecuciÃ³n
        self.per_task_timeout = float(per_task_timeout)
        self.retries = int(max(0, retries))
        self.semaphore = asyncio.Semaphore(int(max(1, concurrency)))
        self.keywords_limit = int(keywords_limit) if keywords_limit else None

    async def run_daily_research(self) -> str:
        log(f"ðŸš€ Starting AI trend research - {now_str()}")

        try:
            # 1) Conectar MCP
            await self.mcp_manager.connect_all_servers()

            # 2) Plataformas activas realmente disponibles
            active_platforms = [p for p in self.platforms if self.mcp_manager.is_platform_available(p)]
            if not active_platforms:
                log("âš ï¸ No hay servidores MCP activos; nada que hacer.")
                return ""

            log(f"ðŸŒ Servidores MCP activos: {active_platforms}")

            # 3) Clientes opcionales para reportes
            notion_client = self.mcp_manager.get_client("notion")
            supabase_client = self.mcp_manager.get_client("supabase")
            notion_parent_id = AppConfig.get_notion_parent_page_id()

            # 4) Gestor de reportes
            self.report_manager = ReportManager(
                reports_dir=AppConfig.get_reports_directory(),
                notion_client=notion_client,
                notion_parent_id=notion_parent_id,
                supabase_client=supabase_client,
            )

            # 5) Cargar keywords activas
            active_keywords = self._load_active_keywords()
            if self.keywords_limit is not None:
                active_keywords = active_keywords[: self.keywords_limit]
            log(f"ðŸ”‘ Investigando {len(active_keywords)} keywords en {len(active_platforms)} plataformas: {active_keywords}")

            # 6) InvestigaciÃ³n concurrente con




---
File: /config_manager.py
---

# config_manager.py
# -*- coding: utf-8 -*-

# Importa el mÃ³dulo 'os' para interactuar con el sistema operativo, principalmente para leer variables de entorno.
import os
# Importa herramientas de 'typing' para aÃ±adir anotaciones de tipo, mejorando la legibilidad y robustez del cÃ³digo.
from typing import Dict, List, Any


class ServerConfig:
    """
    Gestiona las configuraciones de los servidores MCP (Model Context Protocol).
    Define cÃ³mo iniciar y conectar con los diferentes servicios externos (YouTube, GitHub, etc.).
    """

    @staticmethod
    def _clean_env(env: Dict[str, Any]) -> Dict[str, str]:
        """
        MÃ©todo privado para limpiar el diccionario de entorno.
        Elimina claves con valores None o vacÃ­os ("") y convierte todos los valores a string.
        Esto es necesario porque algunas bibliotecas (como Pydantic, usada por MCP) no aceptan None en variables de entorno.
        """
        # Si el diccionario de entrada estÃ¡ vacÃ­o, devuelve uno vacÃ­o.
        if not env:
            return {}
        # Devuelve un nuevo diccionario que solo incluye los Ã­tems vÃ¡lidos y con valores casteados a string.
        return {k: str(v) for k, v in env.items() if v not in (None, "")}

    @staticmethod
    def get_server_configs() -> Dict[str, Dict[str, Any]]:
        """
        Devuelve un diccionario que contiene la configuraciÃ³n detallada para cada servidor MCP.
        Los servidores que requieren credenciales (API keys) se marcan con enabled=False si la clave no estÃ¡ presente.
        """
        # Lee todas las claves de API y tokens de las variables de entorno.
        youtube_key   = os.getenv("YOUTUBE_API_KEY")
        gh_token      = os.getenv("GITHUB_PERSONAL_ACCESS_TOKEN")
        notion_key    = os.getenv("NOTION_API_KEY")
        notion_parent = os.getenv("NOTION_PARENT_PAGE_ID")
        supa_token    = os.getenv("SUPABASE_ACCESS_TOKEN")
        silicon_key   = os.getenv("SILICONFLOW_API_KEY")

        # Lee las rutas configurables desde el entorno para mayor portabilidad.
        download_dir = os.getenv("RESEARCH_PAPERS_DIR", "research-papers")
        research_hub_executable = os.getenv("RESEARCH_HUB_EXECUTABLE", "rust-research-mcp")

        # Construye dinÃ¡micamente la lista de argumentos para el servidor de Notion.
        notion_args = ["@ramidecodes/mcp-server-notion@latest", "-y"]
        # AÃ±ade la clave de API a los argumentos solo si existe, para no exponer un argumento vacÃ­o.
        if notion_key:
            notion_args.append(f"--api-key={notion_key}")

        # Construye dinÃ¡micamente la lista de argumentos para el servidor de Supabase.
        supabase_args = ["-y", "@supabase/mcp-server-supabase@latest"]
        # AÃ±ade el token de acceso a los argumentos solo si existe.
        if supa_token:
            supabase_args += ["--access-token", supa_token]

        # Define el diccionario principal de configuraciones.
        configs: Dict[str, Dict[str, Any]] = {
            "youtube": {
                "server_name": "npx",  # Comando para ejecutar el servidor (a travÃ©s de npx).
                "args": ["-y", "youtube-data-mcp-server"],  # Argumentos para el comando.
                "env": ServerConfig._clean_env({  # Variables de entorno especÃ­ficas para este servidor.
                    "YOUTUBE_API_KEY": youtube_key,
                    "YOUTUBE_TRANSCRIPT_LANG": "ja",  # Configura el idioma de las transcripciones a japonÃ©s.
                }),
                "tools": ["searchVideos", "getVideoDetails", "getTranscripts"],  # Herramientas que expone el servidor.
                "required_env": ["YOUTUBE_API_KEY"],  # Variables de entorno obligatorias.
                "enabled": True if youtube_key else False,  # Se activa solo si la clave de API estÃ¡ presente.
            },
            "github": {
                "server_name": "npx",
                "args": ["-y", "@modelcontextprotocol/server-github"],
                "env": ServerConfig._clean_env({
                    "GITHUB_PERSONAL_ACCESS_TOKEN": gh_token,
                }),
                "tools": ["search_code", "search_repositories", "get_repository"],
                "required_env": ["GITHUB_PERSONAL_ACCESS_TOKEN"],
                "enabled": True if gh_token else False, # Se activa solo si el token de GitHub estÃ¡ presente.
            },
            "web": {
                "server_name": "one-search-mcp",  # Este servidor se ejecuta directamente, sin 'npx'.
                "args": [],  # No necesita argumentos adicionales.
                "env": {  # Variables de entorno para estandarizar y silenciar la salida de la consola.
                    "DOTENVX_SILENT": "1",
                    "FORCE_COLOR": "0",
                    "NO_COLOR": "1"
                },
                "tools": ["one_search", "one_extract", "one_scrape"],
                "enabled": True,  # Este servidor siempre estÃ¡ habilitado ya que no requiere claves.
            },
            "notion": {
                "server_name": "npx",
                "args": ["-y", *notion_args],  # Usa los argumentos construidos dinÃ¡micamente.
                "env": ServerConfig._clean_env({}), # No necesita variables de entorno adicionales.
                "tools": ["create-page", "get-page", "update-page", "query-database", "search"],
                "required_env": ["NOTION_API_KEY"],
                "enabled": True if notion_key else False, # Se activa solo si la clave de Notion estÃ¡ presente.
            },
            "arxiv": {
                "server_name": "npx",
                "args": ["-y", "@langgpt/arxiv-mcp-server@latest"],
                "env": {
                    "SILICONFLOW_API_KEY": silicon_key,
                    "WORK_DIR": "./reports",  # Directorio de trabajo para descargar PDFs.
                    "FORCE_COLOR": "0",
                    "NO_COLOR": "1",
                    "DOTENVX_SILENT": "1"
                },
                "tools": [
                    "search_arxiv", "download_arxiv_pdf", "parse_pdf_to_text",
                    "convert_to_wechat_article", "parse_pdf_to_markdown",
                    "process_arxiv_paper", "clear_workdir"
                ],
                "enabled": bool(silicon_key), # Se activa solo si la clave de SiliconFlow estÃ¡ presente.
            },
            "hackernews": {
                "server_name": "npx",
                "args": ["-y", "@microagents/server-hackernews"],
                "env": ServerConfig._clean_env({}),
                "tools": ["getStories", "getStory", "getStoryWithComments"],
                "required_env": [],  # No requiere variables de entorno.
                "enabled": True,  # Siempre habilitado.
            },
            "supabase": {
                "server_name": "npx",
                "args": supabase_args,  # Usa los argumentos construidos dinÃ¡micamente.
                "env": ServerConfig._clean_env({}),
                "tools": ["execute_sql"],
                "required_env": ["SUPABASE_ACCESS_TOKEN"],
                "enabled": True if supa_token else False, # Se activa solo si el token de Supabase estÃ¡ presente.
            },
            "research_hub": {
                "server_name": research_hub_executable,
                "args": [
                    "--download-dir", download_dir,
                    "--log-level", "info"
                ],
                "env": {
                    "RUST_LOG": "info",
                    "DOTENVX_SILENT": "1",
                    "FORCE_COLOR": "0",
                    "NO_COLOR": "1"
                },
                "tools": [
                    "search_papers", 
                    "download_paper", 
                    "extract_metadata",
                    "search_code", 
                    "generate_bibliography"
                ],
                "required_env": ["RESEARCH_HUB_EXECUTABLE", "RESEARCH_PAPERS_DIR"], 
                "enabled": os.path.exists(research_hub_executable), # Se activa si el binario existe.
            },
        }
        # Devuelve el diccionario completo de configuraciones.
        return configs

    @staticmethod
    def get_enabled_platforms() -> List[str]:
        """Devuelve una lista con los nombres de las plataformas que estÃ¡n actualmente habilitadas."""
        configs = ServerConfig.get_server_configs()
        # Crea una lista de plataformas donde el valor de 'enabled' es True.
        return [platform for platform, config in configs.items() if config.get("enabled", False)]


class AppConfig:
    """Clase para gestionar la configuraciÃ³n general de la aplicaciÃ³n (proveedor de IA, modelos, etc.)."""

    @staticmethod
    def get_ai_provider() -> str:
        """Obtiene el proveedor de IA configurado en .env, con 'openai' como valor por defecto."""
        return os.getenv("AI_PROVIDER", "openai").lower()

    @staticmethod
    def get_api_key(provider: str) -> str:
        """Obtiene la clave de API para un proveedor de IA especÃ­fico."""
        # Mapea el nombre del proveedor a su variable de entorno correspondiente.
        provider_key_map = {
            "anthropic": "ANTHROPIC_API_KEY",
            "gemini": "GOOGLE_API_KEY",
            "groq": "GROQ_API_KEY",
            "openai": "OPENAI_API_KEY",
            # 'ollama' se ejecuta localmente y no requiere clave.
        }
        # Obtiene el nombre de la variable de entorno del mapa.
        env_var_name = provider_key_map.get(provider)
        # Devuelve el valor de la variable de entorno si existe, si no, None.
        return os.getenv(env_var_name) if env_var_name else None

    @staticmethod
    def get_ai_model(provider: str) -> str:
        """Obtiene el nombre del modelo de IA especÃ­fico para un proveedor, si estÃ¡ configurado."""
        # Construye el nombre de la variable de entorno (ej. "AI_MODEL_OPENAI").
        env_var_name = f"AI_MODEL_{provider.upper()}"
        # Devuelve el valor de la variable de entorno.
        return os.getenv(env_var_name)

    @staticmethod
    def get_notion_parent_page_id() -> str:
        """Obtiene el ID de la pÃ¡gina padre de Notion desde las variables de entorno."""
        return os.getenv("NOTION_PARENT_PAGE_ID", "")

    @staticmethod
    def get_reports_directory() -> str:
        """Devuelve el nombre del directorio donde se guardan los informes locales."""
        return "reports"

    @staticmethod
    def validate_required_env_vars() -> List[str]:
        """
        Valida que todas las variables de entorno necesarias estÃ©n definidas.
        Devuelve una lista con las variables que faltan.
        """
        # Define un diccionario de variables requeridas y su descripciÃ³n.
        required_vars = {
            "YOUTUBE_API_KEY": "YouTube API key",
            "GITHUB_PERSONAL_ACCESS_TOKEN": "GitHub access token",
            "NOTION_API_KEY": "Notion API key",
            "NOTION_PARENT_PAGE_ID": "Notion parent page ID",
            "SUPABASE_ACCESS_TOKEN": "Supabase access token",
            "RESEARCH_PAPERS_DIR": "Research papers download directory",
            "RESEARCH_HUB_EXECUTABLE": "Path to the Research Hub executable"
        }
        # AÃ±ade la clave de API del proveedor de IA seleccionado a la lista de requeridos.
        ai_provider = AppConfig.get_ai_provider()
        api_key_env_var = {
            "gemini": "GOOGLE_API_KEY",
            "groq": "GROQ_API_KEY",
            "anthropic": "ANTHROPIC_API_KEY",
            "openai": "OPENAI_API_KEY",
        }.get(ai_provider)
        if api_key_env_var:
            required_vars[api_key_env_var] = f"API Key for {ai_provider.capitalize()}"

        # Crea una lista de las variables que no estÃ¡n definidas.
        missing = [f"{var} ({desc})" for var, desc in required_vars.items() if not os.getenv(var)]
        return missing

    @staticmethod
    def print_config_status():
        """Imprime en la consola un resumen del estado de la configuraciÃ³n actual."""
        print("=== Configuration Status ===")
        ai_provider = AppConfig.get_ai_provider()
        print(f"âœ“ AI Provider configured: {ai_provider.upper()}")

        api_key = AppConfig.get_api_key(ai_provider)
        if ai_provider not in ["ollama"]: # Ollama no necesita clave.
            print(f"âœ“ {ai_provider.capitalize()} API key loaded" if api_key else f"âœ— {ai_provider.capitalize()} API key not found")

        ai_model = AppConfig.get_ai_model(ai_provider)
        print(f"âœ“ Using specific model for {ai_provider}: {ai_model}" if ai_model else f"âœ“ Using default model for {ai_provider}")
        print("---")

        # Comprueba el estado de otras claves de API importantes.
        other_vars = [
            ("YOUTUBE_API_KEY", "YouTube API key"),
            ("GITHUB_PERSONAL_ACCESS_TOKEN", "GitHub access token"),
            ("NOTION_API_KEY", "Notion API key"),
            ("NOTION_PARENT_PAGE_ID", "Notion parent page ID"),
            ("SUPABASE_ACCESS_TOKEN", "Supabase access token"),
            ("RESEARCH_PAPERS_DIR", "Research papers directory"),
            ("RESEARCH_HUB_EXECUTABLE", "Research Hub executable"),
        ]
        for var, description in other_vars:
            # Imprime un tick (âœ“) si la variable estÃ¡ cargada, o una cruz (âœ—) si no.
            print(f"âœ“ {description} loaded" if os.getenv(var) else f"âœ— {description} not found")
        
        if os.getenv("RESEARCH_HUB_EXECUTABLE") and not os.path.exists(os.getenv("RESEARCH_HUB_EXECUTABLE")):
            print(f"âœ— WARNING: Research Hub executable not found at specified path.")

        print("============================")


class PlatformConfig:
    """Define las plataformas que la aplicaciÃ³n soporta para la investigaciÃ³n."""
    # Lista fija de plataformas soportadas en el cÃ³digo.
    SUPPORTED_PLATFORMS = ["web", "youtube", "github", "arxiv", "hackernews", "supabase", "research_hub"]

    @staticmethod
    def get_supported_platforms() -> List[str]:
        """Devuelve una copia de la lista de plataformas soportadas."""
        return PlatformConfig.SUPPORTED_PLATFORMS.copy()

    @staticmethod
    def is_platform_supported(platform: str) -> bool:
        """Comprueba si una plataforma dada estÃ¡ en la lista de soportadas."""
        return platform in PlatformConfig.SUPPORTED_PLATFORMS



---
File: /data_processor.py
---

# data_processor.py
# -*- coding: utf-8 -*-

# Importa el mÃ³dulo 'asyncio' para ejecutar tareas sÃ­ncronas en un hilo.
import asyncio
# Importa el mÃ³dulo 'json' para trabajar con datos en formato JSON.
import json
# Importa el mÃ³dulo 're' para trabajar con expresiones regulares (bÃºsqueda de patrones en texto).
import re
# De 'collections', importa 'Counter' para contar fÃ¡cilmente la frecuencia de elementos en una lista.
from collections import Counter
# De 'datetime', importa 'datetime' para obtener la fecha y hora actuales.
from datetime import datetime
# De 'typing', importa herramientas para anotaciones de tipo.
from typing import Dict, List, Any

# Importa el gestor de clientes de IA para que el analizador pueda usar LLMs.
from ai_client_manager import AIClientManager

class KeywordExtractor:
    """
    Extrae nuevas palabras clave a partir de los datos de investigaciÃ³n.
    Utiliza un LLM (Modelo LingÃ¼Ã­stico Grande) si estÃ¡ disponible para una extracciÃ³n mÃ¡s inteligente.
    Si no, recurre a un mÃ©todo heurÃ­stico local basado en frecuencia de palabras.
    """
    def __init__(self, ai_client_manager: AIClientManager = None):
        """
        Constructor. Recibe un gestor de cliente de IA.
        Este gestor debe tener un mÃ©todo `async chat_completion(prompt, max_tokens=...)`.
        """
        self.ai_client = ai_client_manager

    async def extract_keywords(self, research_data: List[Dict[str, Any]]) -> List[str]:
        """
        MÃ©todo principal para extraer palabras clave.
        Decide si usar el LLM o el mÃ©todo heurÃ­stico de respaldo.
        """
        # Si no hay datos de investigaciÃ³n, no hay nada que hacer.
        if not research_data:
            print("No hay datos de investigaciÃ³n para la extracciÃ³n de keywords.")
            return []

        # Prepara un resumen compacto del contenido para no enviar demasiada informaciÃ³n al LLM.
        content_summary = self._prepare_content_for_analysis(research_data)

        # Si despuÃ©s de preparar el resumen no hay contenido, usa la heurÃ­stica sobre los datos brutos.
        if not content_summary:
            print("No se encontrÃ³ contenido utilizable. Usando heurÃ­stica sobre corpus completo.")
            corpus = self._concat_corpus_from_raw(research_data)
            return self._heuristic_keywords(corpus)

        # Si hay un cliente de IA disponible, intenta usarlo.
        if self.ai_client:
            try:
                # Crea el prompt (la instrucciÃ³n) para el LLM.
                prompt = self._create_extraction_prompt(content_summary)
                # Llama al LLM para obtener una respuesta.
                response = await self.ai_client.chat_completion(prompt, max_tokens=512)
                # Parsea la respuesta del LLM para extraer la lista de palabras clave.
                keywords = self._parse_keywords_from_response(response)
                provider = getattr(self.ai_client, "provider", "ai").capitalize()
                print(f"LLM ({provider}) extrajo {len(keywords)} keywords: {keywords}")
                return keywords
            except Exception as e:
                # Si el LLM falla, informa del error y pasa al mÃ©todo de respaldo.
                print(f"[KeywordExtractor] Fallo con LLM, usando heurÃ­stica. Error: {e}")

        # Si no hay cliente de IA o si fallÃ³, usa el mÃ©todo heurÃ­stico local.
        corpus = self._concat_corpus(content_summary)
        return self._heuristic_keywords(corpus)

    # ---------- MÃ©todos de utilidad internos ----------

    def _prepare_content_for_analysis(self, research_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Compacta los resultados de la investigaciÃ³n para crear un resumen manejable."""
        content_summary: List[Dict[str, Any]] = []
        for data in research_data:
            results = data.get("results", [])
            # Toma como mÃ¡ximo los 3 primeros resultados de cada plataforma para ser conciso.
            for result in results[:3]:
                content_summary.append({
                    "platform": data.get("platform", ""),
                    "keyword": data.get("keyword", ""),
                    "title": result.get("title") or result.get("name") or "",
                    # Acorta la descripciÃ³n a 200 caracteres.
                    "description": (result.get("description") or result.get("snippet") or result.get("abstract") or "")[:200],
                    "topics": result.get("topics", []),
                })
        return content_summary

    def _create_extraction_prompt(self, content_summary: List[Dict[str, Any]]) -> str:
        """Crea el prompt que se enviarÃ¡ al LLM, pidiÃ©ndole que extraiga keywords en formato JSON."""
        return (
            "Analyze this AI trend research data and extract 5-10 new trending keywords related to AI, "
            "machine learning, or technology.\n\n"
            f"Data: {json.dumps(content_summary, indent=2, ensure_ascii=False)}\n\n"
            "Instructions:\n"
            "1. Focus on AI tools, frameworks, companies, techniques, or emerging technologies\n"
            "2. Return only a JSON array of keywords, like: [\"keyword1\", \"keyword2\", \"keyword3\"]\n"
            "3. Prioritize keywords that appear frequently or have high engagement\n"
            "4. Include both English and Japanese keywords if relevant\n"
            "5. If no relevant keywords are found, return an empty array: []\n"
        )

    def _parse_keywords_from_response(self, response: str) -> List[str]:
        """Parsea la respuesta del LLM. Intenta leer un array JSON, y si falla, lo trata como texto plano."""
        if not response:
            return []
        try:
            # Busca una estructura que parezca un array JSON (empieza con [ y termina con ]).
            m = re.search(r"\[.*?\]", response, re.DOTALL)
            if m:
                # Si lo encuentra, intenta decodificarlo como JSON.
                arr = json.loads(m.group())
                # Limpia y devuelve la lista de strings.
                return [s.strip() for s in arr if isinstance(s, str) and s.strip()]
        except Exception as e:
            # Si el parseo JSON falla, lo informa.
            print(f"Error parseando JSON de keywords: {e}")

        # Si no es JSON, lo trata como texto plano separado por comas.
        parts = response.replace("[", "").replace("]", "").replace('"', "")
        kws = [p.strip() for p in parts.split(",") if p.strip()]
        # Devuelve como mÃ¡ximo las 10 primeras.
        return kws[:10]

    def _concat_corpus(self, content_summary: List[Dict[str, Any]]) -> str:
        """Une todos los textos del resumen (tÃ­tulos, descripciones, temas) en un solo bloque de texto (corpus)."""
        parts: List[str] = []
        for item in content_summary:
            parts.append(item.get("title", ""))
            parts.append(item.get("description", ""))
            topics = item.get("topics", [])
            if isinstance(topics, list) and topics:
                parts.extend([str(t) for t in topics])
        return " \n".join([p for p in parts if p])

    def _concat_corpus_from_raw(self, research_data: List[Dict[str, Any]]) -> str:
        """Similar a _concat_corpus, pero trabaja directamente con los datos brutos de investigaciÃ³n."""
        parts: List[str] = []
        for d in research_data:
            for r in d.get("results", []):
                parts.append(r.get("title") or r.get("name") or "")
                parts.append(r.get("description") or r.get("snippet") or r.get("abstract") or "")
                topics = r.get("topics", [])
                if isinstance(topics, list):
                    parts.extend([str(t) for t in topics])
        return " \n".join([p for p in parts if p])

    def _heuristic_keywords(self, corpus: str) -> List[str]:
        """
        MÃ©todo heurÃ­stico de respaldo para extraer keywords.
        Se basa en encontrar las palabras y frases (n-gramas) mÃ¡s frecuentes.
        """
        if not corpus:
            return []

        text = corpus.lower()

        # Extrae tokens (palabras) que parecen relevantes.
        tokens = re.findall(r"[a-z0-9][a-z0-9\-_/\.]{2,}", text)
        # Define una lista de palabras comunes (stop words) para ignorar.
        stop = {
            "https", "http", "www", "com", "org", "from", "with", "that", "this", "what", "when",
            "your", "have", "about", "into", "like", "will", "there", "their", "been", "make",
            "only", "some", "more", "over", "also", "than", "which", "were", "after", "before",
            "because", "could", "should", "would"
        }
        tokens = [t for t in tokens if t not in stop]
        # Obtiene las 20 palabras mÃ¡s comunes.
        singles = [w for w, _ in Counter(tokens).most_common(20)]

        # Busca frases de 2 palabras (bigramas) y 3 palabras (trigramas).
        words = re.findall(r"[a-z0-9]+", text)
        bigrams = [" ".join(words[i:i+2]) for i in range(len(words)-1)]
        trigrams = [" ".join(words[i:i+3]) for i in range(len(words)-2)]
        # Cuenta la frecuencia de los n-gramas mÃ¡s relevantes.
        bf = Counter([b for b in bigrams if len(b) > 6])
        tf = Counter([t for t in trigrams if len(t) > 8])

        # Combina las palabras sueltas y los n-gramas mÃ¡s comunes.
        candidates = singles + [w for w, _ in bf.most_common(10)] + [w for w, _ in tf.most_common(10)]
        # Normaliza y limpia la lista final.
        return self._normalize_keywords(candidates)

    def _normalize_keywords(self, kws: List[str]) -> List[str]:
        """Limpia una lista de keywords: convierte a minÃºsculas, quita espacios y duplicados."""
        out: List[str] = []
        for kw in kws:
            k = re.sub(r"\s+", " ", kw.lower()).strip()
            k = k.strip(" .,:;-/\\|\"'()[]{}")
            if len(k) >= 3:
                out.append(k)
        # Elimina duplicados manteniendo el orden.
        seen = set()
        uniq = []
        for k in out:
            if k not in seen:
                seen.add(k)
                uniq.append(k)
        return uniq


class DataAnalyzer:
    """Analiza los datos recolectados para calcular mÃ©tricas, puntuar keywords y generar recomendaciones."""
    
    def __init__(self, ai_client_manager: AIClientManager = None):
        """Constructor. Recibe el gestor de cliente de IA para generar recomendaciones dinÃ¡micas."""
        self.ai_client = ai_client_manager

    def score_keywords(self, new_keywords: List[str], research_data: List[Dict[str, Any]]) -> Dict[str, int]:
        """Asigna una puntuaciÃ³n a cada nueva palabra clave basada en su frecuencia en los resultados de la investigaciÃ³n."""
        if not new_keywords:
            return {}

        # Crea un gran bloque de texto (corpus) con todos los tÃ­tulos, descripciones y temas.
        parts: List[str] = []
        for item in research_data:
            for r in item.get("results", []):
                title = r.get("title") or r.get("name") or ""
                desc = r.get("description") or r.get("snippet") or r.get("abstract") or ""
                topics = r.get("topics") or []
                parts.append(title)
                parts.append(desc)
                if isinstance(topics, list):
                    parts.extend([str(t) for t in topics])
        text = " \n".join([p for p in parts if p]).lower()

        # Cuenta cuÃ¡ntas veces aparece cada nueva palabra clave en el corpus.
        hits_map: Dict[str, int] = {}
        max_hits = 1
        for kw in new_keywords:
            if not kw:
                continue
            hits = len(re.findall(re.escape(kw.lower()), text))
            hits_map[kw] = hits
            if hits > max_hits:
                max_hits = hits

        # Normaliza las puntuaciones en una escala de 0 a 100 usando una escala logarÃ­tmica.
        import math
        scores: Dict[str, int] = {}
        for kw, h in hits_map.items():
            norm = math.log1p(h) / math.log1p(max_hits) if max_hits > 0 else 0.0
            scores[kw] = int(round(norm * 100))
        return scores

    def calculate_summary_stats(self, research_data: List[Dict[str, Any]], new_keywords: List[str]) -> Dict[str, Any]:
        """Calcula estadÃ­sticas agregadas bÃ¡sicas sobre la ejecuciÃ³n de la investigaciÃ³n."""
        # Cuenta cuÃ¡ntas ejecuciones se hicieron por plataforma.
        per_platform = Counter([d.get("platform", "unknown") for d in research_data])
        # Suma el total de resultados obtenidos.
        total_results = sum(len(d.get("results", [])) for d in research_data)
        # Cuenta cuÃ¡ntas ejecuciones tuvieron errores.
        runs_with_errors = sum(1 for d in research_data if d.get("error"))

        return {
            "timestamp": datetime.now().isoformat(),
            "platform_breakdown": dict(per_platform),
            "total_items": total_results,
            "new_keywords_count": len(new_keywords),
            "runs_with_errors": runs_with_errors,
        }

    async def generate_recommendations(self, research_data: List[Dict[str, Any]], new_keywords: List[str]) -> List[str]:
        """Genera una lista de recomendaciones de acciÃ³n, usando IA si estÃ¡ disponible."""
        # Si no hay cliente de IA o no hay datos, usa el mÃ©todo de respaldo.
        if not self.ai_client or (not research_data and not new_keywords):
            return self._heuristic_recommendations(research_data, new_keywords)

        try:
            prompt = self._create_recommendation_prompt(research_data, new_keywords)
            response = await self.ai_client.chat_completion(prompt, max_tokens=512)
            # Parsea la respuesta en una lista de strings.
            recommendations = [rec.strip("- ").strip() for rec in response.split("\n") if rec.strip("- ").strip()]
            return recommendations if recommendations else ["No specific recommendations generated."]
        except Exception as e:
            print(f"Error generando recomendaciones con IA, usando heurÃ­stica. Error: {e}")
            return self._heuristic_recommendations(research_data, new_keywords)

    def _create_recommendation_prompt(self, research_data: List[Dict[str, Any]], new_keywords: List[str]) -> str:
        """Crea el prompt para que el LLM genere recomendaciones."""
        summary_stats = self.calculate_summary_stats(research_data, new_keywords)
        
        # Prepara un resumen de los hallazgos mÃ¡s importantes para el prompt.
        top_findings = []
        for data in research_data:
            platform = data.get("platform")
            if data.get("results"):
                top_result = data["results"][0]
                title = top_result.get("title") or top_result.get("name")
                if title:
                    top_findings.append(f"- From {platform}: Found '{title}' related to '{data.get('keyword')}'.")
        
        return (
            "You are an AI research analyst. Based on the following summary of a trend investigation, "
            "provide 3-5 actionable and insightful recommendations for a research team. "
            "Focus on what to investigate next, what technologies seem promising, and potential content ideas.\n\n"
            f"--- Data Summary ---\n"
            f"Total items found: {summary_stats['total_items']}\n"
            f"Platforms with most results: {', '.join(summary_stats['platform_breakdown'].keys())}\n"
            f"New keywords discovered: {', '.join(new_keywords)}\n"
            f"Top findings:\n{''.join(top_findings[:5])}\n"
            f"--- End of Summary ---\n\n"
            "Generate the recommendations as a bulleted list (e.g., - Recommendation 1). Do not add any introductory text."
        )

    def _heuristic_recommendations(self, research_data: List[Dict[str, Any]], new_keywords: List[str]) -> List[str]:
        """Genera una lista de recomendaciones de acciÃ³n simples basadas en los resultados."""
        recs: List[str] = []
        platform_counts = Counter([d.get("platform", "unknown") for d in research_data])

        # AÃ±ade recomendaciones especÃ­ficas segÃºn las plataformas que devolvieron datos.
        if platform_counts.get("github", 0) > 0:
            recs.append("Priorizar repositorios con una alta tasa de estrellas (star_rate) y actividad reciente.")
        if platform_counts.get("web", 0) > 0:
            recs.append("Revisar los resultados web en japonÃ©s para detectar tÃ©rminos emergentes locales.")
        if platform_counts.get("arxiv", 0) > 0:
            recs.append("Leer los abstracts recientes de arXiv (â‰¤ 30 dÃ­as) para captar nuevas lÃ­neas de investigaciÃ³n.")
        
        # Si no hay recomendaciones, sugiere ampliar la bÃºsqueda.
        if not recs:
            recs.append("Ampliar las fuentes o las palabras clave para obtener mÃ¡s seÃ±ales.")
        
        # Si se encontraron nuevas keywords, sugiere explorarlas.
        if new_keywords:
            recs.append(f"Explorar en profundidad las nuevas keywords descubiertas: {', '.join(new_keywords[:5])}...")

        return recs



---
File: /keyword_manager.py
---

import json
import os
from datetime import datetime
from typing import Dict, List, Optional, Any, TypedDict

# Define un tipo para la metadata de las keywords para mejorar la legibilidad y el autocompletado.
class KeywordMetadata(TypedDict, total=False):
    score: int
    status: str
    source: str
    created_date: str
    last_used: Optional[str]
    discovered_from: Optional[str]

MasterKeywords = Dict[str, KeywordMetadata]

class KeywordManager:
    """
    Gestiona el ciclo de vida de las palabras clave con persistencia en archivos JSON.
    Maneja tres archivos principales:
      - keywords/master.json: Un catÃ¡logo de todas las keywords descubiertas con sus metadatos.
      - keywords/active.json: Una lista simple de las keywords a investigar en la prÃ³xima ejecuciÃ³n.
      - keywords/history.json: Un registro de las ejecuciones pasadas.
    """

    def __init__(self, keywords_dir: str = "keywords"):
        """Constructor. Define las rutas a los archivos y se asegura de que existan."""
        self.keywords_dir = keywords_dir
        self.master_file = os.path.join(keywords_dir, "master.json")
        self.active_file = os.path.join(keywords_dir, "active.json")
        self.history_file = os.path.join(keywords_dir, "history.json")

        # Asegura que el directorio 'keywords' exista.
        os.makedirs(self.keywords_dir, exist_ok=True)
        # Si los archivos JSON no existen, los crea con un contenido inicial vacÃ­o.
        if not os.path.exists(self.master_file):
            self._atomic_write(self.master_file, {})
        if not os.path.exists(self.active_file):
            self._atomic_write(self.active_file, [])
        if not os.path.exists(self.history_file):
            self._atomic_write(self.history_file, {})

    # ---------------------------------
    # MÃ©todos para cargar/guardar JSON
    # ---------------------------------
    def load_master_keywords(self) -> MasterKeywords:
        """Carga el catÃ¡logo maestro de keywords desde master.json."""
        try:
            with open(self.master_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # Devuelve los datos solo si son un diccionario, para evitar errores.
                return data if isinstance(data, dict) else {}
        except (FileNotFoundError, json.JSONDecodeError):
            # Si hay algÃºn error (archivo no encontrado, JSON mal formado), devuelve un diccionario vacÃ­o.
            return {}

    def load_active_keywords(self) -> List[str]:
        """Carga la lista de keywords activas desde active.json."""
        try:
            with open(self.active_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # Devuelve los datos solo si son una lista.
                return data if isinstance(data, list) else []
        except (FileNotFoundError, json.JSONDecodeError):
            # En caso de error, devuelve una lista vacÃ­a.
            return []

    def load_history(self) -> Dict[str, Any]:
        """Carga el historial de ejecuciones desde history.json."""
        try:
            with open(self.history_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # Devuelve los datos solo si son un diccionario.
                return data if isinstance(data, dict) else {}
        except (FileNotFoundError, json.JSONDecodeError):
            # En caso de error, devuelve un diccionario vacÃ­o.
            return {}

    def save_master_keywords(self, keywords: MasterKeywords):
        """Guarda el catÃ¡logo maestro de keywords en master.json."""
        self._atomic_write(self.master_file, keywords)

    def save_active_keywords(self, keywords: List[str]):
        """Guarda la lista de keywords activas en active.json."""
        self._atomic_write(self.active_file, keywords)

    def save_history(self, history: Dict[str, Any]):
        """Guarda el historial de ejecuciones en history.json."""
        self._atomic_write(self.history_file, history)

    def _atomic_write(self, path: str, data: Any):
        """
        Realiza una escritura "atÃ³mica" para evitar la corrupciÃ³n de archivos.
        Primero escribe en un archivo temporal (.tmp) y, si tiene Ã©xito, lo renombra al archivo final.
        """
        tmp_path = path + ".tmp"
        try:
            with open(tmp_path, "w", encoding="utf-8") as f:
                # Vuelca los datos al archivo JSON con formato legible.
                json.dump(data, f, ensure_ascii=False, indent=2)
            # Reemplaza el archivo original con el nuevo archivo temporal.
            os.replace(tmp_path, path)
        except Exception as e:
            print(f"Error durante la escritura atÃ³mica en {path}: {e}")
            # Si hubo un error, intenta eliminar el archivo temporal si existe.
            if os.path.exists(tmp_path):
                os.remove(tmp_path)


    # ---------------------------------
    # API pÃºblica para gestionar keywords
    # ---------------------------------
    def add_new_keyword(
        self,
        keyword: str,
        score: int,
        status: str,
        source: str,
        discovered_from: Optional[str] = None
    ) -> bool:
        """
        AÃ±ade una nueva palabra clave al catÃ¡logo maestro si no existe previamente.
        Devuelve True si la keyword fue aÃ±adida, False si ya existÃ­a.
        """
        # Limpia la keyword de espacios en blanco.
        keyword = (keyword or "").strip()
        if not keyword:
            return False

        master = self.load_master_keywords()
        # Si la keyword ya estÃ¡ en el catÃ¡logo, no hace nada.
        if keyword in master:
            return False

        # Crea la nueva entrada para la palabra clave.
        now_date = datetime.now().strftime("%Y-%m-%d")
        entry: KeywordMetadata = {
            "score": int(score),
            "status": status,
            "source": source,
            "created_date": now_date,
            "last_used": None  # AÃºn no se ha usado para investigar.
        }
        if discovered_from:
            entry["discovered_from"] = discovered_from

        # AÃ±ade la nueva entrada al catÃ¡logo y lo guarda.
        master[keyword] = entry
        self.save_master_keywords(master)
        return True

    def update_keyword_score(self, keyword: str, new_score: int) -> bool:
        """Actualiza la puntuaciÃ³n de una palabra clave existente."""
        master = self.load_master_keywords()
        if keyword in master:
            master[keyword]["score"] = int(new_score)
            self.save_master_keywords(master)
            return True
        return False

    def mark_keywords_used(self, keywords: List[str]) -> None:
        """Marca una lista de palabras clave como 'usadas' en la fecha actual."""
        if not keywords:
            return

        master = self.load_master_keywords()
        today = datetime.now().strftime("%Y-%m-%d")

        changed = False
        for kw in keywords:
            kw = (kw or "").strip()
            if not kw:
                continue
            # Si la keyword no existÃ­a por alguna razÃ³n, la crea con datos por defecto.
            if kw not in master:
                master[kw] = {
                    "score": 0, "status": "unknown", "source": "runtime",
                    "created_date": today, "last_used": today
                }
                changed = True
            else:
                # Actualiza la fecha del Ãºltimo uso.
                master[kw]["last_used"] = today
                changed = True

        # Guarda los cambios solo si se realizÃ³ alguna modificaciÃ³n.
        if changed:
            self.save_master_keywords(master)

    def record_execution(self, keywords: List[str], status: str = "completed", new_keywords_found: int = 0) -> None:
        """Registra un resumen de la ejecuciÃ³n actual en el archivo de historial."""
        history = self.load_history()
        today = datetime.now().strftime("%Y-%m-%d")
        # Crea o sobrescribe la entrada para el dÃ­a de hoy.
        history[today] = {
            "keywords_used": keywords,
            "execution_time": datetime.now().strftime("%H:%M:%S"),
            "status": status,
            "new_keywords_found": int(new_keywords_found)
        }
        self.save_history(history)

    def get_top_keywords(self, limit: int = 10) -> List[str]:
        """Devuelve una lista de las N mejores keywords segÃºn su puntuaciÃ³n y fecha de Ãºltimo uso."""
        master = self.load_master_keywords()
        
        # Define una funciÃ³n de ordenaciÃ³n compleja:
        def sort_key(item: tuple[str, KeywordMetadata]):
            kw, meta = item
            score = int(meta.get("score", 0))
            # Trata 'None' o '' como la fecha mÃ¡s antigua para priorizar keywords nunca usadas.
            last_used = meta.get("last_used") or "1970-01-01" 
            # Ordena por puntuaciÃ³n descendente (-score) y luego por fecha de Ãºltimo uso ascendente.
            return (-score, last_used)

        # Ordena los Ã­tems del catÃ¡logo usando la clave definida.
        sorted_items = sorted(master.items(), key=sort_key)
        # Devuelve solo los nombres de las keywords del top N.
        return [kw for kw, _ in sorted_items[:limit]]

    def refresh_active_keywords(self, limit: int = 5) -> List[str]:
        """
        Selecciona las mejores keywords del catÃ¡logo maestro y las guarda en active.json
        para que sean usadas en la prÃ³xima ejecuciÃ³n.
        """
        top_keywords = self.get_top_keywords(limit=limit)
        self.save_active_keywords(top_keywords)
        return top_keywords



---
File: /mcp_client_manager.py
---

# mcp_client_manager.py
# -*- coding: utf-8 -*-

# Importa el mÃ³dulo 'asyncio' para la programaciÃ³n asÃ­ncrona.
import asyncio
# Importa el mÃ³dulo 'os' para leer variables de entorno.
import os
# Importa herramientas de 'typing' para anotaciones de tipo.
from typing import Dict, List, Any, Optional
# De 'contextlib', importa 'AsyncExitStack' para gestionar mÃºltiples contextos asÃ­ncronos de forma segura.
from contextlib import AsyncExitStack

# Importa las clases necesarias de la biblioteca 'mcp'.
from mcp import ClientSession
from mcp.client.stdio import stdio_client, StdioServerParameters


class RemoteMCPClient:
    """
    Representa un cliente para un Ãºnico servidor MCP que se ejecuta como un proceso local (ej. iniciado con npx).
    Gestiona la conexiÃ³n, los reintentos, las llamadas a herramientas y el cierre seguro.
    """

    def __init__(self):
        """Constructor. Inicializa el estado del cliente."""
        self.session: Optional[ClientSession] = None  # La sesiÃ³n de comunicaciÃ³n MCP.
        self.exit_stack: Optional[AsyncExitStack] = None  # Para gestionar recursos asÃ­ncronos.
        self._connected: bool = False  # Flag para indicar si la conexiÃ³n estÃ¡ activa.
        self._cleanup_attempted: bool = False  # Flag para evitar limpiezas duplicadas.
        self._available_tools: List[str] = []  # Lista de herramientas que ofrece el servidor.

    async def connect_to_server_by_name(
        self,
        server_name: str,
        args: List[str] = None,
        env: Dict[str, Any] = None
    ) -> bool:
        """
        Establece una conexiÃ³n con un servidor MCP a travÃ©s de su entrada/salida estÃ¡ndar (stdio).
        Implementa una lÃ³gica de reintentos y timeouts adaptables.
        """
        args = args or []
        joined_args = " ".join(args)

        # HeurÃ­stica para definir timeouts de conexiÃ³n mÃ¡s largos para servidores que tardan mÃ¡s en arrancar.
        base_timeout = 15.0
        if "one-search-mcp" in server_name or "one-search-mcp" in joined_args:
            init_timeout = 45.0
        elif "@langgpt/arxiv-mcp-server" in joined_args:
            init_timeout = 60.0
        else:
            init_timeout = base_timeout

        # Permite sobrescribir el timeout globalmente mediante una variable de entorno.
        try:
            init_timeout = float(os.getenv("MCP_INIT_TIMEOUT", init_timeout))
        except (ValueError, TypeError):
            pass

        # Prepara el diccionario de entorno, limpiÃ¡ndolo de valores nulos o vacÃ­os.
        clean_env: Optional[Dict[str, str]] = None
        if env:
            cleaned = {k: str(v) for k, v in env.items() if v not in (None, "")}
            clean_env = cleaned if cleaned else None

        attempts = 2  # NÃºmero de intentos de conexiÃ³n (1 original + 1 reintento).
        last_err: Optional[BaseException] = None
        full_cmd = " ".join([server_name] + args)

        # Bucle de intentos de conexiÃ³n.
        for attempt in range(1, attempts + 1):
            try:
                # Prepara un 'AsyncExitStack' para este intento.
                self.exit_stack = AsyncExitStack()

                print(f"[MCP] Conectando a '{os.path.basename(server_name)}' (intento {attempt}/{attempts})")
                if clean_env:
                    print(f"      â”–â”€ Entorno: {list(clean_env.keys())}")
                print(f"      â”–â”€ Timeout: {int(init_timeout)}s")


                # Define los parÃ¡metros para iniciar el servidor como un subproceso.
                server_params = StdioServerParameters(
                    command=server_name,
                    args=args,
                    env=clean_env
                )

                # Inicia el cliente stdio, que a su vez lanza el proceso del servidor.
                stdio_context = stdio_client(server_params)
                # Entra en el contexto del cliente para obtener los streams de lectura y escritura.
                read_stream, write_stream = await self.exit_stack.enter_async_context(stdio_context)

                # Crea una sesiÃ³n MCP usando los streams.
                session_context = ClientSession(read_stream, write_stream)
                self.session = await self.exit_stack.enter_async_context(session_context)

                # Llama al mÃ©todo 'initialize' del servidor con un tiempo de espera.
                try:
                    await asyncio.wait_for(self.session.initialize(), timeout=init_timeout)
                except asyncio.TimeoutError:
                    raise TimeoutError(f"Timeout en initialize() para '{os.path.basename(server_name)}'")

                # Si la inicializaciÃ³n es exitosa, obtiene la lista de herramientas disponibles.
                response = await self.session.list_tools()
                tools = response.tools
                self._available_tools = [tool.name for tool in tools]
                print(f"  âœ“ ConexiÃ³n exitosa a '{os.path.basename(server_name)}' | Herramientas: {self._available_tools}")

                self._connected = True
                return True  # ConexiÃ³n exitosa, sale del bucle.

            except Exception as e:
                # Si ocurre un error, lo registra y se prepara para el siguiente intento.
                last_err = e
                print(f"  âœ— Error al conectar '{os.path.basename(server_name)}' (intento {attempt}/{attempts}): {e}")

                # Limpia los recursos del intento fallido.
                try:
                    if self.exit_stack:
                        await asyncio.wait_for(self.exit_stack.aclose(), timeout=5.0)
                except Exception as close_err:
                    print(f"    Aviso: Error durante la limpieza del intento fallido: {close_err}")


                self.session = None
                self.exit_stack = None
                self._connected = False

                # Espera un poco antes de reintentar.
                if attempt < attempts:
                    await asyncio.sleep(2.0)

        print(f"  âœ— Fallo definitivo conectando a '{os.path.basename(server_name)}': {last_err}")
        return False

    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]):
        """Llama a una herramienta especÃ­fica del servidor MCP conectado."""
        if not self.session or not self._connected:
            raise ConnectionError("No estÃ¡ conectado a ningÃºn servidor MCP para llamar a la herramienta.")

        try:
            # Llama a la herramienta y espera la respuesta.
            response = await self.session.call_tool(tool_name, arguments)
            # Devuelve el contenido principal de la respuesta, que puede estar en 'content' o 'result'.
            if hasattr(response, "content"):
                return response.content
            if hasattr(response, "result"):
                return response.result
            return response
        except Exception as e:
            print(f"âœ— Error al llamar a la herramienta '{tool_name}': {e}")
            # Devuelve None o relanza una excepciÃ³n mÃ¡s especÃ­fica.
            return None

    def get_available_tools(self) -> List[str]:
        """Devuelve la lista de nombres de herramientas disponibles en el servidor."""
        return self._available_tools

    async def _cleanup(self):
        """MÃ©todo privado para cerrar y limpiar los recursos del cliente de forma segura."""
        if self._cleanup_attempted:
            return
        self._cleanup_attempted = True

        try:
            # Usa el 'AsyncExitStack' para cerrar todos los contextos abiertos (sesiÃ³n, proceso, etc.).
            if self.exit_stack:
                await asyncio.wait_for(self.exit_stack.aclose(), timeout=5.0)
        except asyncio.TimeoutError:
            print("Aviso: Tiempo de espera de limpieza agotado, forzando cierre")
        except asyncio.CancelledError:
            print("Aviso: La limpieza fue cancelada")
        except Exception as e:
            print(f"Aviso: Error durante la limpieza: {e}")
        finally:
            self.exit_stack = None

    async def close(self):
        """MÃ©todo pÃºblico para cerrar la conexiÃ³n con el servidor de forma segura."""
        if not self._connected:
            return
        self._connected = False
        try:
            # Llama al mÃ©todo de limpieza con un tiempo de espera.
            await asyncio.wait_for(self._cleanup(), timeout=10.0)
        except Exception as e:
            print(f"Aviso: Error durante el cierre: {e}")
        finally:
            # Resetea el estado del cliente.
            self.session = None
            self.exit_stack = None


class MCPClientManager:
    """
    Gestiona un conjunto de mÃºltiples 'RemoteMCPClient', uno para cada plataforma.
    Orquesta la conexiÃ³n y desconexiÃ³n de todos ellos.
    """

    def __init__(self, server_configs: Dict[str, Dict]):
        """Constructor. Recibe las configuraciones de todos los servidores."""
        self.server_configs = server_configs
        # Diccionario para almacenar las instancias de cliente, una por plataforma.
        self.clients: Dict[str, Optional[RemoteMCPClient]] = {}

    async def connect_all_servers(self):
        """Intenta conectar a todos los servidores que estÃ¡n marcados como habilitados en la configuraciÃ³n."""
        print("\n[MCP] Conectando a todos los servidores habilitados...")
        
        # Crea tareas para conectar a todos los servidores en paralelo.
        tasks = [
            self._connect_single_server(platform, config)
            for platform, config in self.server_configs.items()
            if config.get("enabled", False)
        ]
        
        # Ejecuta las tareas de conexiÃ³n concurrentemente.
        await asyncio.gather(*tasks)

        # Imprime los servidores omitidos.
        for platform, config in self.server_configs.items():
            if not config.get("enabled", False):
                print(f"  â†· Omitido '{platform}' (deshabilitado en config)")


    async def _connect_single_server(self, platform: str, config: Dict):
        """Crea un cliente y intenta conectar a un Ãºnico servidor MCP."""
        try:
            mcp_client = RemoteMCPClient()
            args = config.get("args", [])
            env = config.get("env", {})
            # Llama al mÃ©todo de conexiÃ³n del cliente.
            success = await mcp_client.connect_to_server_by_name(config["server_name"], args, env)

            # Si la conexiÃ³n es exitosa, almacena el cliente. Si no, almacena None.
            self.clients[platform] = mcp_client if success else None
        except Exception as e:
            print(f"  âœ— Fallo crÃ­tico al inicializar la conexiÃ³n para {platform}: {e}")
            self.clients[platform] = None

    def get_client(self, platform: str) -> Optional[RemoteMCPClient]:
        """Devuelve la instancia del cliente para una plataforma, o None si no estÃ¡ conectado."""
        return self.clients.get(platform)

    def is_platform_available(self, platform: str) -> bool:
        """Comprueba si el cliente de una plataforma estÃ¡ conectado y disponible."""
        client = self.clients.get(platform)
        return client is not None and client._connected

    def get_available_tools(self, platform: str) -> List[str]:
        """Obtiene la lista de herramientas disponibles para una plataforma especÃ­fica."""
        client = self.get_client(platform)
        return client.get_available_tools() if client else []

    async def close_all_clients(self):
        """
        Cierra todos los clientes MCP conectados de forma SECUENCIAL.
        Esto es importante en asyncio para evitar problemas de cancelaciÃ³n de tareas.
        """
        print("[MCP] Cerrando todos los clientes...")
        # Itera sobre una copia de los Ã­tems para poder modificar el diccionario original.
        for platform, client in list(self.clients.items()):
            if client:
                try:
                    print(f"      â”–â”€ Cerrando '{platform}'...")
                    await client.close()  # Espera a que cada cliente se cierre antes de pasar al siguiente.
                except Exception as e:
                    print(f"      â”–â”€ Error al cerrar '{platform}': {e}")
        # Limpia el diccionario de clientes.
        self.clients.clear()
        print("[MCP] Todos los clientes cerrados.")



---
File: /platform_handlers.py
---

# platform_handlers.py
# -*- coding: utf-8 -*-

# Este archivo es una versiÃ³n consolidada y mejorada que fusiona la lÃ³gica de
# platform_manager.py y platform_handlers.py, eliminando la redundancia.

from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from datetime import datetime
import json
import re
from collections import Counter
from urllib.parse import urlparse

# NOTA: No se importa AIClientManager aquÃ­ para evitar una dependencia circular.
# Se pasa como un argumento en el mÃ©todo de la fÃ¡brica 'create_handler'.

# ---------------- Clase Base ----------------
class BasePlatformHandler(ABC):
    """
    Define la plantilla (interfaz) que todos los manejadores de plataforma deben seguir.
    Garantiza una estructura consistente.
    """
    def __init__(self, platform_name: str):
        """Constructor. Almacena el nombre de la plataforma."""
        self.platform_name = platform_name
    
    @abstractmethod
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        """MÃ©todo abstracto para la lÃ³gica de investigaciÃ³n. Debe ser implementado por las subclases."""
        pass
    
    @abstractmethod
    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        """MÃ©todo abstracto para procesar la respuesta cruda. Debe ser implementado por las subclases."""
        pass
    
    def create_error_result(self, keyword: str, error: str) -> Dict[str, Any]:
        """MÃ©todo de utilidad para crear un resultado de error estandarizado."""
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": [], "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": {}, "error": str(error)
        }

# ---------------- Manejador de YouTube ----------------
class YouTubeHandler(BasePlatformHandler):
    """Manejador con la lÃ³gica especÃ­fica para investigar en YouTube."""
    def __init__(self):
        super().__init__("youtube")
    
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        try:
            params = {"query": keyword, "order": "relevance", "type": "video", "max_results": 10}
            tool_name = config["tools"][0]
            response = await client.call_tool(tool_name, params)
            return self.process_response(response, keyword)
        except Exception as e:
            return self.create_error_result(keyword, str(e))
    
    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        results = []
        data = self._extract_data_from_response(response)
        videos = data.get('videos', data.get('items', [])) if isinstance(data, dict) else (data if isinstance(data, list) else [])
        
        for video in videos:
            snippet = video.get('snippet', {})
            video_id = video.get('id', {}).get('videoId', '')
            if not video_id: continue

            results.append({
                'title': snippet.get('title', ''), 'description': snippet.get('description', ''),
                'published_at': snippet.get('publishedAt', ''), 'channel': snippet.get('channelTitle', ''),
                'video_id': video_id, 'url': f"https://www.youtube.com/watch?v={video_id}",
                'content_type': self._classify_content(snippet.get('title', ''), snippet.get('description', '')),
                'language': self._detect_language(snippet.get('title', '') + ' ' + snippet.get('description', ''))
            })
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": results, "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": self._calculate_engagement_metrics(results)
        }
    
    def _extract_data_from_response(self, response: Any) -> Any:
        if isinstance(response, list) and len(response) > 0 and hasattr(response[0], 'text'):
            try: return json.loads(response[0].text)
            except (json.JSONDecodeError, AttributeError): return {}
        elif hasattr(response, 'content'): return response.content
        return response if isinstance(response, (dict, list)) else {}

    def _classify_content(self, title: str, description: str) -> str:
        text = (title + ' ' + description).lower()
        if any(k in text for k in ['è§£èª¬', 'èª¬æ˜Ž', 'å…¥é–€', 'åŸºç¤Ž', 'å­¦ç¿’', 'ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«']): return "è§£èª¬å‹•ç”»"
        elif any(k in text for k in ['ãƒ‡ãƒ¢', 'ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³', 'å®Ÿæ¼”', 'ã‚µãƒ³ãƒ—ãƒ«']): return "ãƒ‡ãƒ¢"
        elif any(k in text for k in ['ã‚«ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹', 'ã‚»ãƒŸãƒŠãƒ¼', 'è¬›æ¼”', 'ç™ºè¡¨', 'talk']): return "ã‚«ãƒ³ãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹"
        elif any(k in text for k in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ€æ–°', 'ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ', 'ãƒªãƒªãƒ¼ã‚¹']): return "ãƒ‹ãƒ¥ãƒ¼ã‚¹"
        else: return "ãã®ä»–"
    
    def _detect_language(self, text: str) -> str:
        if re.search(r'[\u3040-\u30ff\u3400-\u4dbf\u4e00-\u9fff]', text): return "ja"
        return "en"
    
    def _calculate_engagement_metrics(self, results: List[Dict]) -> Dict:
        return {"total_videos": len(results)}

# ---------------- Manejador de GitHub ----------------
class GitHubHandler(BasePlatformHandler):
    """Manejador robusto para investigar en GitHub."""
    def __init__(self):
        super().__init__("github")
    
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        try:
            params = {
                "query": f"{keyword} stars:>50", "sort": "stars",
                "order": "desc", "per_page": 10
            }
            tool_name = "search_repositories"
            response = await client.call_tool(tool_name, params)
            return self.process_response(response, keyword)
        except Exception as e:
            return self.create_error_result(keyword, str(e))
    
    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        results = []
        repos = self._extract_repositories(response)
        for repo in repos:
            if isinstance(repo, dict):
                stars = repo.get('stargazers_count', repo.get('stars', 0))
                created_at = repo.get('created_at', '')
                star_rate, days_old, is_trending = self._calculate_trend_metrics(stars, created_at)
                results.append({
                    'name': repo.get('name', ''), 'description': repo.get('description', ''),
                    'owner': repo.get('owner', {}).get('login', ''), 'stars': stars, 'language': repo.get('language', ''),
                    'url': repo.get('html_url', ''), 'created_at': created_at, 'topics': repo.get('topics', []),
                    'star_rate': round(star_rate, 2), 'days_old': days_old, 'is_trending': is_trending,
                    'trend_score': self._calculate_trend_score(stars, days_old, star_rate)
                })
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": results, "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": self._calculate_engagement_metrics(results)
        }
    
    def _extract_repositories(self, response: Any) -> List[Dict]:
        if not response: return []
        if isinstance(response, str):
            try: return self._extract_repositories(json.loads(response))
            except json.JSONDecodeError: return []
        if isinstance(response, list) and len(response) > 0 and hasattr(response[0], 'text'):
            try: return self._extract_repositories(json.loads(response[0].text))
            except (json.JSONDecodeError, AttributeError): return []
        if isinstance(response, list): return response
        if isinstance(response, dict): return response.get('items', response.get('repositories', []))
        return []

    def _calculate_trend_metrics(self, stars: int, created_at: str) -> tuple:
        if not created_at or not isinstance(stars, int): return (0.0, 0, False)
        try:
            created_date = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
            days_old = (datetime.now(created_date.tzinfo) - created_date).days
            if days_old <= 0: return (stars, 0, False)
            star_rate = stars / days_old
            is_trending = (stars >= 100 and days_old <= 365 and star_rate > 0.5)
            return (star_rate, days_old, is_trending)
        except (ValueError, TypeError): return (0.0, 0, False)
    
    def _calculate_trend_score(self, stars: int, days_old: int, star_rate: float) -> float:
        if days_old <= 0: return 0.0
        base_score = min(star_rate * 10, 50)
        recency_bonus = max(0, (365 - days_old) / 365 * 30)
        star_bonus = min(stars / 200, 20)
        return round(min(base_score + recency_bonus + star_bonus, 100), 2)
    
    def _calculate_engagement_metrics(self, results: List[Dict]) -> Dict:
        if not results: return {"repo_count": 0}
        languages = [r.get('language') for r in results if r.get('language')]
        return {
            "repo_count": len(results),
            "total_stars": sum(r.get('stars', 0) for r in results),
            "trending_repos_count": sum(1 for r in results if r.get('is_trending')),
            "top_languages": dict(Counter(languages).most_common(3)) if languages else {}
        }

# ---------------- Manejador de Web ----------------
class WebHandler(BasePlatformHandler):
    def __init__(self):
        super().__init__("web")
    
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        try:
            params = {"query": f"{keyword} site:github.com OR site:arxiv.org OR site:huggingface.co", "language": "ja", "region": "jp", "max_results": 10}
            tool_name = config["tools"][0]
            response = await client.call_tool(tool_name, params)
            return self.process_response(response, keyword)
        except Exception as e:
            return self.create_error_result(keyword, str(e))
    
    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        results = self._parse_web_results(response)
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": results, "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": {"search_count": len(results)}
        }
    
    def _parse_web_results(self, response: Any) -> List[Dict]:
        if isinstance(response, list) and len(response) > 0 and hasattr(response[0], 'text'):
            try: response = json.loads(response[0].text)
            except (json.JSONDecodeError, AttributeError): response = {}
        
        results = []
        web_results = response.get('results', []) if isinstance(response, dict) else []
        for result in web_results:
            url = result.get('url', result.get('link', ''))
            results.append({
                'title': result.get('title', ''), 'snippet': result.get('snippet', ''),
                'url': url, 'source': urlparse(url).netloc if url else ''
            })
        return results

# ---------------- Manejador de ArXiv ----------------
class ArxivHandler(BasePlatformHandler):
    def __init__(self, ai_client_manager: Optional[Any] = None):
        super().__init__("arxiv")
        self.ai_client = ai_client_manager
    
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        try:
            english_query = await self._translate_keyword(keyword)
            params = {"query": english_query, "max_results": 10, "sort_by": "relevance"}
            response = await client.call_tool("search_arxiv", params)
            return self.process_response(response, keyword)
        except Exception as e:
            return self.create_error_result(keyword, str(e))

    async def _translate_keyword(self, keyword: str) -> str:
        if not self.ai_client or not re.search(r'[\u3040-\u30ff]', keyword): return keyword
        try:
            prompt = f"Translate the following Japanese technical keyword to English for an ArXiv search. Provide only the English translation, no extra text. Keyword: '{keyword}'"
            translation = await self.ai_client.chat_completion(prompt)
            return translation.strip().replace('"', '') or keyword
        except Exception: return keyword
            
    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        results = []
        papers = self._extract_papers(response)
        for paper in papers:
            if isinstance(paper, dict):
                published_date = paper.get('published', '')
                days_old, is_recent = self._calculate_time_metrics(published_date)
                results.append({
                    'title': paper.get('title', ''), 'abstract': paper.get('summary', ''),
                    'authors': paper.get('authors', []), 'published_date': published_date,
                    'url': paper.get('url', ''), 'days_old': days_old, 'is_recent': is_recent,
                })
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": results, "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": {"paper_count": len(results), "recent_paper_count": sum(1 for r in results if r['is_recent'])}
        }

    def _extract_papers(self, response: Any) -> List[Dict]:
        if isinstance(response, list) and len(response)>0 and hasattr(response[0], 'text'):
            try: response = json.loads(response[0].text)
            except (json.JSONDecodeError, AttributeError): return []
        if isinstance(response, dict): return response.get('results', [])
        if isinstance(response, list): return response
        return []

    def _calculate_time_metrics(self, published_date: str) -> tuple:
        if not published_date: return (9999, False)
        try:
            pub_date = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
            days_old = (datetime.now(pub_date.tzinfo) - pub_date).days
            return (days_old, days_old <= 90)
        except (ValueError, TypeError): return (9999, False)

# ---------------- Manejador de HackerNews ----------------
class HackerNewsHandler(BasePlatformHandler):
    def __init__(self, ai_client_manager: Optional[Any] = None):
        super().__init__("hackernews")
        self.ai_client = ai_client_manager
        
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        try:
            english_keyword = await self._translate_keyword(keyword)
            # HackerNews tool might be named 'search' or similar, adapt as needed.
            # Assuming the tool is 'getStories' for this implementation.
            params = {"query": english_keyword, "max_results": 15}
            response = await client.call_tool(config['tools'][0], params)
            return self.process_response(response, keyword)
        except Exception as e:
            return self.create_error_result(keyword, str(e))
    
    async def _translate_keyword(self, keyword: str) -> str:
        # Same translation logic as Arxiv
        if not self.ai_client or not re.search(r'[\u3040-\u30ff]', keyword): return keyword
        try:
            prompt = f"Translate the following Japanese keyword to a simple English equivalent for a HackerNews search. Provide only the English translation. Keyword: '{keyword}'"
            translation = await self.ai_client.chat_completion(prompt)
            return translation.strip().replace('"', '') or keyword
        except Exception: return keyword

    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        results = []
        posts = self._extract_posts(response)
        for post in posts:
            if isinstance(post, dict):
                results.append({
                    'title': post.get('title', ''), 'url': post.get('url', ''),
                    'score': post.get('score', post.get('points', 0)),
                    'comments_count': post.get('descendants', post.get('num_comments', 0)),
                })
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": results, "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": self._calculate_engagement_metrics(results)
        }
    
    def _extract_posts(self, response: Any) -> List[Dict]:
        if isinstance(response, list): return response
        if isinstance(response, dict): return response.get('hits', [])
        return []

    def _calculate_engagement_metrics(self, results: List[Dict]) -> Dict:
        if not results: return {"post_count": 0}
        post_count = len(results)
        total_score = sum(r.get('score', 0) for r in results)
        return {"post_count": post_count, "avg_score": round(total_score / post_count if post_count > 0 else 0)}

# ---------------- Manejador de Supabase (Placeholder) ----------------
class SupabaseHandler(BasePlatformHandler):
    def __init__(self):
        super().__init__("supabase")
    
    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        return self.create_error_result(keyword, "Supabase no se utiliza para la investigaciÃ³n de keywords.")
    
    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        return self.create_error_result(keyword, "Supabase no se utiliza para la investigaciÃ³n de keywords.")

# ---------------- Manejador de Research Hub ----------------
class ResearchHubHandler(BasePlatformHandler):
    def __init__(self):
        super().__init__("research_hub")

    async def research_keyword(self, client: Any, keyword: str, config: Dict) -> Dict[str, Any]:
        try:
            params = {"query": keyword, "limit": 10}
            response = await client.call_tool("search_papers", params)
            return self.process_response(response, keyword)
        except Exception as e:
            return self.create_error_result(keyword, str(e))

    def process_response(self, response: Any, keyword: str) -> Dict[str, Any]:
        results = []
        papers_data = response
        if isinstance(response, list) and len(response) > 0 and hasattr(response[0], 'content'):
            try: papers_data = json.loads(response[0].content)
            except (json.JSONDecodeError, AttributeError): papers_data = []

        if isinstance(papers_data, list):
            for paper in papers_data:
                results.append({
                    'title': paper.get('title', 'N/A'), 'authors': ", ".join(paper.get('authors', [])),
                    'url': paper.get('url', ''), 'abstract': paper.get('summary', ''),
                    'source': paper.get('source', 'Unknown'), 'year': paper.get('year', 0)
                })
        return {
            "platform": self.platform_name, "keyword": keyword, "timestamp": datetime.now().isoformat(),
            "results": results, "new_keywords": [], "sentiment_score": 0.0,
            "engagement_metrics": self._calculate_engagement_metrics(results)
        }

    def _calculate_engagement_metrics(self, results: List[Dict]) -> Dict:
        if not results: return {"paper_count": 0, "recent_papers_count": 0}
        current_year = datetime.now().year
        recent_papers = sum(1 for p in results if p.get('year', 0) >= current_year - 2)
        return {"paper_count": len(results), "recent_papers_count": recent_papers}

# ---------------- FÃ¡brica de Manejadores ----------------
class PlatformHandlerFactory:
    """Utiliza el patrÃ³n de diseÃ±o Factory para crear el manejador de plataforma correcto."""
    
    _handler_classes: Dict[str, Any] = {
        "youtube": YouTubeHandler,
        "github": GitHubHandler,
        "web": WebHandler,
        "arxiv": ArxivHandler,
        "hackernews": HackerNewsHandler,
        "supabase": SupabaseHandler,
        "research_hub": ResearchHubHandler
    }

    @staticmethod
    def create_handler(platform: str, ai_client_manager: Optional[Any] = None) -> BasePlatformHandler:
        handler_class = PlatformHandlerFactory._handler_classes.get(platform)
        if not handler_class:
            raise ValueError(f"No hay un manejador disponible para la plataforma: {platform}")
        
        # Inyecta el cliente de IA si el constructor del manejador lo acepta.
        import inspect
        sig = inspect.signature(handler_class.__init__)
        if 'ai_client_manager' in sig.parameters:
            return handler_class(ai_client_manager=ai_client_manager)
        else:
            return handler_class()



---
File: /README.md
---

# AI Trend Research Engine

Basado en los repositorios:
[EmpowerHerDev/ai-trend-research-system](https://github.com/EmpowerHerDev/ai-trend-research-system) y [Ladvien/research_hub_mcp](https://github.com/Ladvien/research_hub_mcp)

![Python Version](https://img.shields.io/badge/python-3.9%2B-blue)![License](https://img.shields.io/badge/license-MIT-green)![Status](https://img.shields.io/badge/status-activo-brightgreen)

<p align="center">
  <strong>Languages:</strong>
  <br>
  <a href="#-english">English</a> | <a href="#-espaÃ±ol">EspaÃ±ol</a> | <a href="#-catalÃ ">CatalÃ </a>
</p>

---

<a name="-english"></a>
## ðŸ‡¬ðŸ‡§ English

<details>
<summary><strong>Table of Contents</strong></summary>

- [ðŸš€ Key Features](#-key-features)
- [ðŸ›ï¸ Project Architecture](#ï¸-project-architecture)
- [ðŸ› ï¸ Installation and Setup](#ï¸-installation-and-setup)
  - [1. Prerequisites](#1-prerequisites)
  - [2. Clone the Repository](#2-clone-the-repository)
  - [3. Install Python Dependencies](#3-install-python-dependencies)
  - [4. Configure Environment Variables](#4-configure-environment-variables)
  - [5. Set Up the Database (Supabase)](#5-set-up-the-database-supabase)
- [â–¶ï¸ Usage](#ï¸-usage)
  - [Daily Trend Research](#daily-trend-research)
  - [Deep Dive Research](#deep-dive-research)
- [ðŸ¤ Contributing](#-contributing)
- [ðŸ“„ License](#-license)

</details>

An automated and modular system for researching trends in Artificial Intelligence. It collects and processes data from multiple platforms (GitHub, YouTube, ArXiv, etc.), uses language models (LLMs) to extract insights, discover new keywords, and generates comprehensive reports in multiple formats (JSON, Notion, Supabase).

**Note:** This project was developed by modifying the [EmpowerHerDev/ai-trend-research-system](https://github.com/EmpowerHerDev/ai-trend-research-system) project and integrating the custom research server from [Ladvien/research_hub_mcp](https://github.com/Ladvien/research_hub_mcp). The result is a flexible and modular system that now supports multiple large language model (LLM) providers, including **OpenAI (GPT), Google (Gemini), Anthropic (Claude), Groq, and Ollama**. This work is also part of a personal learning journey in the field of artificial intelligence agent development.

### ðŸš€ Key Features

-   **Multi-Platform Research**: Gathers data from YouTube, GitHub, web searches, ArXiv, HackerNews, and a custom paper research engine.
-   **AI-Powered Processing**: Uses LLMs (OpenAI, Gemini, Groq, Anthropic, Ollama) for advanced tasks such as:
    -   Intelligent extraction of new keywords.
    -   Generation of dynamic and contextual recommendations.
    -   Translation of search terms for multilingual queries.
-   **Asynchronous Architecture**: Built with `asyncio` for high concurrency and efficiency in network operations and process handling.
-   **Report Generation**: Automatically creates detailed reports in:
    -   **JSON**: Local files for archiving and analysis.
    -   **Notion**: Structured and easy-to-read pages in your workspace.
    -   **Supabase**: Records in a PostgreSQL database for long-term persistence.
-   **Keyword Management**: Maintains a lifecycle for keywords, with a master catalog, an active list for research, and a history of executions.
-   **Highly Configurable**: All settings (API keys, AI model selection, file paths) are managed through a `.env` file for easy portability and security.
-   **Standard Protocol**: Uses the **Model Context Protocol (MCP)** to communicate with each platform's servers, ensuring a standardized and decoupled interface.

### ðŸ›ï¸ Project Architecture

The system is designed with a modular and decoupled architecture to facilitate maintenance and extension.

-   `ai_trend_researcher.py`: The main orchestrator that runs the daily research workflow.
-   `config_manager.py`: Centralizes the loading and validation of all configurations from the `.env` file.
-   `mcp_client_manager.py`: Manages the lifecycle (connection, calls, closing) of clients for each platform's MCP servers.
-   `platform_handlers.py`: Contains the specific logic to interact with each platform (e.g., `YouTubeHandler`, `GitHubHandler`), process their data, and standardize it.
-   `data_processor.py`: Handles data analysis, keyword extraction, and recommendation generation using both heuristics and LLMs.
-   `ai_client_manager.py`: Acts as a factory to interact uniformly with different language model providers (OpenAI, Gemini, etc.).
-   `report_generator.py`: Generates the final reports in all supported formats (JSON, Notion, Supabase).
-   `keyword_manager.py`: Manages the keyword database in JSON files.
-   `research_assistant.py`: An advanced script for performing deep research dives using the custom `research_hub` server.

### ðŸ› ï¸ Installation and Setup

Follow these steps to get the project running in your local environment.

#### 1. Prerequisites

-   **Python 3.9+**
-   **Node.js and npm** (required for `npx`, which runs the community's MCP servers).
-   **Git**
-   (Optional) **Rust Compiler**, if you want to compile the `research_hub` executable from the source code.

#### 2. Clone the Repository

```bash
git clone https://github.com/your_user/your_repository.git
cd your_repository
```

#### 3. Install Python Dependencies

Create a virtual environment (recommended) and install the required libraries.

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

#### 4. Configure Environment Variables

This is the most important step. The project is controlled via a `.env` file.

1.  Copy the example file:
    ```bash
    cp .env.example .env
    ```
2.  Edit the `.env` file with a text editor and fill in all the necessary API keys and paths.

```env
# .env.example

# --- API Keys (Required for the platforms you use) ---
YOUTUBE_API_KEY=AIzaSy...
GITHUB_PERSONAL_ACCESS_TOKEN=ghp_...
NOTION_API_KEY=secret_...
SUPABASE_ACCESS_TOKEN=eyJhbGciOi...
SILICONFLOW_API_KEY= # Optional, for the ArXiv server

# --- Notion Configuration (Required if using Notion) ---
NOTION_PARENT_PAGE_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# --- System Path Configuration (IMPORTANT!) ---
# Modify these paths to point to directories on your machine.
RESEARCH_PAPERS_DIR="/home/user/documents/research-papers"
RESEARCH_HUB_EXECUTABLE="/home/user/projects/rust-research-mcp/target/release/rust-research-mcp"

# --- AI Provider Configuration ---
# Choose one: "openai", "gemini", "groq", "anthropic", "ollama"
AI_PROVIDER="openai"

# Fill in the API key for your chosen provider.
OPENAI_API_KEY="sk-..."
GOOGLE_API_KEY="AIzaSy..."
GROQ_API_KEY="gsk_..."
ANTHROPIC_API_KEY="sk-ant-..."

# --- (Optional) Specific AI Models ---
# If left blank, default models will be used.
AI_MODEL_OPENAI="gpt-4o"
AI_MODEL_GEMINI="gemini-1.5-flash"
AI_MODEL_GROQ="llama3-70b-8192"
AI_MODEL_ANTHROPIC="claude-3-haiku-20240307"
AI_MODEL_OLLAMA="llama3"
```

#### 5. Set Up the Database (Supabase)

If you plan to use the Supabase integration, make sure your database table matches the schema defined in `supabase_schema.sql`. You can run this script in the SQL editor of your Supabase project.

### â–¶ï¸ Usage

Once configured, you can run the two main workflows.

#### Daily Trend Research

This is the main workflow. It will run the research on all enabled platforms, analyze the data, and generate reports.

```bash
python ai_trend_researcher.py
```

The script will print its progress to the console. Upon completion, you will find the JSON report in the `reports/` directory and, if configured, a new page in Notion and a new record in your Supabase table.

#### Deep Dive Research

This script uses the `research_hub` server to perform advanced paper searches, download them, analyze them, and generate bibliographies.

1.  Ensure the `research_hub` executable is correctly configured in your `.env`.
2.  (Optional) Add search terms to the `terminos.txt` file.

```bash
python research_assistant.py
```

The results of this execution (CSVs, JSONs, BibTeX files, and logs) will be saved in subdirectories within `salidas/` to keep each run organized.

### ðŸ¤ Contributing

Contributions are welcome. If you have ideas for improving the project, new platforms to integrate, or find any bugs, please open an issue or submit a pull request.

### ðŸ“„ License

This project is licensed under the MIT License. See the `LICENSE` file for more details.

---

<a name="-espaÃ±ol"></a>
## ðŸ‡ªðŸ‡¸ EspaÃ±ol

<details>
<summary><strong>Tabla de Contenidos</strong></summary>

- [ðŸš€ CaracterÃ­sticas Principales](#-caracterÃ­sticas-principales)
- [ðŸ›ï¸ Arquitectura del Proyecto](#ï¸-arquitectura-del-proyecto)
- [ðŸ› ï¸ InstalaciÃ³n y ConfiguraciÃ³n](#ï¸-instalaciÃ³n-y-configuraciÃ³n)
  - [1. Prerrequisitos](#1-prerrequisitos)
  - [2. Clonar el Repositorio](#2-clonar-el-repositorio)
  - [3. Instalar Dependencias de Python](#3-instalar-dependencias-de-python)
  - [4. Configurar las Variables de Entorno](#4-configurar-las-variables-de-entorno)
  - [5. Configurar la Base de Datos (Supabase)](#5-configurar-la-base-de-datos-supabase)
- [â–¶ï¸ Uso](#ï¸-uso)
  - [InvestigaciÃ³n Diaria de Tendencias](#investigaciÃ³n-diaria-de-tendencias)
  - [InmersiÃ³n Profunda de InvestigaciÃ³n](#inmersiÃ³n-profunda-de-investigaciÃ³n)
- [ðŸ¤ Contribuciones](#-contribuciones)
- [ðŸ“„ Licencia](#-licencia)

</details>

Un sistema automatizado y modular para la investigaciÃ³n de tendencias en Inteligencia Artificial. Recopila y procesa datos de mÃºltiples plataformas (GitHub, YouTube, ArXiv, etc.), utiliza modelos de lenguaje (LLMs) para extraer *insights*, descubrir nuevas palabras clave y genera informes completos en mÃºltiples formatos (JSON, Notion, Supabase).

**Nota:** Este proyecto ha sido desarrollado modificando el proyecto [EmpowerHerDev/ai-trend-research-system](https://github.com/EmpowerHerDev/ai-trend-research-system) e integrando el servidor de investigaciÃ³n personalizado de [Ladvien/research_hub_mcp](https://github.com/Ladvien/research_hub_mcp). El resultado es un sistema flexible y modular que ahora es compatible con mÃºltiples proveedores de modelos de lenguaje grande (LLM), incluyendo **OpenAI (GPT), Google (Gemini), Anthropic (Claude), Groq y Ollama**. Este trabajo tambiÃ©n forma parte de un proceso de aprendizaje personal en el campo del desarrollo de agentes de inteligencia artificial.

### ðŸš€ CaracterÃ­sticas Principales

-   **InvestigaciÃ³n Multiplataforma**: Recopila datos de YouTube, GitHub, bÃºsquedas web, ArXiv, HackerNews y un motor de investigaciÃ³n de *papers* personalizado.
-   **Procesamiento con IA**: Utiliza LLMs (OpenAI, Gemini, Groq, Anthropic, Ollama) para tareas avanzadas como:
    -   ExtracciÃ³n inteligente de nuevas palabras clave.
    -   GeneraciÃ³n de recomendaciones dinÃ¡micas y contextuales.
    -   TraducciÃ³n de tÃ©rminos de bÃºsqueda para consultas multilingÃ¼es.
-   **Arquitectura AsÃ­ncrona**: Construido con `asyncio` para una alta concurrencia y eficiencia en operaciones de red y manejo de procesos.
-   **GeneraciÃ³n de Informes**: Crea automÃ¡ticamente informes detallados en:
    -   **JSON**: Archivos locales para archivo y anÃ¡lisis.
    -   **Notion**: PÃ¡ginas estructuradas y fÃ¡ciles de leer en tu *workspace*.
    -   **Supabase**: Registros en una base de datos PostgreSQL para persistencia a largo plazo.
-   **GestiÃ³n de Keywords**: Mantiene un ciclo de vida para las palabras clave, con un catÃ¡logo maestro, una lista activa para investigar y un historial de ejecuciones.
-   **Altamente Configurable**: Toda la configuraciÃ³n (API keys, selecciÃ³n de modelos de IA, rutas de archivos) se gestiona a travÃ©s de un archivo `.env` para facilitar la portabilidad y seguridad.
-   **Protocolo EstÃ¡ndar**: Utiliza el **Model Context Protocol (MCP)** para comunicarse con los servidores de cada plataforma, asegurando una interfaz estandarizada y desacoplada.

### ðŸ›ï¸ Arquitectura del Proyecto

El sistema estÃ¡ diseÃ±ado con una arquitectura modular y desacoplada para facilitar su mantenimiento y extensiÃ³n.

-   `ai_trend_researcher.py`: El orquestador principal que ejecuta el flujo de investigaciÃ³n diario.
-   `config_manager.py`: Centraliza la carga y validaciÃ³n de toda la configuraciÃ³n desde el archivo `.env`.
-   `mcp_client_manager.py`: Gestiona el ciclo de vida (conexiÃ³n, llamadas, cierre) de los clientes para los servidores MCP de cada plataforma.
-   `platform_handlers.py`: Contiene la lÃ³gica especÃ­fica para interactuar con cada plataforma (ej. `YouTubeHandler`, `GitHubHandler`), procesar sus datos y estandarizarlos.
-   `data_processor.py`: Se encarga del anÃ¡lisis de datos, la extracciÃ³n de keywords y la generaciÃ³n de recomendaciones utilizando tanto heurÃ­sticas como LLMs.
-   `ai_client_manager.py`: ActÃºa como una fÃ¡brica para interactuar de forma unificada con diferentes proveedores de modelos de lenguaje (OpenAI, Gemini, etc.).
-   `report_generator.py`: Genera los informes finales en todos los formatos soportados (JSON, Notion, Supabase).
-   `keyword_manager.py`: Administra la base de datos de palabras clave en archivos JSON.
-   `research_assistant.py`: Un script avanzado para realizar inmersiones profundas de investigaciÃ³n utilizando el servidor personalizado `research_hub`.

### ðŸ› ï¸ InstalaciÃ³n y ConfiguraciÃ³n

Sigue estos pasos para poner en marcha el proyecto en tu entorno local.

#### 1. Prerrequisitos

-   **Python 3.9+**
-   **Node.js y npm** (necesario para `npx`, que ejecuta los servidores MCP de la comunidad).
-   **Git**
-   (Opcional) **Compilador de Rust**, si deseas compilar el ejecutable de `research_hub` desde el cÃ³digo fuente.

#### 2. Clonar el Repositorio

```bash
git clone https://github.com/tu_usuario/tu_repositorio.git
cd tu_repositorio
```

#### 3. Instalar Dependencias de Python

Crea un entorno virtual (recomendado) e instala las bibliotecas necesarias.

```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
pip install -r requirements.txt```

#### 4. Configurar las Variables de Entorno

Este es el paso mÃ¡s importante. El proyecto se controla mediante un archivo `.env`.

1.  Copia el archivo de ejemplo:
    ```bash
    cp .env.example .env
    ```
2.  Edita el archivo `.env` con un editor de texto y rellena todas las claves de API y rutas necesarias.

```env
# .env.example

# --- Claves de API (Obligatorias para las plataformas que uses) ---
YOUTUBE_API_KEY=AIzaSy...
GITHUB_PERSONAL_ACCESS_TOKEN=ghp_...
NOTION_API_KEY=secret_...
SUPABASE_ACCESS_TOKEN=eyJhbGciOi...
SILICONFLOW_API_KEY= # Opcional, para el servidor de ArXiv

# --- ConfiguraciÃ³n de Notion (Obligatoria si se usa Notion) ---
NOTION_PARENT_PAGE_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# --- ConfiguraciÃ³n de Rutas del Sistema (Â¡IMPORTANTE!) ---
# Modifica estas rutas para que apunten a directorios en tu mÃ¡quina.
RESEARCH_PAPERS_DIR="/home/user/documentos/research-papers"
RESEARCH_HUB_EXECUTABLE="/home/user/proyectos/rust-research-mcp/target/release/rust-research-mcp"

# --- ConfiguraciÃ³n del Proveedor de IA ---
# Elige uno: "openai", "gemini", "groq", "anthropic", "ollama"
AI_PROVIDER="openai"

# Rellena la clave de API para el proveedor que hayas elegido.
OPENAI_API_KEY="sk-..."
GOOGLE_API_KEY="AIzaSy..."
GROQ_API_KEY="gsk_..."
ANTHROPIC_API_KEY="sk-ant-..."

# --- (Opcional) Modelos EspecÃ­ficos de IA ---
# Si se dejan en blanco, se usarÃ¡n los modelos por defecto.
AI_MODEL_OPENAI="gpt-4o"
AI_MODEL_GEMINI="gemini-1.5-flash"
AI_MODEL_GROQ="llama3-70b-8192"
AI_MODEL_ANTHROPIC="claude-3-haiku-20240307"
AI_MODEL_OLLAMA="llama3"
```

#### 5. Configurar la Base de Datos (Supabase)

Si planeas usar la integraciÃ³n con Supabase, asegÃºrate de que tu tabla en la base de datos coincida con el esquema definido en `supabase_schema.sql`. Puedes ejecutar ese script en el editor SQL de tu proyecto de Supabase.

### â–¶ï¸ Uso

Una vez configurado, puedes ejecutar los dos flujos de trabajo principales.

#### InvestigaciÃ³n Diaria de Tendencias

Este es el flujo principal. EjecutarÃ¡ la investigaciÃ³n en todas las plataformas habilitadas, analizarÃ¡ los datos y generarÃ¡ los informes.

```bash
python ai_trend_researcher.py
```

El script imprimirÃ¡ su progreso en la consola. Al finalizar, encontrarÃ¡s el informe JSON en el directorio `reports/` y, si estÃ¡ configurado, una nueva pÃ¡gina en Notion y un nuevo registro en tu tabla de Supabase.

#### InmersiÃ³n Profunda de InvestigaciÃ³n

Este script utiliza el servidor `research_hub` para realizar bÃºsquedas avanzadas de *papers*, descargarlos, analizarlos y generar bibliografÃ­as.

1.  AsegÃºrate de que el ejecutable `research_hub` estÃ© correctamente configurado en tu `.env`.
2.  (Opcional) AÃ±ade tÃ©rminos de bÃºsqueda al archivo `terminos.txt`.

```bash
python research_assistant.py
```

Los resultados de esta ejecuciÃ³n (CSVs, JSONs, archivos BibTeX y logs) se guardarÃ¡n en subdirectorios dentro de `salidas/` para mantener cada ejecuciÃ³n organizada.

### ðŸ¤ Contribuciones

Las contribuciones son bienvenidas. Si tienes ideas para mejorar el proyecto, nuevas plataformas para integrar o encuentras algÃºn error, por favor abre un *issue* o envÃ­a un *pull request*.

### ðŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Consulta el archivo `LICENSE` para mÃ¡s detalles.

---

<a name="-catalÃ "></a>
## CAT CatalÃ 

<details>
<summary><strong>Taula de Continguts</strong></summary>

- [ðŸš€ CaracterÃ­stiques Principals](#-caracterÃ­stiques-principals)
- [ðŸ›ï¸ Arquitectura del Projecte](#ï¸-arquitectura-del-projecte)
- [ðŸ› ï¸ InstalÂ·laciÃ³ i ConfiguraciÃ³](#ï¸-installaciÃ³-i-configuraciÃ³)
  - [1. Prerequisits](#1-prerequisits)
  - [2. Clonar el Repositori](#2-clonar-el-repositori)
  - [3. InstalÂ·lar DependÃ¨ncies de Python](#3-installar-dependÃ¨ncies-de-python)
  - [4. Configurar les Variables d'Entorn](#4-configurar-les-variables-dentorn)
  - [5. Configurar la Base de Dades (Supabase)](#5-configurar-la-base-de-dades-supabase)
- [â–¶ï¸ Ãšs](#ï¸-Ãºs)
  - [Recerca DiÃ ria de TendÃ¨ncies](#recerca-diÃ ria-de-tendÃ¨ncies)
  - [Recerca d'ImmersiÃ³ Profunda](#recerca-dimmersiÃ³-profunda)
- [ðŸ¤ Contribucions](#-contribucions)
- [ðŸ“„ LlicÃ¨ncia](#-llicÃ¨ncia)

</details>

Un sistema automatitzat i modular per a la investigaciÃ³ de tendÃ¨ncies en IntelÂ·ligÃ¨ncia Artificial. Recopila i processa dades de mÃºltiples plataformes (GitHub, YouTube, ArXiv, etc.), utilitza models de llenguatge (LLMs) per extreure *insights*, descobrir noves paraules clau i genera informes complets en mÃºltiples formats (JSON, Notion, Supabase).

**Nota:** Aquest projecte ha estat desenvolupat modificant el projecte [EmpowerHerDev/ai-trend-research-system](https://github.com/EmpowerHerDev/ai-trend-research-system) i integrant el servidor de recerca personalitzat de [Ladvien/research_hub_mcp](https://github.com/Ladvien/research_hub_mcp). El resultat Ã©s un sistema flexible i modular que ara Ã©s compatible amb mÃºltiples proveÃ¯dors de models de llenguatge grans (LLM), incloent-hi **OpenAI (GPT), Google (Gemini), Anthropic (Claude), Groq i Ollama**. Aquest treball tambÃ© forma part d'un procÃ©s d'aprenentatge personal en el camp del desenvolupament d'agents d'intelÂ·ligÃ¨ncia artificial.

### ðŸš€ CaracterÃ­stiques Principals

-   **Recerca Multiplataforma**: Recopila dades de YouTube, GitHub, cerques web, ArXiv, HackerNews i un motor de recerca de *papers* personalitzat.
-   **Processament amb IA**: Utilitza LLMs (OpenAI, Gemini, Groq, Anthropic, Ollama) per a tasques avanÃ§ades com:
    -   ExtracciÃ³ intelÂ·ligent de noves paraules clau.
    -   GeneraciÃ³ de recomanacions dinÃ miques i contextuals.
    -   TraducciÃ³ de termes de cerca per a consultes multilingÃ¼es.
-   **Arquitectura AsÃ­ncrona**: ConstruÃ¯t amb `asyncio` per a una alta concurrÃ¨ncia i eficiÃ¨ncia en operacions de xarxa i gestiÃ³ de processos.
-   **GeneraciÃ³ d'Informes**: Crea automÃ ticament informes detallats en:
    -   **JSON**: Fitxers locals per a arxiu i anÃ lisi.
    -   **Notion**: PÃ gines estructurades i fÃ cils de llegir al teu *workspace*.
    -   **Supabase**: Registres en una base de dades PostgreSQL per a persistÃ¨ncia a llarg termini.
-   **GestiÃ³ de Keywords**: MantÃ© un cicle de vida per a les paraules clau, amb un catÃ leg mestre, una llista activa per investigar i un historial d'execucions.
-   **Altament Configurable**: Tota la configuraciÃ³ (claus d'API, selecciÃ³ de models d'IA, rutes de fitxers) es gestiona mitjanÃ§ant un fitxer `.env` per facilitar la portabilitat i seguretat.
-   **Protocol EstÃ ndard**: Utilitza el **Model Context Protocol (MCP)** per comunicar-se amb els servidors de cada plataforma, assegurant una interfÃ­cie estandarditzada i desacoblada.

### ðŸ›ï¸ Arquitectura del Projecte

El sistema estÃ  dissenyat amb una arquitectura modular i desacoblada per facilitar el seu manteniment i extensiÃ³.

-   `ai_trend_researcher.py`: L'orquestrador principal que executa el flux de recerca diari.
-   `config_manager.py`: Centralitza la cÃ rrega i validaciÃ³ de tota la configuraciÃ³ des del fitxer `.env`.
-   `mcp_client_manager.py`: Gestiona el cicle de vida (connexiÃ³, trucades, tancament) dels clients per als servidors MCP de cada plataforma.
-   `platform_handlers.py`: ContÃ© la lÃ²gica especÃ­fica per interactuar amb cada plataforma (ex. `YouTubeHandler`, `GitHubHandler`), processar les seves dades i estandarditzar-les.
-   `data_processor.py`: S'encarrega de l'anÃ lisi de dades, l'extracciÃ³ de paraules clau i la generaciÃ³ de recomanacions utilitzant tant heurÃ­stiques com LLMs.
-   `ai_client_manager.py`: Actua com una fÃ brica per interactuar de manera unificada amb diferents proveÃ¯dors de models de llenguatge (OpenAI, Gemini, etc.).
-   `report_generator.py`: Genera els informes finals en tots els formats suportats (JSON, Notion, Supabase).
-   `keyword_manager.py`: Administra la base de dades de paraules clau en fitxers JSON.
-   `research_assistant.py`: Un script avanÃ§at per realitzar immersions profundes de recerca utilitzant el servidor personalitzat `research_hub`.

### ðŸ› ï¸ InstalÂ·laciÃ³ i ConfiguraciÃ³

Segueix aquests passos per posar en marxa el projecte al teu entorn local.

#### 1. Prerequisits

-   **Python 3.9+**
-   **Node.js i npm** (necessari per a `npx`, que executa els servidors MCP de la comunitat).
-   **Git**
-   (Opcional) **Compilador de Rust**, si vols compilar l'executable de `research_hub` des del codi font.

#### 2. Clonar el Repositori

```bash
git clone https://github.com/el_teu_usuari/el_teu_repositori.git
cd el_teu_repositori
```

#### 3. InstalÂ·lar DependÃ¨ncies de Python

Crea un entorn virtual (recomanat) i instalÂ·la les llibreries necessÃ ries.

```bash
python -m venv venv
source venv/bin/activate  # A Windows: venv\Scripts\activate
pip install -r requirements.txt
```

#### 4. Configurar les Variables d'Entorn

Aquest Ã©s el pas mÃ©s important. El projecte es controla mitjanÃ§ant un fitxer `.env`.

1.  Copia el fitxer d'exemple:
    ```bash
    cp .env.example .env
    ```
2.  Edita el fitxer `.env` amb un editor de text i omple totes les claus d'API i rutes necessÃ ries.

```env
# .env.example

# --- Claus d'API (ObligatÃ²ries per a les plataformes que facis servir) ---
YOUTUBE_API_KEY=AIzaSy...
GITHUB_PERSONAL_ACCESS_TOKEN=ghp_...
NOTION_API_KEY=secret_...
SUPABASE_ACCESS_TOKEN=eyJhbGciOi...
SILICONFLOW_API_KEY= # Opcional, per al servidor d'ArXiv

# --- ConfiguraciÃ³ de Notion (ObligatÃ²ria si s'usa Notion) ---
NOTION_PARENT_PAGE_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# --- ConfiguraciÃ³ de Rutes del Sistema (IMPORTANT!) ---
# Modifica aquestes rutes perquÃ¨ apuntin a directoris a la teva mÃ quina.
RESEARCH_PAPERS_DIR="/home/user/documents/research-papers"
RESEARCH_HUB_EXECUTABLE="/home/user/projectes/rust-research-mcp/target/release/rust-research-mcp"

# --- ConfiguraciÃ³ del ProveÃ¯dor d'IA ---
# Tria'n un: "openai", "gemini", "groq", "anthropic", "ollama"
AI_PROVIDER="openai"

# Omple la clau d'API per al proveÃ¯dor que hagis triat.
OPENAI_API_KEY="sk-..."
GOOGLE_API_KEY="AIzaSy..."
GROQ_API_KEY="gsk_..."
ANTHROPIC_API_KEY="sk-ant-..."

# --- (Opcional) Models EspecÃ­fics d'IA ---
# Si es deixen en blanc, s'utilitzaran els models per defecte.
AI_MODEL_OPENAI="gpt-4o"
AI_MODEL_GEMINI="gemini-1.5-flash"
AI_MODEL_GROQ="llama3-70b-8192"
AI_MODEL_ANTHROPIC="claude-3-haiku-20240307"
AI_MODEL_OLLAMA="llama3"
```

#### 5. Configurar la Base de Dades (Supabase)

Si planeges fer servir la integraciÃ³ amb Supabase, assegura't que la teva taula a la base de dades coincideixi amb l'esquema definit a `supabase_schema.sql`. Pots executar aquest script a l'editor SQL del teu projecte de Supabase.

### â–¶ï¸ Ãšs

Un cop configurat, pots executar els dos fluxos de treball principals.

#### Recerca DiÃ ria de TendÃ¨ncies

Aquest Ã©s el flux principal. ExecutarÃ  la recerca a totes les plataformes habilitades, analitzarÃ  les dades i generarÃ  els informes.

```bash
python ai_trend_researcher.py
```

L'script imprimirÃ  el seu progrÃ©s a la consola. En finalitzar, trobarÃ s l'informe JSON al directori `reports/` i, si estÃ  configurat, una nova pÃ gina a Notion i un nou registre a la teva taula de Supabase.

#### Recerca d'ImmersiÃ³ Profunda

Aquest script utilitza el servidor `research_hub` per realitzar cerques avanÃ§ades de *papers*, descarregar-los, analitzar-los i generar bibliografies.

1.  Assegura't que l'executable `research_hub` estigui correctament configurat al teu `.env`.
2.  (Opcional) Afegeix termes de cerca al fitxer `terminos.txt`.

```bash
python research_assistant.py```

Els resultats d'aquesta execuciÃ³ (CSVs, JSONs, fitxers BibTeX i logs) es desaran en subdirectoris dins de `salidas/` per mantenir cada execuciÃ³ organitzada.

### ðŸ¤ Contribucions

Les contribucions sÃ³n benvingudes. Si tens idees per millorar el projecte, noves plataformes per integrar o trobes algun error, si us plau obre un *issue* o envia un *pull request*.

### ðŸ“„ LlicÃ¨ncia

Aquest projecte estÃ  sota la LlicÃ¨ncia MIT. Consulta el fitxer `LICENSE` per a mÃ©s detalls.



---
File: /report_generator.py
---

# report_generator.py

# Importa el mÃ³dulo 'json' para trabajar con datos JSON.
import json
# Importa el mÃ³dulo 'os' para interactuar con el sistema de archivos (crear directorios y archivos).
import os
# Importa 'datetime' para obtener la fecha y hora actuales.
from datetime import datetime
# Importa herramientas de 'typing' para anotaciones de tipo.
from typing import Dict, List, Any, Optional
# Importa la clase 'RemoteMCPClient' para interactuar con los servidores de Notion y Supabase.
from mcp_client_manager import RemoteMCPClient


class JSONReportGenerator:
    """Genera informes de la investigaciÃ³n en formato de archivo JSON."""
    
    def __init__(self, reports_dir: str = "reports"):
        """Constructor. Define el directorio donde se guardarÃ¡n los informes."""
        self.reports_dir = reports_dir
    
    def generate_report(self, research_data: List[Dict], new_keywords: List[str], 
                       summary: Dict[str, Any], recommendations: List[str]) -> str:
        """Crea un archivo JSON con todos los datos de la investigaciÃ³n."""
        today = datetime.now().strftime("%Y-%m-%d")
        # Define la estructura del informe.
        report = {
            "date": today,
            "summary": summary,
            "new_keywords": new_keywords,
            "recommendations": recommendations,
            "detailed_results": research_data
        }
        
        # Construye la ruta completa del archivo.
        report_file = os.path.join(self.reports_dir, f"ai_trends_{today}.json")
        # Asegura que el directorio de informes exista.
        os.makedirs(self.reports_dir, exist_ok=True)
        
        # Abre el archivo en modo escritura y vuelca el diccionario del informe como JSON.
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2) # indent=2 para formato legible.
        
        return report_file # Devuelve la ruta del archivo creado.


class NotionReportGenerator:
    """Genera informes como una nueva pÃ¡gina en Notion."""
    
    def __init__(self, notion_client: Optional[RemoteMCPClient], parent_page_id: str):
        """Constructor. Necesita el cliente MCP de Notion y el ID de la pÃ¡gina padre."""
        self.notion_client = notion_client
        self.parent_page_id = parent_page_id
    
    async def create_notion_report(self, report: Dict) -> Any:
        """Crea una pÃ¡gina en Notion con el contenido del informe."""
        # Si no hay cliente o ID de pÃ¡gina, no se puede crear el informe.
        if not self.notion_client or not self.parent_page_id:
            print("Aviso: Cliente de Notion o ID de pÃ¡gina padre no disponible. Omitiendo informe de Notion.")
            return None
        
        try:
            today = report.get("date", datetime.now().strftime("%Y-%m-%d"))
            page_title = f"Informe de Tendencias IA - {today}"
            
            print("  - Creando bloques de contenido para Notion...")
            blocks = self._create_notion_blocks(report)
            
            # Valida la estructura de los bloques antes de enviarlos a la API de Notion.
            if not self._validate_blocks_structure(blocks):
                print("Error: La estructura de los bloques de Notion generados es invÃ¡lida. Omitiendo informe.")
                return None
            
            # Llama a la herramienta 'create-page' del servidor MCP de Notion.
            response = await self.notion_client.call_tool(
                "create-page",
                {
                    "parent_type": "page_id",
                    "parent_id": self.parent_page_id,
                    # Las propiedades (como el tÃ­tulo) deben ser un string JSON.
                    "properties": json.dumps({
                        "title": {"title": [{"text": {"content": page_title}}]}
                    }),
                    # El contenido (los bloques) tambiÃ©n debe ser un string JSON.
                    "children": json.dumps(blocks)
                }
            )
            
            print(f"  âœ“ Informe de Notion creado: '{page_title}'")
            return response
            
        except Exception as e:
            print(f"  âœ— Error al crear el informe de Notion: {e}")
            return None
    
    def _validate_blocks_structure(self, blocks: List[Dict]) -> bool:
        """Valida que la estructura bÃ¡sica de los bloques sea correcta para la API de Notion."""
        if not isinstance(blocks, list): return False
        for block in blocks:
            if not isinstance(block, dict): return False
            if "object" not in block or block["object"] != "block": return False
            if "type" not in block: return False
            block_type = block["type"]
            if block_type not in block: return False
        return True
    
    def _create_rich_text(self, text: str) -> List[Dict]:
        """FunciÃ³n de utilidad para crear un objeto 'rich_text' de Notion."""
        return [{"type": "text", "text": {"content": text}}]

    def _create_notion_blocks(self, report: Dict) -> List[Dict]:
        """Crea una lista detallada de bloques de contenido de Notion a partir de los datos del informe."""
        blocks = []
        
        # --- Resumen ---
        blocks.append({"object": "block", "type": "heading_2", "heading_2": {"rich_text": self._create_rich_text("ðŸ“Š Resumen")}})
        summary = report.get("summary", {})
        summary_text = (
            f"â€¢ Resultados Totales: {summary.get('total_items', 0)}\n"
            f"â€¢ Nuevas Keywords: {summary.get('new_keywords_count', 0)}\n"
            f"â€¢ Ejecuciones con Errores: {summary.get('runs_with_errors', 0)}"
        )
        blocks.append({"object": "block", "type": "paragraph", "paragraph": {"rich_text": self._create_rich_text(summary_text)}})
        
        # --- Recomendaciones ---
        blocks.append({"object": "block", "type": "heading_2", "heading_2": {"rich_text": self._create_rich_text("ðŸ“‹ Recomendaciones")}})
        recommendations = report.get("recommendations", [])
        if recommendations:
            for rec in recommendations:
                if rec and isinstance(rec, str) and rec.strip():
                    blocks.append({"object": "block", "type": "bulleted_list_item", "bulleted_list_item": {"rich_text": self._create_rich_text(rec.strip())}})
        else:
            blocks.append({"object": "block", "type": "paragraph", "paragraph": {"rich_text": self._create_rich_text("No se generaron recomendaciones.")}})
        
        # --- Nuevas Keywords ---
        blocks.append({"object": "block", "type": "heading_2", "heading_2": {"rich_text": self._create_rich_text("ðŸ” Nuevas Keywords Descubiertas")}})
        new_keywords = report.get("new_keywords", [])
        if new_keywords:
            blocks.append({"object": "block", "type": "paragraph", "paragraph": {"rich_text": self._create_rich_text(", ".join(new_keywords))}})
        else:
            blocks.append({"object": "block", "type": "paragraph", "paragraph": {"rich_text": self._create_rich_text("No se encontraron nuevas keywords.")}})

        # --- Resultados Detallados ---
        blocks.append({"object": "block", "type": "heading_2", "heading_2": {"rich_text": self._create_rich_text("ðŸ”¬ Resultados Detallados por Plataforma")}})
        platform_results = self._group_results_by_platform(report.get("detailed_results", []))
        
        for platform, results in platform_results.items():
            if not results: continue
            blocks.append({"object": "block", "type": "heading_3", "heading_3": {"rich_text": self._create_rich_text(f" à¤ªà¥à¤²à¥‡à¤Ÿà¤«à¥‰à¤°à¥à¤®: {platform.upper()}")}})
            for i, result in enumerate(results[:3], 1): # Limita a los 3 primeros resultados para ser conciso.
                title = str(result.get('title', result.get('name', 'Sin tÃ­tulo'))).strip()
                url = result.get('url', '')
                snippet = (result.get('description', result.get('snippet', '')) or "")[:250].strip()
                
                if not title: continue
                
                toggle_block = {
                    "object": "block",
                    "type": "toggle",
                    "toggle": {
                        "rich_text": self._create_rich_text(f"{i}. {title}"),
                        "children": []
                    }
                }
                
                if url:
                    toggle_block["toggle"]["children"].append({"object": "block", "type": "paragraph", "paragraph": {"rich_text": self._create_rich_text(f"ðŸ”— URL: {url}")}})
                if snippet:
                     toggle_block["toggle"]["children"].append({"object": "block", "type": "paragraph", "paragraph": {"rich_text": self._create_rich_text(f"ðŸ“ Extracto: {snippet}...")}})
                
                blocks.append(toggle_block)
        
        return blocks
    
    def _group_results_by_platform(self, detailed_results: List[Dict]) -> Dict[str, List[Dict]]:
        """Agrupa una lista de resultados en un diccionario por plataforma."""
        platform_results = {}
        for data in detailed_results:
            platform = data.get("platform", "unknown")
            if not data.get("error") and data.get("results"):
                if platform not in platform_results:
                    platform_results[platform] = []
                platform_results[platform].extend(data["results"])
        return platform_results


class SupabaseReportGenerator:
    """Genera informes guardando los datos en una tabla de Supabase."""
    
    def __init__(self, supabase_client: Optional[RemoteMCPClient]):
        """Constructor. Necesita el cliente MCP de Supabase."""
        self.supabase_client = supabase_client
    
    async def create_supabase_report(self, report: Dict) -> Any:
        """Inserta los datos del informe en una tabla de la base de datos Supabase."""
        if not self.supabase_client:
            print("Aviso: Cliente de Supabase no disponible. Omitiendo informe de Supabase.")
            return None
        
        try:
            today = report.get("date", datetime.now().strftime("%Y-%m-%d"))
            
            # Define la consulta SQL para insertar los datos.
            # ADVERTENCIA: Este mÃ©todo formatea una cadena SQL. Es seguro en este contexto
            # porque los datos son generados por la propia aplicaciÃ³n, pero para datos
            # externos, se deben usar consultas parametrizadas si el servidor MCP las soporta.
            sql = """
            INSERT INTO ai_trend_reports (date, summary, detailed_results, new_keywords, recommendations)
            VALUES ('{date}', '{summary}', '{detailed_results}', '{new_keywords}', '{recommendations}')
            RETURNING id;
            """
            
            # Prepara los parÃ¡metros, convirtiendo los diccionarios/listas a strings JSON
            # y escapando comillas simples para evitar errores de sintaxis SQL.
            params = {
                "date": today,
                "summary": json.dumps(report.get("summary", {}), ensure_ascii=False).replace("'", "''"),
                "detailed_results": json.dumps(report.get("detailed_results", []), ensure_ascii=False).replace("'", "''"),
                "new_keywords": json.dumps(report.get("new_keywords", []), ensure_ascii=False).replace("'", "''"),
                "recommendations": json.dumps(report.get("recommendations", []), ensure_ascii=False).replace("'", "''"),
            }
            # Formatea la consulta SQL con los parÃ¡metros.
            query = sql.format(**params)

            # Llama a la herramienta 'execute_sql' del servidor MCP de Supabase.
            response = await self.supabase_client.call_tool("execute_sql", {"query": query})
            
            print(f"  âœ“ Informe de Supabase creado para la fecha: {today}")
            return response
            
        except Exception as e:
            print(f"  âœ— Error al crear el informe de Supabase: {e}")
            return None


class ReportManager:
    """Clase de alto nivel que gestiona la generaciÃ³n de todos los tipos de informes."""
    
    def __init__(self, reports_dir: str = "reports", notion_client: Optional[RemoteMCPClient] = None, 
                 notion_parent_id: Optional[str] = None, supabase_client: Optional[RemoteMCPClient] = None):
        """Constructor. Inicializa todos los generadores de informes necesarios."""
        self.json_generator = JSONReportGenerator(reports_dir)
        self.notion_generator = NotionReportGenerator(notion_client, notion_parent_id)
        self.supabase_generator = SupabaseReportGenerator(supabase_client)
    
    async def generate_all_reports(self, research_data: List[Dict], new_keywords: List[str],
                                 summary: Dict[str, Any], recommendations: List[str]) -> str:
        """Orquesta la generaciÃ³n de informes en JSON, Notion y Supabase."""
        # Primero, siempre genera el informe JSON local.
        report_file = self.json_generator.generate_report(
            research_data, new_keywords, summary, recommendations
        )
        
        # Prepara un diccionario unificado con los datos del informe.
        report_data = {
            "date": datetime.now().strftime("%Y-%m-%d"),
            "summary": summary,
            "detailed_results": research_data,
            "new_keywords": new_keywords,
            "recommendations": recommendations
        }
        
        # Genera los informes de Notion y Supabase en paralelo si estÃ¡n disponibles.
        tasks = []
        if self.notion_generator:
            tasks.append(self.notion_generator.create_notion_report(report_data))
        if self.supabase_generator:
            tasks.append(self.supabase_generator.create_supabase_report(report_data))
            
        if tasks:
            await asyncio.gather(*tasks)
        
        return report_file # Devuelve la ruta del archivo JSON como confirmaciÃ³n.



---
File: /requirement.txt
---

# Biblioteca oficial para interactuar con los modelos de IA de Anthropic (Claude).
anthropic

# Biblioteca para el protocolo MCP (Model Context Protocol), que estandariza la comunicaciÃ³n con herramientas externas.
mcp

# Biblioteca estÃ¡ndar en Python para realizar peticiones HTTP a APIs y servicios web.
requests

# Biblioteca para cargar las variables de entorno definidas en el archivo .env en el script de Python.
python-dotenv

# --- Bibliotecas de IA aÃ±adidas ---

# Biblioteca oficial de Google para interactuar con los modelos de IA Gemini.
google-generativeai

# Biblioteca oficial para interactuar con la API de Groq, que ofrece inferencia rÃ¡pida de LLMs.
groq

# Biblioteca para interactuar con Ollama, que permite ejecutar modelos de IA localmente.
ollama

# Biblioteca oficial para interactuar con los modelos de IA de OpenAI (GPT).
openai



---
File: /requirements.txt
---

# Biblioteca oficial para interactuar con los modelos de IA de Anthropic (Claude).
anthropic

# Biblioteca para el protocolo MCP (Model Context Protocol), que estandariza la comunicaciÃ³n con herramientas externas.
mcp

# Biblioteca estÃ¡ndar en Python para realizar peticiones HTTP a APIs y servicios web.
requests

# Biblioteca para cargar las variables de entorno definidas en el archivo .env en el script de Python.
python-dotenv

# --- Bibliotecas de IA aÃ±adidas ---

# Biblioteca oficial de Google para interactuar con los modelos de IA Gemini.
google-generativeai

# Biblioteca oficial para interactuar con la API de Groq, que ofrece inferencia rÃ¡pida de LLMs.
groq

# Biblioteca para interactuar con Ollama, que permite ejecutar modelos de IA localmente.
ollama

# Biblioteca oficial para interactuar con los modelos de IA de OpenAI (GPT).
openai



---
File: /research_assistant_con_hackers_LLM.py
---

# research_assistant_con_hackers_LLM.py
# ============================================================
# Flujo de investigaciÃ³n con rust-research-mcp (MCP server)
# 1. Lee 'terminos.txt' y construye topics (MAX_TOPICS).
# 2. Busca papers (search_papers) para DOIs y metadatos.
# 3. Descarga PDFs (download_paper) con control de paralelismo.
# 4. Genera bibliografÃ­a (BibTeX) desde los metadatos recolectados.
# 5. Busca discusiones en Hacker News por cada topic y filtra con un LLM.
# ============================================================

from __future__ import annotations
import asyncio
import json
import os
import re
from datetime import datetime
from typing import Any, Iterable, List, Dict, Optional

from dotenv import load_dotenv
import aiofiles
import aiofiles.os

# MÃ³dulos del proyecto
from config_manager import ServerConfig, AppConfig
from mcp_client_manager import MCPClientManager, RemoteMCPClient
from ai_client_manager import AIClientManager

load_dotenv()

# ============================================================
# ðŸ”§ PARÃMETROS CONFIGURABLES (env-first)
# ============================================================

def env_bool(name: str, default: bool) -> bool:
    val = os.getenv(name, str(default)).strip().lower()
    return val in ("1", "true", "t", "yes", "y", "on")

def env_int(name: str, default: int) -> int:
    try:
        v = os.getenv(name)
        return int(v) if v is not None else default
    except ValueError:
        return default

# â€”â€”â€” Control de salidas â€”â€”â€”
SEPARATE_RUNS_IN_SUBFOLDER: bool = env_bool("SEPARATE_RUNS_IN_SUBFOLDER", True)
RUN_TAG: Optional[str] = os.getenv("RUN_TAG")

# â€”â€”â€” BÃºsqueda y filtrado â€”â€”â€”
MAX_TOPICS: int = env_int("MAX_TOPICS", 10)
MAX_RESULTS_PER_TOPIC: int = env_int("MAX_RESULTS_PER_TOPIC", 10)
MAX_HN_RESULTS_PER_TOPIC: int = env_int("MAX_HN_RESULTS_PER_TOPIC", 15)


# â€”â€”â€” Descarga de PDFs â€”â€”â€”
DOWNLOAD_ALL_PAPERS: bool = env_bool("DOWNLOAD_ALL_PAPERS", False)
SELECT_TOP_K: int = env_int("SELECT_TOP_K", 5)
MAX_PARALLEL_DOWNLOADS: int = env_int("MAX_PARALLEL_DOWNLOADS", 4)

# â€”â€”â€” Timeouts/reintentos MCP â€”â€”â€”
MCP_INIT_RETRIES: int = env_int("MCP_INIT_RETRIES", 1)
RESEARCH_HUB_INIT_TIMEOUT: int = env_int("RESEARCH_HUB_INIT_TIMEOUT", 45)
HACKERNEWS_INIT_TIMEOUT: int = env_int("HACKERNEWS_INIT_TIMEOUT", 15)

# ============================================================
# Constantes y Utilidades
# ============================================================
DOI_REGEX = re.compile(r'^10\.\d{4,9}/.+$')

def slugify(s: str) -> str:
    """Convierte un string en un formato seguro para nombres de archivo."""
    s = str(s).lower().strip()
    s = re.sub(r'[\s\W-]+', '-', s)
    s = s.strip('-')
    return s[:75] if len(s) > 75 else s

def now_str() -> str:
    """Devuelve la fecha y hora actual como un string formateado."""
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def extract_text(blobs: Optional[Iterable[Any]]) -> str:
    """Extrae y une el contenido de texto de una respuesta MCP."""
    if not blobs: 
        return ""
    parts: List[str] = []
    for b in blobs:
        if hasattr(b, "text"):
            parts.append(getattr(b, "text", "") or "")
        elif isinstance(b, str):
            parts.append(b)
    return "\n".join(parts).strip()

def parse_text_response_to_papers(raw_text: str, topic: str) -> List[Dict[str, Any]]:
    """Parsea la respuesta de texto del servidor Rust para la bÃºsqueda de papers."""
    papers = []
    if not raw_text:
        return papers

    content = raw_text.split('\n\n', 1)[-1]
    paper_blocks = re.split(r'\n\n(?=\d+\.\s)', content)

    for block in paper_blocks:
        if not block.strip():
            continue
        paper_data = {"topic": topic, "title": None, "doi": None, "source": None, "year": None, "authors": None, "journal": None}
        
        # ExtracciÃ³n de campos con expresiones regulares
        title_match = re.search(r'^\d+\.\s*(.*?)\s*\(Relevance:', block)
        if title_match: paper_data['title'] = title_match.group(1).strip()
        
        doi_match = re.search(r'ðŸ“–\s*DOI:\s*(.*)', block)
        if doi_match:
            doi_str = doi_match.group(1).strip()
            paper_data['doi'] = doi_str if doi_str and " " not in doi_str else None

        source_match = re.search(r'ðŸ”\s*Source:\s*(.*)', block)
        if source_match: paper_data['source'] = source_match.group(1).strip()
            
        year_match = re.search(r'ðŸ“…\s*Year:\s*(\d{4})', block)
        if year_match: paper_data['year'] = int(year_match.group(1).strip())

        authors_match = re.search(r'ðŸ‘¤\s*Authors:\s*(.*)', block)
        if authors_match: paper_data['authors'] = authors_match.group(1).strip()

        journal_match = re.search(r'Journal:\s*(.*)', block)
        if journal_match: paper_data['journal'] = journal_match.group(1).strip()
            
        if paper_data.get('title'):
            paper_data['url'] = f"https://doi.org/{paper_data['doi']}" if paper_data.get('doi') else None
            paper_data['is_valid_doi'] = bool(paper_data['doi'] and DOI_REGEX.match(paper_data['doi']))
            papers.append(paper_data)
            
    return papers

class AdvancedResearchAssistant:
    """Orquesta el flujo de investigaciÃ³n avanzada, gestionando clientes, archivos y lÃ³gica de negocio."""
    
    def __init__(self, topics: List[str]):
        """Inicializa el asistente con los temas de investigaciÃ³n."""
        self.topics = topics
        self.mcp_manager: MCPClientManager = None
        self.ai_client: Optional[AIClientManager] = None
        self.rh_client: Optional[RemoteMCPClient] = None
        self.hn_client: Optional[RemoteMCPClient] = None

        # Directorios de salida (inicializados como None)
        self.salidas_dir = self.csv_dir = self.logs_dir = self.bib_dir = self.json_dir = None
        self.downloads_dir = os.getenv("RESEARCH_PAPERS_DIR", os.path.join(os.getcwd(), "ResearchPapers"))

    async def setup_output_dirs(self):
        """Configura los directorios de salida para esta ejecuciÃ³n."""
        base = "salidas"
        if SEPARATE_RUNS_IN_SUBFOLDER:
            tag = RUN_TAG or datetime.now().strftime("%Y%m%d_%H%M%S")
            base = os.path.join(base, tag)
        
        self.salidas_dir = base
        self.csv_dir = os.path.join(base, "csv")
        self.logs_dir = os.path.join(base, "logs")
        self.bib_dir = os.path.join(base, "bib")
        self.json_dir = os.path.join(base, "json")
        
        for d in (self.salidas_dir, self.csv_dir, self.logs_dir, self.bib_dir, self.json_dir, self.downloads_dir):
            await aiofiles.os.makedirs(d, exist_ok=True)
        print(f"ðŸ“‚ Salidas se guardarÃ¡n en: {self.salidas_dir}")
        print(f"ðŸ“¥ PDFs se guardarÃ¡n en: {self.downloads_dir}")

    async def _save_json(self, name: str, data: Any):
        """Guarda datos en un archivo JSON en el directorio de salida."""
        path = os.path.join(self.json_dir, name)
        async with aiofiles.open(path, "w", encoding="utf-8") as f:
            await f.write(json.dumps(data, ensure_ascii=False, indent=2))
        print(f"ðŸ’¾ JSON guardado: {os.path.basename(path)}")
    
    async def _save_csv(self, name: str, rows: List[Dict[str, Any]]):
        """Guarda una lista de diccionarios en un archivo CSV."""
        if not rows: return
        path = os.path.join(self.csv_dir, name)
        async with aiofiles.open(path, "w", encoding="utf-8-sig", newline="") as f:
            fields = list(rows[0].keys())
            await f.write(",".join(fields) + "\n")
            for row in rows:
                values = [f"\"{str(row.get(k, '')).replace('\"', '\"\"')}\"" for k in fields]
                await f.write(",".join(values) + "\n")
        print(f"ðŸ’¾ CSV guardado: {os.path.basename(path)} ({len(rows)} filas)")

    async def _save_bib(self, name: str, content: str):
        """Guarda contenido en un archivo BibTeX."""
        path = os.path.join(self.bib_dir, name)
        async with aiofiles.open(path, "w", encoding="utf-8") as f:
            await f.write(content)
        print(f"ðŸ“š BibliografÃ­a guardada: {os.path.basename(path)}")

    async def _save_log(self, name: str, content: str):
        """Guarda texto en un archivo de registro."""
        path = os.path.join(self.logs_dir, name)
        async with aiofiles.open(path, "w", encoding="utf-8") as f:
            await f.write(content)
        print(f"ðŸ“œ Log de error guardado: {os.path.basename(path)}")

    async def _connect_with_retries(self, name: str, cfg: Dict[str, Any], timeout: int, retries: int) -> bool:
        """Intenta conectar a un servidor MCP con reintentos."""
        delay = 2.0
        for attempt in range(retries + 1):
            try:
                print(f"[{now_str()}] [MCP] Conectando a '{name}' (intento {attempt+1}/{retries+1}) | timeout={timeout}s")
                await asyncio.wait_for(self.mcp_manager._connect_single_server(name, cfg), timeout=timeout)
                if self.mcp_manager.is_platform_available(name):
                    print(f"  [Ã‰XITO] ConexiÃ³n establecida con '{name}'")
                    return True
            except Exception as e:
                err = f"Timeout en initialize() para '{name}'" if isinstance(e, asyncio.TimeoutError) else str(e)
            print(f"  [FALLO] Error conectando a '{name}': {err}")
            if attempt < retries:
                await asyncio.sleep(delay)
                delay *= 2
        print(f"  [FALLO] ConexiÃ³n definitiva fallida con '{name}'")
        return False

    async def connect_services(self) -> bool:
        """Inicializa y conecta a todos los servicios externos (MCP, IA)."""
        # 1. Conectar servidores MCP
        server_configs = ServerConfig.get_server_configs()
        self.mcp_manager = MCPClientManager(server_configs)
        
        print("\nðŸ”— Conectando a servidores MCPâ€¦")
        rh_cfg = server_configs.get("research_hub", {})
        hn_cfg = server_configs.get("hackernews", {})

        rh_ok, hn_ok = await asyncio.gather(
            self._connect_with_retries("research_hub", rh_cfg, RESEARCH_HUB_INIT_TIMEOUT, MCP_INIT_RETRIES),
            self._connect_with_retries("hackernews", hn_cfg, HACKERNEWS_INIT_TIMEOUT, MCP_INIT_RETRIES),
        )
        self.rh_client = self.mcp_manager.get_client("research_hub") if rh_ok else None
        self.hn_client = self.mcp_manager.get_client("hackernews") if hn_ok else None

        # 2. Inicializar cliente de IA
        try:
            ai_provider = AppConfig.get_ai_provider()
            api_key = AppConfig.get_api_key(ai_provider)
            ai_model = AppConfig.get_ai_model(ai_provider)
            if ai_provider != "ollama" and not api_key:
                print(f"âš ï¸  [OMITIDO] No hay API Key para {ai_provider}. Se omitirÃ¡ el filtrado con LLM.")
            else:
                self.ai_client = AIClientManager(provider=ai_provider, api_key=api_key, model=ai_model)
                print(f"âœ… Cliente IA inicializado: {ai_provider.upper()} (modelo: {ai_model or 'default'})")
        except Exception as e:
            print(f"âš ï¸  [FALLO] No se pudo inicializar el cliente de IA: {e}. Se omitirÃ¡ el filtrado con LLM.")
        
        return self.rh_client is not None or self.hn_client is not None

    async def _search_papers(self) -> List[Dict[str, Any]]:
        """Paso A: Busca papers para cada topic y los devuelve combinados y deduplicados."""
        print("\n--- PASO A: BÃºsqueda de Papers AcadÃ©micos ---")
        if not self.rh_client:
            print("  [OMITIDO] El cliente de Research Hub no estÃ¡ disponible.")
            return []
        
        all_papers: List[Dict[str, Any]] = []
        for topic in self.topics:
            print(f"  -> Buscando topic: '{topic}'...")
            try:
                res = await self.rh_client.call_tool("search_papers", {"query": topic, "limit": MAX_RESULTS_PER_TOPIC})
                papers = parse_text_response_to_papers(extract_text(res), topic)
                all_papers.extend(papers)
                print(f"     Encontrados {len(papers)} resultados.")
            except Exception as e:
                print(f"     [FALLO] Error buscando topic '{topic}': {e}")
        
        # Deduplicado por DOI
        seen_dois = set()
        unique_papers = [p for p in all_papers if p.get("doi") not in seen_dois and not seen_dois.add(p.get("doi"))]
        
        print(f"\nâœ¨ [Ã‰XITO] Total de papers Ãºnicos encontrados: {len(unique_papers)}")
        return unique_papers

    async def _select_and_download_papers(self, papers: List[Dict[str, Any]]):
        """Pasos B y C: Selecciona DOIs para descargar y ejecuta la descarga en paralelo."""
        print("\n--- PASOS B & C: SelecciÃ³n y Descarga de PDFs ---")
        if not self.rh_client:
            print("  [OMITIDO] El cliente de Research Hub no estÃ¡ disponible.")
            return

        # Paso B: SelecciÃ³n
        selected_dois = [p['doi'] for p in papers if p.get('is_valid_doi')]
        if not DOWNLOAD_ALL_PAPERS:
            selected_dois = selected_dois[:SELECT_TOP_K]
            print(f"  -> SelecciÃ³n: TOP {SELECT_TOP_K} papers ({len(selected_dois)} con DOI vÃ¡lido).")
        else:
            print(f"  -> SelecciÃ³n: TODOS los papers ({len(selected_dois)} con DOI vÃ¡lido).")
        
        if not selected_dois:
            print("  [AVISO] No hay DOIs vÃ¡lidos para descargar.")
            return

        await self._save_json("02_dois_seleccionados.json", {"dois": selected_dois})

        # Paso C: Descarga
        print(f"  -> Descargando {len(selected_dois)} papers en paralelo (max {MAX_PARALLEL_DOWNLOADS})...")
        semaphore = asyncio.Semaphore(MAX_PARALLEL_DOWNLOADS)
        doi_map = {p['doi']: p for p in papers if p.get('doi')}

        async def _download_one(doi: str):
            async with semaphore:
                title = doi_map.get(doi, {}).get('title', 'untitled')
                filename = f"{slugify(title)}_{slugify(doi)}.pdf"
                try:
                    res = await self.rh_client.call_tool("download_paper", {"doi": doi, "filename": filename})
                    raw_text = extract_text(res)
                    if "Download successful!" in raw_text or "File already exists" in raw_text:
                        print(f"     âœ“ Descargado (o ya existÃ­a): {doi}")
                        return doi, {"status": "ok", "title": title}
                    print(f"     âœ— Fallo en descarga: {doi}")
                    return doi, {"status": "failed", "reason": raw_text.strip()}
                except Exception as e:
                    print(f"     âœ— Error grave durante la descarga de {doi}: {e}")
                    return doi, {"status": "error", "reason": str(e)}

        tasks = [_download_one(doi) for doi in selected_dois]
        results = await asyncio.gather(*tasks)
        download_manifest = {doi: result for doi, result in results}
        await self._save_json("03_manifiesto_descarga.json", download_manifest)
        print("âœ¨ [Ã‰XITO] Proceso de descarga completado.")

    def _paper_to_bibtex(self, paper: Dict[str, Any]) -> str:
        """Convierte un diccionario de metadatos de un paper a una entrada BibTeX."""
        author_lastname = "unknown"
        if paper.get("authors"):
            try: author_lastname = slugify(paper["authors"].split(',')[0].split(' ')[-1])
            except: pass
        
        year_str = str(paper.get('year', 'nodate'))
        title_slug = slugify(paper.get('title', 'notitle')[:10])
        key = f"{author_lastname}{year_str}{title_slug}"

        entry = f"@article{{{key},\n"
        if paper.get('title'): entry += f"  title     = {{{{{paper['title']}}}}},\n"
        if paper.get('authors'): entry += f"  author    = {{{paper['authors']}}},\n"
        if paper.get('year'): entry += f"  year      = {{{paper['year']}}},\n"
        if paper.get('journal'): entry += f"  journal   = {{{paper['journal']}}},\n"
        if paper.get('doi'): entry += f"  doi       = {{{paper['doi']}}},\n"
        entry += "}"
        return entry
        
    async def _generate_bibliography(self, papers: List[Dict[str, Any]]):
        """Paso D: Genera una bibliografÃ­a completa a partir de los metadatos recolectados."""
        print("\n--- PASO D: GeneraciÃ³n de BibliografÃ­a ---")
        papers_with_doi = [p for p in papers if p.get('is_valid_doi')]
        if not papers_with_doi:
            print("  [OMITIDO] No se encontraron papers con DOI vÃ¡lido para generar bibliografÃ­a.")
            return

        print(f"  -> Usando metadatos de {len(papers_with_doi)} papers.")
        bib_entries = [self._paper_to_bibtex(p) for p in papers_with_doi]
        await self._save_bib("bibliografia_final.bib", "\n\n".join(bib_entries))
        print("âœ¨ [Ã‰XITO] BibliografÃ­a generada.")

    async def _search_hackernews(self) -> Dict[str, Any]:
        """
        Paso E: Busca en Hacker News para cada topic.
        MEJORA: Maneja explÃ­citamente errores de JSON y guarda la respuesta cruda.
        """
        print("\n--- PASO E: BÃºsqueda en Hacker News ---")
        if not self.hn_client:
            print("  [OMITIDO] El cliente de Hacker News no estÃ¡ disponible.")
            return {}

        all_results = {}
        for topic in self.topics:
            print(f"  -> Buscando en HN: '{topic}'...")
            raw_response_text = ""
            try:
                params = {"query": topic, "max_results": MAX_HN_RESULTS_PER_TOPIC}
                response = await self.hn_client.call_tool("getStories", params)
                
                stories = []
                # El problema puede estar aquÃ­: la respuesta puede ser un texto vacÃ­o o un error HTML
                raw_response_text = extract_text(response)
                if not raw_response_text.strip():
                     print(f"     [AVISO] Respuesta vacÃ­a del servidor para '{topic}'.")
                     all_results[topic] = []
                     continue

                # Intentamos decodificar el JSON
                response_json = json.loads(raw_response_text)
                
                # Asumimos que la respuesta es una lista de historias (o un dict con 'hits')
                story_items = response_json if isinstance(response_json, list) else response_json.get('hits', [])

                all_results[topic] = story_items
                print(f"     Encontrados {len(story_items)} resultados.")
            
            except json.JSONDecodeError as e:
                log_filename = f"hackernews_error_response_{slugify(topic)}.log"
                error_msg = f"     [FALLO] El servidor HN no devolviÃ³ un JSON vÃ¡lido para '{topic}'. Error: {e}"
                print(error_msg)
                await self._save_log(log_filename, f"{error_msg}\n\n--- RESPUESTA RECIBIDA ---\n{raw_response_text}")
                all_results[topic] = []
            except Exception as e:
                print(f"     [FALLO] Error inesperado buscando en HN para '{topic}': {e}")
                all_results[topic] = []
        
        print("âœ¨ [Ã‰XITO] BÃºsqueda en Hacker News completada.")
        return all_results

    async def _filter_with_llm(self, hn_results: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """Paso F: Usa un LLM para validar la relevancia de los resultados de Hacker News."""
        print("\n--- PASO F: Filtrado de HN con LLM ---")
        if not self.ai_client:
            print("  [OMITIDO] Cliente de IA no disponible.")
            return hn_results
        
        final_results = {}
        for topic, stories in hn_results.items():
            if not stories:
                final_results[topic] = []
                continue

            print(f"  -> Pidiendo al LLM que valide {len(stories)} historias para: '{topic}'...")
            titles_str = "\n".join([f"{i+1}. {s.get('title', 'N/A')}" for i, s in enumerate(stories)])
            
            prompt = (
                f"Identify which of the following Hacker News titles are relevant to the research topic: '{topic}'.\n\n"
                "A title is relevant if it discusses the topic directly, not if it just uses some of the same words in a different context.\n\n"
                f"Candidate Titles:\n{titles_str}\n\n"
                "Return a JSON object with a single key 'relevant_indices', a list of the numbers of relevant titles. Example: {\"relevant_indices\": [1, 4, 5]}.\n"
                "If none are relevant, return an empty list. Respond ONLY with the JSON object."
            )

            try:
                response_text = await self.ai_client.chat_completion(prompt)
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                response_json = json.loads(json_match.group(0)) if json_match else {}
                relevant_indices = response_json.get("relevant_indices", [])
                
                filtered = [stories[i-1] for i in relevant_indices if 0 < i <= len(stories)]
                final_results[topic] = filtered
                print(f"     LLM identificÃ³ {len(filtered)} historias relevantes.")
            except Exception as e:
                print(f"     [FALLO] Error con el LLM para '{topic}': {e}. Se mantienen resultados sin filtrar.")
                final_results[topic] = stories
        
        print("âœ¨ [Ã‰XITO] Filtrado con LLM completado.")
        return final_results

    async def run(self):
        """Ejecuta el flujo completo de investigaciÃ³n."""
        print(f"\n[{now_str()}] ðŸš€ Iniciando Asistente de InvestigaciÃ³nâ€¦")
        await self.setup_output_dirs()
        
        if not await self.connect_services():
            print("\nâŒ [ERROR] No se pudo conectar a los servicios necesarios. Abortando.")
            return

        try:
            # Flujo de Research Hub
            found_papers = await self._search_papers()
            if found_papers:
                await self._save_json("01_papers_encontrados.json", found_papers)
                await self._save_csv("01_papers_encontrados.csv", found_papers)
                await self._select_and_download_papers(found_papers)
                await self._generate_bibliography(found_papers)
            
            # Flujo de Hacker News
            hackernews_results = await self._search_hackernews()
            await self._save_json("04_hackernews_raw.json", hackernews_results)
            
            final_hn_results = await self._filter_with_llm(hackernews_results)
            await self._save_json("05_hackernews_filtrado_llm.json", final_hn_results)
            
            print(f"\nðŸŽ‰ [{now_str()}] Proceso completado con Ã©xito.")

        finally:
            print("\nðŸ Finalizando y cerrando conexionesâ€¦")
            if self.mcp_manager:
                await self.mcp_manager.close_all_clients()
                print("   [Ã‰XITO] Conexiones MCP cerradas.")

async def main():
    """Punto de entrada principal del script."""
    try:
        async with aiofiles.open("terminos.txt", "r", encoding="utf-8") as f:
            terminos = [t.strip() for t in (await f.read()).splitlines() if t.strip()]
        if not terminos:
            print("âš ï¸  'terminos.txt' estÃ¡ vacÃ­o. No hay nada que procesar.")
            return
    except FileNotFoundError:
        print("âŒ ERROR: No se encontrÃ³ 'terminos.txt'. Por favor, crea el archivo con los temas a investigar.")
        return
    
    topics = terminos[:MAX_TOPICS]
    print(f"âœ… Temas a investigar ({len(topics)}): {topics}")
    
    assistant = AdvancedResearchAssistant(topics)
    await assistant.run()

if __name__ == "__main__":
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())


---
File: /research_assistant.py
---

# research_assistant.py
# ============================================================
# Flujo de investigaciÃ³n con rust-research-mcp (MCP server)
# 1. Busca papers por temas para descubrir DOIs.
# 2. Descarga los papers seleccionados.
# 3. Vuelve a consultar cada DOI para obtener metadatos enriquecidos (autores, etc.).
# 4. Genera una bibliografÃ­a completa con los datos enriquecidos.
# ============================================================

import asyncio
import json
import os
import re
import csv
from datetime import datetime
from typing import Any, Iterable, List, Dict, Optional

from dotenv import load_dotenv
import aiofiles
import aiofiles.os

# MÃ³dulos del proyecto
from config_manager import ServerConfig
from mcp_client_manager import MCPClientManager, RemoteMCPClient

load_dotenv()

# ============================================================
# ðŸ”§ PARÃMETROS CONFIGURABLES
# ============================================================

def env_bool(name: str, default: bool) -> bool:
    val = os.getenv(name, str(default)).strip().lower()
    return val in ("1", "true", "t", "yes", "y", "on")

def env_int(name: str, default: int) -> int:
    val = os.getenv(name)
    return int(val) if val and val.isdigit() else default

# â€”â€”â€” Control de salidas â€”â€”â€”
SEPARATE_RUNS_IN_SUBFOLDER: bool = env_bool("SEPARATE_RUNS_IN_SUBFOLDER", True)
RUN_TAG: Optional[str] = os.getenv("RUN_TAG")

# â€”â€”â€” BÃºsqueda y filtrado â€”â€”â€”
MAX_TOPICS: int = env_int("MAX_TOPICS", 10)
MAX_RESULTS_PER_TOPIC: int = env_int("MAX_RESULTS_PER_TOPIC", 10)

# â€”â€”â€” Descarga de PDFs â€”â€”â€”
DOWNLOAD_ALL_PAPERS: bool = env_bool("DOWNLOAD_ALL_PAPERS", False)
SELECT_TOP_K: int = env_int("SELECT_TOP_K", 5)
MAX_PARALLEL_DOWNLOADS: int = env_int("MAX_PARALLEL_DOWNLOADS", 4)

# ============================================================
# Constantes y Utilidades
# ============================================================
DOI_REGEX = re.compile(r'^10\.\d{4,9}/.+$')

# Directorios de salida globales
SALIDAS_DIR, CSV_DIR, LOGS_DIR, BIB_DIR, JSON_DIR = "", "", "", "", ""
DOWNLOADS_DIR = ""

async def setup_output_dirs() -> None:
    """Configura los directorios de salida y descarga de forma asÃ­ncrona."""
    global SALIDAS_DIR, CSV_DIR, LOGS_DIR, BIB_DIR, JSON_DIR, DOWNLOADS_DIR
    
    DOWNLOADS_DIR = os.getenv("RESEARCH_PAPERS_DIR", os.path.join(os.getcwd(), "ResearchPapers"))
    
    base = "salidas"
    if SEPARATE_RUNS_IN_SUBFOLDER:
        tag = RUN_TAG or datetime.now().strftime("%Y%m%d_%H%M%S")
        base = os.path.join(base, tag)
    
    SALIDAS_DIR = base
    CSV_DIR, LOGS_DIR, BIB_DIR, JSON_DIR = (os.path.join(base, d) for d in ["csv", "logs", "bib", "json"])
    
    for d in (base, CSV_DIR, LOGS_DIR, BIB_DIR, JSON_DIR, DOWNLOADS_DIR):
        await aiofiles.os.makedirs(d, exist_ok=True)

def slugify(s: str) -> str:
    s = str(s).lower().strip()
    s = re.sub(r'[\s\W-]+', '-', s)
    s = s.strip('-')
    return s[:75] if len(s) > 75 else s

async def guardar_csv(nombre: str, rows: List[Dict[str, Any]]) -> None:
    path = os.path.join(CSV_DIR, nombre)
    if not rows: return
    async with aiofiles.open(path, "w", encoding="utf-8-sig", newline="") as f:
        fields = list(rows[0].keys())
        await f.write(",".join(fields) + "\n")
        for row in rows:
            values = [f'"{str(row.get(k, "")).replace("\"", "\"\"")}"' for k in fields]
            await f.write(",".join(values) + "\n")
    print(f"ðŸ’¾ CSV guardado: {path} ({len(rows)} filas)")

async def guardar_json(nombre: str, data: Any) -> None:
    path = os.path.join(JSON_DIR, nombre)
    async with aiofiles.open(path, "w", encoding="utf-8") as f:
        await f.write(json.dumps(data, ensure_ascii=False, indent=2))
    print(f"ðŸ’¾ JSON guardado: {path}")

async def guardar_bib(nombre: str, contenido: str) -> None:
    path = os.path.join(BIB_DIR, nombre)
    async with aiofiles.open(path, "w", encoding="utf-8") as f:
        await f.write(contenido)
    print(f"ðŸ“š BibliografÃ­a guardada: {path}")

def extract_text(blobs: Optional[Iterable[Any]]) -> str:
    parts: List[str] = []
    if not blobs: return ""
    for b in blobs:
        if hasattr(b, "text"): parts.append(getattr(b, "text", "") or "")
        elif isinstance(b, str): parts.append(b)
    return "\n".join(parts).strip()

def parse_text_response_to_papers(raw_text: str, topic: str) -> List[Dict[str, Any]]:
    """Parsea la respuesta de texto formateada del servidor Rust para la bÃºsqueda."""
    papers = []
    content = raw_text.split('\n\n', 1)[-1]
    paper_blocks = re.split(r'\n\n(?=\d+\.\s)', content)

    for block in paper_blocks:
        if not block.strip(): continue
        paper_data = {"topic": topic, "title": None, "doi": None, "source": None, "year": None}
        
        title_match = re.search(r'^\d+\.\s*(.*?)\s*\(Relevance:', block)
        if title_match: paper_data['title'] = title_match.group(1).strip()
        
        doi_match = re.search(r'ðŸ“–\s*DOI:\s*(.*)', block)
        if doi_match:
            doi_str = doi_match.group(1).strip()
            paper_data['doi'] = doi_str if doi_str and " " not in doi_str else None

        source_match = re.search(r'ðŸ”\s*Source:\s*(.*)', block)
        if source_match: paper_data['source'] = source_match.group(1).strip()
            
        year_match = re.search(r'ðŸ“…\s*Year:\s*(\d{4})', block)
        if year_match: paper_data['year'] = int(year_match.group(1).strip())
            
        if paper_data.get('title'):
            paper_data['url'] = f"https://doi.org/{paper_data['doi']}" if paper_data.get('doi') else None
            paper_data['is_valid_doi'] = bool(paper_data['doi'] and DOI_REGEX.match(paper_data['doi']))
            papers.append(paper_data)
            
    return papers

async def step_a_search_papers(rh_client: RemoteMCPClient, topics: List[str]) -> List[Dict[str, Any]]:
    """Busca papers para cada topic y los devuelve combinados y deduplicados."""
    all_papers = []
    for topic in topics:
        print(f"\n--- ðŸ”Ž Buscando topic: '{topic}' ---")
        try:
            res = await rh_client.call_tool("search_papers", {"query": topic, "limit": MAX_RESULTS_PER_TOPIC})
            raw_text_content = extract_text(res)
            papers = parse_text_response_to_papers(raw_text_content, topic)
            all_papers.extend(papers)
            print(f"  -> Encontrados {len(papers)} resultados para '{topic}'.")
        except Exception as e:
            print(f"  âœ— Error buscando topic '{topic}': {e}")
    
    seen_dois = set()
    unique_papers = []
    for p in all_papers:
        doi = p.get("doi")
        if doi and doi not in seen_dois:
            seen_dois.add(doi)
            unique_papers.append(p)
        elif not doi:
             unique_papers.append(p)
    
    print(f"\nâœ¨ Total de papers Ãºnicos encontrados: {len(unique_papers)}")
    return unique_papers

async def step_b_select_papers(papers: List[Dict[str, Any]]) -> List[str]:
    """Filtra y selecciona los DOIs vÃ¡lidos para descargar."""
    print("\n--- ðŸ§  Seleccionando papers para descarga ---")
    
    selected_dois = [p['doi'] for p in papers if p.get('is_valid_doi')]
    
    if DOWNLOAD_ALL_PAPERS:
        print(f"  -> SelecciÃ³n: TODOS ({len(selected_dois)} papers)")
        return selected_dois
    else:
        print(f"  -> SelecciÃ³n: TOP {SELECT_TOP_K} (usando los primeros encontrados)")
        return selected_dois[:SELECT_TOP_K]

async def step_c_download_papers(rh_client: RemoteMCPClient, dois: List[str], papers_metadata: List[Dict]) -> Dict[str, Dict]:
    """Descarga los papers seleccionados en paralelo."""
    if not dois: return {}
    print(f"\n--- ðŸ“¥ Descargando {len(dois)} papers en paralelo (max {MAX_PARALLEL_DOWNLOADS}) ---")
    semaphore = asyncio.Semaphore(MAX_PARALLEL_DOWNLOADS)
    
    doi_map = {p['doi']: p for p in papers_metadata if p.get('doi')}

    async def _download_one(doi: str):
        async with semaphore:
            title = doi_map.get(doi, {}).get('title', 'untitled')
            filename = f"{slugify(title)}_{slugify(doi)}.pdf"
            try:
                res = await rh_client.call_tool("download_paper", {"doi": doi, "filename": filename, "directory": DOWNLOADS_DIR})
                raw_response_text = extract_text(res)
                
                success_match = re.search(r'File:\s*(.*?)\n', raw_response_text)
                
                if ("Download successful!" in raw_response_text or "File already exists" in raw_response_text) and success_match:
                    file_path = success_match.group(1).strip()
                    print(f"  âœ“ Descargado (o ya existÃ­a): {doi}")
                    return doi, {"status": "ok", "path": file_path, "title": title}
                else:
                    print(f"  âœ— Fallo en descarga: {doi}")
                    return doi, {"status": "failed", "reason": raw_response_text.strip()}

            except Exception as e:
                print(f"  âœ— Error grave durante la descarga de {doi}: {e}")
                return doi, {"status": "error", "reason": str(e)}

    tasks = [_download_one(doi) for doi in dois]
    results = await asyncio.gather(*tasks)
    return {doi: result for doi, result in results}

def paper_dict_to_bibtex_entry(paper: Dict[str, Any]) -> str:
    """Convierte un diccionario de paper enriquecido en una entrada BibTeX string."""
    # Crea una clave Ãºnica a partir del primer autor y aÃ±o
    author_lastname = "unknown"
    if paper.get("authors"):
        try:
            author_lastname = slugify(paper["authors"].split(',')[0].split(' ')[-1])
        except: # noqa
            pass # Mantener 'unknown' si el formato del autor es inesperado
    
    year_str = str(paper.get('year', 'nodate'))
    key = f"{author_lastname}{year_str}"

    entry = f"@article{{{key},\n"
    if paper.get('title'):
        entry += f"  title     = {{{{{paper['title']}}}}},\n"
    if paper.get('authors'):
        entry += f"  author    = {{{paper['authors']}}},\n"
    if paper.get('year'):
        entry += f"  year      = {{{paper['year']}}},\n"
    if paper.get('journal'):
        entry += f"  journal   = {{{paper['journal']}}},\n"
    if paper.get('doi'):
        entry += f"  doi       = {{{paper['doi']}}},\n"
    entry += "}"
    return entry

async def step_d_generate_bibliography(rh_client: RemoteMCPClient, papers: List[Dict[str, Any]]) -> str:
    """Enriquece los metadatos de los papers y genera una bibliografÃ­a completa."""
    papers_with_doi = [p for p in papers if p.get('is_valid_doi')]
    if not papers_with_doi:
        return "% No se encontraron papers con DOI vÃ¡lido para generar la bibliografÃ­a."

    print(f"\n--- ðŸ“š Generando BibliografÃ­a ---")
    print(f"  -> Enriqueciendo metadatos para {len(papers_with_doi)} papers...")

    enriched_papers = []
    for paper in papers_with_doi:
        try:
            print(f"     - Obteniendo detalles para DOI: {paper['doi']}")
            # Llama a search_papers con el DOI para obtener metadatos ricos
            res = await rh_client.call_tool("search_papers", {"query": paper['doi'], "limit": 1})
            raw_text = extract_text(res)
            
            # Parsea la respuesta rica (puede tener mÃ¡s campos)
            # Usamos un parser simple aquÃ­, asumiendo un formato similar
            enriched_data = paper.copy() # Empezamos con los datos que ya tenemos
            
            authors_match = re.search(r'ðŸ‘¤\s*Authors:\s*(.*)', raw_text)
            if authors_match:
                enriched_data['authors'] = authors_match.group(1).strip()

            journal_match = re.search(r' L Journal:\s*(.*)', raw_text)
            if journal_match:
                enriched_data['journal'] = journal_match.group(1).strip()
            
            enriched_papers.append(enriched_data)
        except Exception as e:
            print(f"  âœ— Error enriqueciendo {paper.get('doi')}: {e}. Usando datos bÃ¡sicos.")
            enriched_papers.append(paper) # AÃ±adir con datos bÃ¡sicos si falla

    print(f"  -> Creando entradas BibTeX...")
    bib_entries = [paper_dict_to_bibtex_entry(p) for p in enriched_papers]
        
    print("  -> BibliografÃ­a generada con Ã©xito.")
    return "\n\n".join(bib_entries)

def construir_topics_desde_terminos(terminos: List[str], max_topics: int) -> List[str]:
    if not terminos: return ["model context protocol"]
    topics_a_buscar = terminos[:max_topics]
    print(f"âœ… Construidos {len(topics_a_buscar)} topics para la bÃºsqueda: {topics_a_buscar}")
    return topics_a_buscar

async def main():
    """Flujo principal que orquesta la investigaciÃ³n."""
    await setup_output_dirs()
    
    try:
        async with aiofiles.open("terminos.txt", "r", encoding="utf-8") as f:
            contenido = await f.read()
        terminos = [t.strip() for t in contenido.splitlines() if t.strip()]
        if not terminos:
            print("âš ï¸  El archivo 'terminos.txt' estÃ¡ vacÃ­o. No hay nada que procesar.")
            return
    except FileNotFoundError:
        print("âŒ ERROR: El archivo 'terminos.txt' no se encontrÃ³.")
        return

    topics = construir_topics_desde_terminos(terminos, MAX_TOPICS)
    
    print(f"\nIniciando Asistente de InvestigaciÃ³n...")
    print(f"ðŸ“‚ Salidas en: {SALIDAS_DIR}")
    print(f"ðŸ“¥ PDFs se guardarÃ¡n en: {DOWNLOADS_DIR}")
    
    server_configs = ServerConfig.get_server_configs()
    mcp_manager = MCPClientManager(server_configs)
    
    try:
        print("\nðŸ”— Conectando al servidor de Research Hub...")
        await mcp_manager._connect_single_server("research_hub", server_configs["research_hub"])
        rh_client = mcp_manager.get_client("research_hub")
        if not rh_client:
            print("âŒ No se pudo conectar al servidor de Research Hub. Abortando.")
            return
        print("âœ… Conectado.")

        found_papers = await step_a_search_papers(rh_client, topics)
        if not found_papers:
            print("\nâš ï¸ No se encontraron papers en ninguna de las bÃºsquedas. Terminando.")
            return
        
        await guardar_json("00_resultados_completos.json", found_papers)
        await guardar_csv("01_papers_encontrados.csv", found_papers)
        
        selected_dois = await step_b_select_papers(found_papers)
        if not selected_dois:
            print("\nâš ï¸ No se seleccionaron papers con DOI vÃ¡lido para descargar.")
        else:
            await guardar_json("02_dois_seleccionados.json", {"dois": selected_dois})
            download_manifest = await step_c_download_papers(rh_client, selected_dois, found_papers)
            await guardar_json("03_manifiesto_descarga.json", download_manifest)

        bib_content = await step_d_generate_bibliography(rh_client, found_papers)
        await guardar_bib("bibliografia_final.bib", bib_content)
        
        print("\nðŸŽ‰ Proceso completado con Ã©xito.")

    finally:
        print("\nðŸ Finalizando y cerrando conexiones...")
        await mcp_manager.close_all_clients()

if __name__ == "__main__":
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())



---
File: /README.md
---

# AI Trend Research Engine

Un sistema automatizado y modular para la investigaciÃ³n de tendencias en Inteligencia Artificial. Recopila, procesa y analiza datos de mÃºltiples fuentes para descubrir *insights*, generar informes y mantener un ciclo de vida de palabras clave en constante evoluciÃ³n.

![Python Version](https://img.shields.io/badge/python-3.9%2B-blue)![License](https://img.shields.io/badge/license-MIT-green)![Status](https://img.shields.io/badge/status-activo-brightgreen)![Built with](https://img.shields.io/badge/built%20with-asyncio-purple)

<p align="center">
  <strong>Languages:</strong>
  <br>
  <a href="#-english">English</a> | <a href="#-espaÃ±ol">EspaÃ±ol</a> | <a href="#-catalÃ ">CatalÃ </a>
</p>

---

<a name="-english"></a>
## ðŸ‡¬ðŸ‡§ English

<details>
<summary><strong>Table of Contents</strong></summary>

- [ðŸš€ Key Features](#-key-features-1)
- [ðŸ›ï¸ System Architecture](#ï¸-system-architecture)
- [ðŸ’» Tech Stack](#-tech-stack)
- [ðŸ› ï¸ Installation and Setup](#ï¸-installation-and-setup-1)
- [â–¶ï¸ Usage](#ï¸-usage-1)
- [ðŸ¤ Contributing](#-contributing-1)
- [ðŸ“„ License](#-license-1)

</details>

### AI Trend Research Engine

An automated and modular system for researching trends in Artificial Intelligence. It collects and processes data from multiple platforms (GitHub, YouTube, ArXiv, etc.), uses large language models (LLMs) to extract insights, discover new keywords, and generates comprehensive reports in multiple formats (JSON, Notion, Supabase).

**Note:** This project was developed by modifying the [EmpowerHerDev/ai-trend-research-system](https://github.com/EmpowerHerDev/ai-trend-research-system) project and integrating the custom research server from [Ladvien/research_hub_mcp](https://github.com/Ladvien/research_hub_mcp). The result is a flexible and modular system that now supports multiple large language model (LLM) providers, including **OpenAI (GPT), Google (Gemini), Anthropic (Claude), Groq, and Ollama**. This work is also part of a personal learning journey in the field of artificial intelligence agent development.

### ðŸš€ Key Features

-   **Multi-Platform Research**: Gathers data from YouTube, GitHub, web searches, ArXiv, HackerNews, and a custom paper research engine.
-   **Multi-LLM Support**: Seamlessly switch between different LLM providers like OpenAI, Gemini, Groq, Anthropic, and local models with Ollama.
-   **AI-Powered Analysis**: Uses LLMs for advanced tasks such as intelligent keyword extraction, dynamic recommendation generation, and multilingual query translation.
-   **Asynchronous Architecture**: Built with `asyncio` for high concurrency and efficiency in handling network I/O and managing multiple data source servers.
-   **Automated Reporting**: Automatically creates detailed reports in **JSON** (for local archiving), **Notion** (for collaborative workspaces), and **Supabase** (for long-term data persistence).
-   **Dynamic Keyword Lifecycle**: Implements a full lifecycle for keywords: discovery, scoring, active research, and history tracking, all managed through simple JSON files.
-   **Decoupled & Standardized**: Uses the **Model Context Protocol (MCP)** to communicate with each platform's server, ensuring a standardized, modular, and easily extensible interface.
-   **Highly Configurable**: All settings (API keys, AI model selection, file paths) are managed through a single `.env` file for easy portability and security.

### ðŸ›ï¸ System Architecture

The system is designed with a modular architecture where a central orchestrator manages various specialized components. The communication with external data sources is standardized through MCP servers.

```mermaid
graph TD
    subgraph "Core Engine"
        A[ai_trend_researcher.py] --> B{KeywordManager};
        A --> C{MCPClientManager};
        A --> D{DataProcessor};
        A --> E{ReportManager};
    end

    subgraph "AI Integration"
        F[AIClientManager] -- Manages --> G[OpenAI, Gemini, Groq, ...];
        D -- Uses --> F;
    end

    subgraph "Data Sources (via MCP)"
        C -- Connects to --> H[YouTube Server];
        C -- Connects to --> I[GitHub Server];
        C -- Connects to --> J[ArXiv Server];
        C -- Connects to --> K[Web/HackerNews...];
        C -- Connects to --> L[ResearchHub Server];
    end

    subgraph "Output Reports"
        E -- Generates --> M[JSON Files];
        E -- Generates --> N[Notion Pages];
        E -- Generates --> O[Supabase Records];
    end

    B --> A;
```

-   **Orchestrator (`ai_trend_researcher.py`)**: The main script that coordinates the entire research workflow, from loading keywords to generating final reports.
-   **Configuration (`config_manager.py`)**: Loads and validates all environment variables and server settings.
-   **Connectivity (`mcp_client_manager.py`)**: Manages the lifecycle (start, connect, call, stop) of all MCP servers for the data platforms.
-   **AI Factory (`ai_client_manager.py`)**: Provides a unified interface to interact with various LLM providers.
-   **Platform Logic (`platform_handlers.py`)**: Contains the specific logic for querying each platform and standardizing its response.
-   **Analysis (`data_processor.py`)**: Uses LLMs and heuristics to extract new keywords, score them, and generate actionable recommendations.
-   **Reporting (`report_generator.py`)**: Creates the final reports in all supported formats.
-   **Keyword Database (`keyword_manager.py`)**: Manages the state of keywords across research cycles.

### ðŸ’» Tech Stack

-   **Backend**: Python 3.9+
-   **Concurrency**: `asyncio`
-   **AI Providers**: OpenAI, Google Gemini, Anthropic Claude, Groq, Ollama
-   **Tool Protocol**: Model Context Protocol (MCP)
-   **Dependencies**: `python-dotenv`, `aiofiles`, provider-specific SDKs (e.g., `openai`, `google-generativeai`)
-   **Prerequisites**: Node.js & npm (to run community MCP servers via `npx`), Rust (optional, to compile the `research_hub` server)

### ðŸ› ï¸ Installation and Setup

#### 1. Prerequisites

-   **Python 3.9+**
-   **Node.js and npm** (required for `npx`, which runs the community's MCP servers).
-   **Git**
-   (Optional) **Rust Compiler**, if you want to compile the `research_hub` executable from the source code. Otherwise, you can use a pre-compiled binary.

#### 2. Clone the Repository

```bash
git clone https://github.com/your-user/your-repository.git
cd your-repository
```

#### 3. Install Python Dependencies

It is highly recommended to use a virtual environment.

```bash
python -m venv venv
# On macOS/Linux:
source venv/bin/activate
# On Windows:
venv\Scripts\activate

pip install -r requirements.txt
```

#### 4. Configure Environment Variables

This is the most critical step.

1.  Create your environment file from the example:
    ```bash
    cp .env.example .env
    ```
2.  Open the `.env` file and fill in your API keys and paths. You only need to fill in the keys for the services you intend to use.

    ```env
    # .env

    # --- AI Provider Configuration ---
    # Choose one: "openai", "gemini", "groq", "anthropic", "ollama"
    AI_PROVIDER="openai"

    # Fill in the API keys for the providers you might use.
    OPENAI_API_KEY="sk-..."
    GOOGLE_API_KEY="AIzaSy..."
    GROQ_API_KEY="gsk_..."
    ANTHROPIC_API_KEY="sk-ant-..."
    # No key needed for Ollama

    # --- (Optional) Specific AI Models ---
    AI_MODEL_OPENAI="gpt-4o"
    AI_MODEL_GEMINI="gemini-1.5-flash"
    # ... and so on for other providers

    # --- API Keys for Data Sources ---
    YOUTUBE_API_KEY=AIzaSy...
    GITHUB_PERSONAL_ACCESS_TOKEN=ghp_...

    # --- API Keys for Report Outputs ---
    NOTION_API_KEY=secret_...
    NOTION_PARENT_PAGE_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
    SUPABASE_ACCESS_TOKEN=eyJhbGciOi...

    # --- System Path Configuration (IMPORTANT!) ---
    # MUST be absolute paths.
    RESEARCH_PAPERS_DIR="/path/on/your/machine/research-papers"
    RESEARCH_HUB_EXECUTABLE="/path/on/your/machine/rust-research-mcp"
    ```

#### 5. Set Up the Database (Supabase)

If you plan to use the Supabase integration, make sure your database table matches the schema defined in `supabase_schema.sql`. You can run this script in the SQL editor of your Supabase project.

### â–¶ï¸ Usage

The project has two main execution workflows.

#### 1. Daily Trend Research

This is the primary workflow. It runs research on all enabled platforms using the keywords in `keywords/active.json`, analyzes the data, discovers new keywords, and generates reports.

```bash
python ai_trend_researcher.py
```
The script will log its progress to the console. When it finishes, you will find:
- A JSON report in the `reports/` directory.
- A new page in your specified Notion workspace (if configured).
- A new record in your Supabase table (if configured).

#### 2. Deep Dive Research (`research_assistant`)

This advanced script uses the custom `research_hub` server to perform in-depth academic research. It searches for papers, downloads PDFs, and generates bibliographies.

1. Ensure the `RESEARCH_HUB_EXECUTABLE` path in your `.env` is correct.
2. Add search terms to the `terminos.txt` file (one per line).

```bash
python research_assistant.py
```
The results (CSVs, JSONs, BibTeX files, and logs) will be saved in a timestamped subdirectory within `salidas/` to keep each run organized.

### ðŸ¤ Contributing

Contributions are welcome! If you have ideas for improving the project, new platforms to integrate, or find any bugs, please open an issue or submit a pull request.

### ðŸ“„ License

This project is licensed under the MIT License. See the `LICENSE` file for more details.

---

<a name="-espaÃ±ol"></a>
## ðŸ‡ªðŸ‡¸ EspaÃ±ol

<details>
<summary><strong>Tabla de Contenidos</strong></summary>

- [ðŸš€ CaracterÃ­sticas Principales](#-caracterÃ­sticas-principales-1)
- [ðŸ›ï¸ Arquitectura del Sistema](#ï¸-arquitectura-del-sistema)
- [ðŸ’» Stack TecnolÃ³gico](#-stack-tecnolÃ³gico)
- [ðŸ› ï¸ InstalaciÃ³n y ConfiguraciÃ³n](#ï¸-instalaciÃ³n-y-configuraciÃ³n-1)
- [â–¶ï¸ Uso](#ï¸-uso-1)
- [ðŸ¤ Contribuciones](#-contribuciones-1)
- [ðŸ“„ Licencia](#-licencia-1)

</details>

### AI Trend Research Engine

Un sistema automatizado y modular para la investigaciÃ³n de tendencias en Inteligencia Artificial. Recopila y procesa datos de mÃºltiples plataformas (GitHub, YouTube, ArXiv, etc.), utiliza modelos de lenguaje (LLMs) para extraer *insights*, descubrir nuevas palabras clave y genera informes completos en mÃºltiples formatos (JSON, Notion, Supabase).

**Nota:** Este proyecto ha sido desarrollado modificando el proyecto [EmpowerHerDev/ai-trend-research-system](https://github.com/EmpowerHerDev/ai-trend-research-system) e integrando el servidor de investigaciÃ³n personalizado de [Ladvien/research_hub_mcp](https://github.com/Ladvien/research_hub_mcp). El resultado es un sistema flexible y modular que ahora es compatible con mÃºltiples proveedores de modelos de lenguaje grande (LLM), incluyendo **OpenAI (GPT), Google (Gemini), Anthropic (Claude), Groq y Ollama**. Este trabajo tambiÃ©n forma parte de un proceso de aprendizaje personal en el campo del desarrollo de agentes de inteligencia artificial.

### ðŸš€ CaracterÃ­sticas Principales

-   **InvestigaciÃ³n Multiplataforma**: Recopila datos de YouTube, GitHub, bÃºsquedas web, ArXiv, HackerNews y un motor de investigaciÃ³n de *papers* personalizado.
-   **Soporte Multi-LLM**: Cambia fÃ¡cilmente entre diferentes proveedores de LLM como OpenAI, Gemini, Groq, Anthropic y modelos locales con Ollama.
-   **AnÃ¡lisis con IA**: Utiliza LLMs para tareas avanzadas como la extracciÃ³n inteligente de palabras clave, la generaciÃ³n de recomendaciones dinÃ¡micas y la traducciÃ³n de consultas multilingÃ¼es.
-   **Arquitectura AsÃ­ncrona**: Construido con `asyncio` para una alta concurrencia y eficiencia en el manejo de I/O de red y la gestiÃ³n de mÃºltiples servidores de datos.
-   **Informes Automatizados**: Crea automÃ¡ticamente informes detallados en **JSON** (para archivo local), **Notion** (para espacios de trabajo colaborativos) y **Supabase** (para persistencia de datos a largo plazo).
-   **Ciclo de Vida DinÃ¡mico de Keywords**: Implementa un ciclo de vida completo para las palabras clave: descubrimiento, puntuaciÃ³n, investigaciÃ³n activa y seguimiento histÃ³rico, todo gestionado a travÃ©s de simples archivos JSON.
-   **Desacoplado y Estandarizado**: Utiliza el **Model Context Protocol (MCP)** para comunicarse con el servidor de cada plataforma, asegurando una interfaz estandarizada, modular y fÃ¡cilmente extensible.
-   **Altamente Configurable**: Toda la configuraciÃ³n (API keys, selecciÃ³n de modelos de IA, rutas de archivos) se gestiona a travÃ©s de un Ãºnico archivo `.env` para facilitar la portabilidad y seguridad.

### ðŸ›ï¸ Arquitectura del Sistema

El sistema estÃ¡ diseÃ±ado con una arquitectura modular donde un orquestador central gestiona varios componentes especializados. La comunicaciÃ³n con las fuentes de datos externas se estandariza a travÃ©s de servidores MCP.

```mermaid
graph TD
    subgraph "Motor Principal"
        A[ai_trend_researcher.py] --> B{KeywordManager};
        A --> C{MCPClientManager};
        A --> D{DataProcessor};
        A --> E{ReportManager};
    end

    subgraph "IntegraciÃ³n IA"
        F[AIClientManager] -- Gestiona --> G[OpenAI, Gemini, Groq, ...];
        D -- Usa --> F;
    end

    subgraph "Fuentes de Datos (vÃ­a MCP)"
        C -- Conecta a --> H[Servidor YouTube];
        C -- Conecta a --> I[Servidor GitHub];
        C -- Conecta a --> J[Servidor ArXiv];
        C -- Conecta a --> K[Web/HackerNews...];
        C -- Conecta a --> L[Servidor ResearchHub];
    end

    subgraph "Informes de Salida"
        E -- Genera --> M[Archivos JSON];
        E -- Genera --> N[PÃ¡ginas en Notion];
        E -- Genera --> O[Registros en Supabase];
    end

    B --> A;
```

-   **Orquestador (`ai_trend_researcher.py`)**: El script principal que coordina todo el flujo de investigaciÃ³n, desde la carga de keywords hasta la generaciÃ³n de informes finales.
-   **ConfiguraciÃ³n (`config_manager.py`)**: Carga y valida todas las variables de entorno y configuraciones de los servidores.
-   **Conectividad (`mcp_client_manager.py`)**: Gestiona el ciclo de vida (inicio, conexiÃ³n, llamada, cierre) de todos los servidores MCP para las plataformas de datos.
-   **FÃ¡brica de IA (`ai_client_manager.py`)**: Proporciona una interfaz unificada para interactuar con diversos proveedores de LLM.
-   **LÃ³gica de Plataformas (`platform_handlers.py`)**: Contiene la lÃ³gica especÃ­fica para consultar cada plataforma y estandarizar su respuesta.
-   **AnÃ¡lisis (`data_processor.py`)**: Utiliza LLMs y heurÃ­sticas para extraer nuevas palabras clave, puntuarlas y generar recomendaciones accionables.
-   **Informes (`report_generator.py`)**: Crea los informes finales en todos los formatos soportados.
-   **Base de Datos de Keywords (`keyword_manager.py`)**: Administra el estado de las palabras clave a travÃ©s de los ciclos de investigaciÃ³n.

### ðŸ’» Stack TecnolÃ³gico

-   **Backend**: Python 3.9+
-   **Concurrencia**: `asyncio`
-   **Proveedores IA**: OpenAI, Google Gemini, Anthropic Claude, Groq, Ollama
-   **Protocolo de Herramientas**: Model Context Protocol (MCP)
-   **Dependencias**: `python-dotenv`, `aiofiles`, SDKs de proveedores (ej. `openai`, `google-generativeai`)
-   **Prerrequisitos**: Node.js & npm (para ejecutar servidores MCP de la comunidad vÃ­a `npx`), Rust (opcional, para compilar el servidor `research_hub`)

### ðŸ› ï¸ InstalaciÃ³n y ConfiguraciÃ³n

#### 1. Prerrequisitos

-   **Python 3.9+**
-   **Node.js y npm** (necesario para `npx`, que ejecuta los servidores MCP de la comunidad).
-   **Git**
-   (Opcional) **Compilador de Rust**, si deseas compilar el ejecutable de `research_hub` desde el cÃ³digo fuente. Si no, puedes usar un binario precompilado.

#### 2. Clonar el Repositorio

```bash
git clone https://github.com/tu-usuario/tu-repositorio.git
cd tu-repositorio
```

#### 3. Instalar Dependencias de Python

Se recomienda encarecidamente utilizar un entorno virtual.

```bash
python -m venv venv
# En macOS/Linux:
source venv/bin/activate
# En Windows:
venv\Scripts\activate

pip install -r requirements.txt
```

#### 4. Configurar las Variables de Entorno

Este es el paso mÃ¡s crÃ­tico.

1.  Crea tu archivo de entorno a partir del ejemplo:
    ```bash
    cp .env.example .env
    ```
2.  Abre el archivo `.env` y rellena tus claves de API y rutas. Solo necesitas rellenar las claves para los servicios que vayas a utilizar.

    ```env
    # .env

    # --- ConfiguraciÃ³n del Proveedor de IA ---
    # Elige uno: "openai", "gemini", "groq", "anthropic", "ollama"
    AI_PROVIDER="openai"

    # Rellena las claves de API para los proveedores que puedas usar.
    OPENAI_API_KEY="sk-..."
    GOOGLE_API_KEY="AIzaSy..."
    GROQ_API_KEY="gsk_..."
    ANTHROPIC_API_KEY="sk-ant-..."
    # No se necesita clave para Ollama

    # --- (Opcional) Modelos EspecÃ­ficos de IA ---
    AI_MODEL_OPENAI="gpt-4o"
    AI_MODEL_GEMINI="gemini-1.5-flash"
    # ... etc. para otros proveedores

    # --- Claves de API para Fuentes de Datos ---
    YOUTUBE_API_KEY=AIzaSy...
    GITHUB_PERSONAL_ACCESS_TOKEN=ghp_...

    # --- Claves de API para Salidas de Informes ---
    NOTION_API_KEY=secret_...
    NOTION_PARENT_PAGE_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
    SUPABASE_ACCESS_TOKEN=eyJhbGciOi...

    # --- ConfiguraciÃ³n de Rutas del Sistema (Â¡IMPORTANTE!) ---
    # DEBEN ser rutas absolutas.
    RESEARCH_PAPERS_DIR="/ruta/en/tu/maquina/research-papers"
    RESEARCH_HUB_EXECUTABLE="/ruta/en/tu/maquina/rust-research-mcp"
    ```

#### 5. Configurar la Base de Datos (Supabase)

Si planeas usar la integraciÃ³n con Supabase, asegÃºrate de que tu tabla en la base de datos coincida con el esquema definido en `supabase_schema.sql`. Puedes ejecutar ese script en el editor SQL de tu proyecto de Supabase.

### â–¶ï¸ Uso

El proyecto tiene dos flujos de ejecuciÃ³n principales.

#### 1. InvestigaciÃ³n Diaria de Tendencias

Este es el flujo de trabajo principal. Ejecuta la investigaciÃ³n en todas las plataformas habilitadas usando las palabras clave de `keywords/active.json`, analiza los datos, descubre nuevas keywords y genera informes.

```bash
python ai_trend_researcher.py
```
El script registrarÃ¡ su progreso en la consola. Cuando finalice, encontrarÃ¡s:
- Un informe JSON en el directorio `reports/`.
- Una nueva pÃ¡gina en tu espacio de trabajo de Notion (si estÃ¡ configurado).
- Un nuevo registro en tu tabla de Supabase (si estÃ¡ configurado).

#### 2. InmersiÃ³n Profunda de InvestigaciÃ³n (`research_assistant`)

Este script avanzado utiliza el servidor personalizado `research_hub` para realizar investigaciones acadÃ©micas en profundidad. Busca *papers*, descarga los PDF y genera bibliografÃ­as.

1. AsegÃºrate de que la ruta `RESEARCH_HUB_EXECUTABLE` en tu `.env` sea correcta.
2. AÃ±ade tÃ©rminos de bÃºsqueda al archivo `terminos.txt` (uno por lÃ­nea).

```bash
python research_assistant.py
```
Los resultados (CSVs, JSONs, archivos BibTeX y logs) se guardarÃ¡n en un subdirectorio con marca de tiempo dentro de `salidas/` para mantener cada ejecuciÃ³n organizada.

### ðŸ¤ Contribuciones

Â¡Las contribuciones son bienvenidas! Si tienes ideas para mejorar el proyecto, nuevas plataformas para integrar o encuentras algÃºn error, por favor abre un *issue* o envÃ­a un *pull request*.

### ðŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Consulta el archivo `LICENSE` para mÃ¡s detalles.

---

<a name="-catalÃ "></a>
## CAT CatalÃ 

<details>
<summary><strong>Taula de Continguts</strong></summary>

- [ðŸš€ CaracterÃ­stiques Principals](#-caracterÃ­stiques-principals-2)
- [ðŸ›ï¸ Arquitectura del Sistema](#ï¸-arquitectura-del-sistema-1)
- [ðŸ’» Stack TecnolÃ²gic](#-stack-tecnolÃ²gic-1)
- [ðŸ› ï¸ InstalÂ·laciÃ³ i ConfiguraciÃ³](#ï¸-installaciÃ³-i-configuraciÃ³-2)
- [â–¶ï¸ Ãšs](#ï¸-Ãºs-2)
- [ðŸ¤ Contribucions](#-contribucions-2)
- [ðŸ“„ LlicÃ¨ncia](#-llicÃ¨ncia-2)

</details>

### AI Trend Research Engine

Un sistema automatitzat i modular per a la investigaciÃ³ de tendÃ¨ncies en IntelÂ·ligÃ¨ncia Artificial. Recopila i processa dades de mÃºltiples plataformes (GitHub, YouTube, ArXiv, etc.), utilitza models de llenguatge (LLMs) per extreure *insights*, descobrir noves paraules clau i genera informes complets en mÃºltiples formats (JSON, Notion, Supabase).

**Nota:** Aquest projecte ha estat desenvolupat modificant el projecte [EmpowerHerDev/ai-trend-research-system](https://github.com/EmpowerHerDev/ai-trend-research-system) i integrant el servidor de recerca personalitzat de [Ladvien/research_hub_mcp](https://github.com/Ladvien/research_hub_mcp). El resultat Ã©s un sistema flexible i modular que ara Ã©s compatible amb mÃºltiples proveÃ¯dors de models de llenguatge grans (LLM), incloent-hi **OpenAI (GPT), Google (Gemini), Anthropic (Claude), Groq i Ollama**. Aquest treball tambiÃ©n forma part d'un procÃ©s d'aprenentatge personal en el camp del desenvolupament d'agents d'intelÂ·ligÃ¨ncia artificial.

### ðŸš€ CaracterÃ­stiques Principals

-   **Recerca Multiplataforma**: Recopila dades de YouTube, GitHub, cerques web, ArXiv, HackerNews i un motor de recerca de *papers* personalitzat.
-   **Suport Multi-LLM**: Canvia fÃ cilment entre diferents proveÃ¯dors de LLM com OpenAI, Gemini, Groq, Anthropic i models locals amb Ollama.
-   **AnÃ lisi amb IA**: Utilitza LLMs per a tasques avanÃ§ades com l'extracciÃ³ intelÂ·ligent de paraules clau, la generaciÃ³ de recomanacions dinÃ miques i la traducciÃ³ de consultes multilingÃ¼es.
-   **Arquitectura AsÃ­ncrona**: ConstruÃ¯t amb `asyncio` per a una alta concurrÃ¨ncia i eficiÃ¨ncia en la gestiÃ³ d'I/O de xarxa i la gestiÃ³ de mÃºltiples servidors de dades.
-   **Informes Automatitzats**: Crea automÃ ticament informes detallats en **JSON** (per a arxiu local), **Notion** (per a espais de treball colÂ·laboratius) i **Supabase** (per a persistÃ¨ncia de dades a llarg termini).
-   **Cicle de Vida DinÃ mic de Keywords**: Implementa un cicle de vida complet per a les paraules clau: descobriment, puntuaciÃ³, recerca activa i seguiment histÃ²ric, tot gestionat a travÃ©s de simples fitxers JSON.
-   **Desacoblat i Estandarditzat**: Utilitza el **Model Context Protocol (MCP)** per comunicar-se amb el servidor de cada plataforma, assegurant una interfÃ­cie estandarditzada, modular i fÃ cilment extensible.
-   **Altament Configurable**: Tota la configuraciÃ³ (claus d'API, selecciÃ³ de models d'IA, rutes de fitxers) es gestiona a travÃ©s d'un Ãºnic fitxer `.env` per facilitar la portabilitat i seguretat.

### ðŸ›ï¸ Arquitectura del Sistema

El sistema estÃ  dissenyat amb una arquitectura modular on un orquestrador central gestiona diversos components especialitzats. La comunicaciÃ³ amb les fonts de dades externes s'estandarditza a travÃ©s de servidors MCP.

```mermaid
graph TD
    subgraph "Motor Principal"
        A[ai_trend_researcher.py] --> B{KeywordManager};
        A --> C{MCPClientManager};
        A --> D{DataProcessor};
        A --> E{ReportManager};
    end

    subgraph "IntegraciÃ³ IA"
        F[AIClientManager] -- Gestiona --> G[OpenAI, Gemini, Groq, ...];
        D -- Utilitza --> F;
    end

    subgraph "Fonts de Dades (via MCP)"
        C -- Connecta a --> H[Servidor YouTube];
        C -- Connecta a --> I[Servidor GitHub];
        C -- Connecta a --> J[Servidor ArXiv];
        C -- Connecta a --> K[Web/HackerNews...];
        C -- Connecta a --> L[Servidor ResearchHub];
    end

    subgraph "Informes de Sortida"
        E -- Genera --> M[Fitxers JSON];
        E -- Genera --> N[PÃ gines a Notion];
        E -- Genera --> O[Registres a Supabase];
    end

    B --> A;
```

-   **Orquestrador (`ai_trend_researcher.py`)**: L'script principal que coordina tot el flux de recerca, des de la cÃ rrega de paraules clau fins a la generaciÃ³ d'informes finals.
-   **ConfiguraciÃ³ (`config_manager.py`)**: Carrega i valida totes les variables d'entorn i configuracions dels servidors.
-   **Connectivitat (`mcp_client_manager.py`)**: Gestiona el cicle de vida (inici, connexiÃ³, trucada, tancament) de tots els servidors MCP per a les plataformes de dades.
-   **FÃ brica d'IA (`ai_client_manager.py`)**: Proporciona una interfÃ­cie unificada per interactuar amb diversos proveÃ¯dors de LLM.
-   **LÃ²gica de Plataformes (`platform_handlers.py`)**: ContÃ© la lÃ²gica especÃ­fica per consultar cada plataforma i estandarditzar la seva resposta.
-   **AnÃ lisi (`data_processor.py`)**: Utilitza LLMs i heurÃ­stiques per extreure noves paraules clau, puntuar-les i generar recomanacions accionables.
-   **Informes (`report_generator.py`)**: Crea els informes finals en tots els formats suportats.
-   **Base de Dades de Keywords (`keyword_manager.py`)**: Administra l'estat de les paraules clau a travÃ©s dels cicles de recerca.

### ðŸ’» Stack TecnolÃ²gic

-   **Backend**: Python 3.9+
-   **ConcurrÃ¨ncia**: `asyncio`
-   **ProveÃ¯dors IA**: OpenAI, Google Gemini, Anthropic Claude, Groq, Ollama
-   **Protocol d'Eines**: Model Context Protocol (MCP)
-   **DependÃ¨ncies**: `python-dotenv`, `aiofiles`, SDKs de proveÃ¯dors (ex. `openai`, `google-generativeai`)
-   **Prerequisits**: Node.js & npm (per executar servidors MCP de la comunitat via `npx`), Rust (opcional, per compilar el servidor `research_hub`)

### ðŸ› ï¸ InstalÂ·laciÃ³ i ConfiguraciÃ³

#### 1. Prerequisits

-   **Python 3.9+**
-   **Node.js i npm** (necessari per a `npx`, que executa els servidors MCP de la comunitat).
-   **Git**
-   (Opcional) **Compilador de Rust**, si vols compilar l'executable de `research_hub` des del codi font. Si no, pots fer servir un binari precompilat.

#### 2. Clonar el Repositori

```bash
git clone https://github.com/el-teu-usuari/el-teu-repositori.git
cd el-teu-repositori
```

#### 3. InstalÂ·lar DependÃ¨ncies de Python

Ã‰s molt recomanable utilitzar un entorn virtual.

```bash
python -m venv venv
# A macOS/Linux:
source venv/bin/activate
# A Windows:
venv\Scripts\activate

pip install -r requirements.txt
```

#### 4. Configurar les Variables d'Entorn

Aquest Ã©s el pas mÃ©s crÃ­tic.

1.  Crea el teu fitxer d'entorn a partir de l'exemple:
    ```bash
    cp .env.example .env
    ```
2.  Obre el fitxer `.env` i omple les teves claus d'API i rutes. NomÃ©s cal omplir les claus per als serveis que vulguis utilitzar.

    ```env
    # .env

    # --- ConfiguraciÃ³ del ProveÃ¯dor d'IA ---
    # Tria'n un: "openai", "gemini", "groq", "anthropic", "ollama"
    AI_PROVIDER="openai"

    # Omple les claus d'API per als proveÃ¯dors que puguis utilitzar.
    OPENAI_API_KEY="sk-..."
    GOOGLE_API_KEY="AIzaSy..."
    GROQ_API_KEY="gsk_..."
    ANTHROPIC_API_KEY="sk-ant-..."
    # No cal clau per a Ollama

    # --- (Opcional) Models EspecÃ­fics d'IA ---
    AI_MODEL_OPENAI="gpt-4o"
    AI_MODEL_GEMINI="gemini-1.5-flash"
    # ... etc. per a altres proveÃ¯dors

    # --- Claus d'API per a Fonts de Dades ---
    YOUTUBE_API_KEY=AIzaSy...
    GITHUB_PERSONAL_ACCESS_TOKEN=ghp_...

    # --- Claus d'API per a Sortides d'Informes ---
    NOTION_API_KEY=secret_...
    NOTION_PARENT_PAGE_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
    SUPABASE_ACCESS_TOKEN=eyJhbGciOi...

    # --- ConfiguraciÃ³ de Rutes del Sistema (IMPORTANT!) ---
    # HAN de ser rutes absolutes.
    RESEARCH_PAPERS_DIR="/ruta/a/la/teva/maquina/research-papers"
    RESEARCH_HUB_EXECUTABLE="/ruta/a/la/teva/maquina/rust-research-mcp"
    ```

#### 5. Configurar la Base de Dades (Supabase)

Si planeges fer servir la integraciÃ³ amb Supabase, assegura't que la teva taula a la base de dades coincideixi amb l'esquema definit a `supabase_schema.sql`. Pots executar aquest script a l'editor SQL del teu projecte de Supabase.

### â–¶ï¸ Ãšs

El projecte tÃ© dos fluxos d'execuciÃ³ principals.

#### 1. Recerca DiÃ ria de TendÃ¨ncies

Aquest Ã©s el flux de treball principal. Executa la recerca a totes les plataformes habilitades fent servir les paraules clau de `keywords/active.json`, analitza les dades, descobreix noves keywords i genera informes.

```bash
python ai_trend_researcher.py
```
L'script registrarÃ  el seu progrÃ©s a la consola. Quan acabi, trobarÃ s:
- Un informe JSON al directori `reports/`.
- Una nova pÃ gina al teu espai de treball de Notion (si estÃ  configurat).
- Un nou registre a la teva taula de Supabase (si estÃ  configurat).

#### 2. ImmersiÃ³ Profunda de Recerca (`research_assistant`)

Aquest script avanÃ§at utilitza el servidor personalitzat `research_hub` per a realitzar recerques acadÃ¨miques en profunditat. Cerca *papers*, descarrega els PDF i genera bibliografies.

1. Assegura't que la ruta `RESEARCH_HUB_EXECUTABLE` al teu `.env` sigui correcta.
2. Afegeix termes de cerca al fitxer `terminos.txt` (un per lÃ­nia).

```bash
python research_assistant.py
```
Els resultats (CSVs, JSONs, fitxers BibTeX i logs) es desaran en un subdirectori amb marca de temps dins de `salidas/` per mantenir cada execuciÃ³ organitzada.

### ðŸ¤ Contribucions

Les contribucions sÃ³n benvingudes! Si tens idees per millorar el projecte, noves plataformes per integrar o trobes algun error, si us plau obre un *issue* o envia un *pull request*.

### ðŸ“„ LlicÃ¨ncia

Aquest projecte estÃ  sota la LlicÃ¨ncia MIT. Consulta el fitxer `LICENSE` per a mÃ©s detalls.



---
File: /report_generator.py
---

# report_generator.py
# -*- coding: utf-8 -*-

import os
import json
import csv
import asyncio
from datetime import datetime
from typing import Dict, List, Any, Optional

# --- Clases Base (sin cambios) ---

class BaseReportGenerator:
    def __init__(self, reports_dir: str):
        self.reports_dir = reports_dir
        os.makedirs(self.reports_dir, exist_ok=True)
    
    def get_timestamp_str(self) -> str:
        return datetime.now().strftime("%Y%m%d_%H%M%S")

# =======================================================================
# BLOQUE DE CÃ“DIGO CORREGIDO
# =======================================================================

class LocalFileReportGenerator(BaseReportGenerator):
    """Genera informes en archivos locales (JSON y CSV)."""
    async def create_local_report(self, report_data: Dict[str, Any]) -> str:
        timestamp = self.get_timestamp_str()
        file_path = os.path.join(self.reports_dir, f"ai_trend_report_{timestamp}.json")
        csv_path = os.path.join(self.reports_dir, f"research_results_{timestamp}.csv")

        try:
            # --- GeneraciÃ³n de JSON (sin cambios) ---
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=4, ensure_ascii=False)
            print(f"ðŸ“„ Informe JSON guardado en: {file_path}")
            
            # --- GeneraciÃ³n de CSV (CORREGIDA) ---
            research_results = report_data.get("research_data", [])
            if research_results:
                # 1. Recolectar TODAS las claves posibles de TODOS los resultados.
                all_keys = set()
                for result in research_results:
                    if isinstance(result, dict):
                        all_keys.update(result.keys())
                
                # 2. Si no hay claves, no hacer nada.
                if not all_keys:
                    return file_path

                # 3. Usar el conjunto de todas las claves como fieldnames.
                with open(csv_path, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=sorted(list(all_keys))) # Ordenar para consistencia
                    writer.writeheader()
                    # El writer ahora conoce la clave 'error' y no fallarÃ¡.
                    writer.writerows(research_results)
                print(f"ðŸ“„ Informe CSV guardado en: {csv_path}")

            return file_path
        except Exception as e:
            print(f"ðŸ”¥ Error al generar el informe local: {e}")
            return ""

# =======================================================================
# FIN DEL BLOQUE CORREGIDO
# =======================================================================

class NotionReportGenerator(BaseReportGenerator):
    """Genera un informe en una pÃ¡gina de Notion."""
    def __init__(self, notion_client: Any, notion_parent_id: str, reports_dir: str):
        super().__init__(reports_dir)
        self.notion_client = notion_client
        self.parent_page_id = notion_parent_id

    async def create_notion_report(self, report_data: Dict[str, Any]):
        if not self.notion_client or not self.parent_page_id:
            return
        
        print("ðŸ“„ Generando informe en Notion...")
        try:
            await asyncio.sleep(1) # Simula la llamada a la API
            print("âœ… Informe de Notion generado (simulado).")
        except Exception as e:
            print(f"ðŸ”¥ Error al generar el informe de Notion: {e}")

class SupabaseReportGenerator(BaseReportGenerator):
    """Guarda los resultados de la investigaciÃ³n en una tabla de Supabase."""
    def __init__(self, supabase_client: Any, reports_dir: str):
        super().__init__(reports_dir)
        self.supabase_client = supabase_client
    
    async def create_supabase_report(self, report_data: Dict[str, Any]):
        if not self.supabase_client:
            return
            
        print("ðŸ’¾ Guardando resultados en Supabase...")
        try:
            await asyncio.sleep(1) # Simula la llamada a la API
            print("âœ… Resultados guardados en Supabase (simulado).")
        except Exception as e:
            print(f"ðŸ”¥ Error al guardar en Supabase: {e}")


class ReportManager:
    """
    Orquesta la generaciÃ³n de mÃºltiples tipos de informes.
    """
    def __init__(
        self,
        reports_dir: str,
        notion_client: Optional[Any] = None,
        notion_parent_id: Optional[str] = None,
        supabase_client: Optional[Any] = None,
    ):
        self.reports_dir = reports_dir
        self.local_reporter = LocalFileReportGenerator(reports_dir)
        self.notion_reporter = None
        self.supabase_reporter = None

        if notion_client and notion_parent_id:
            self.notion_reporter = NotionReportGenerator(
                notion_client=notion_client,
                notion_parent_id=notion_parent_id,
                reports_dir=reports_dir
            )
        
        if supabase_client:
            self.supabase_reporter = SupabaseReportGenerator(
                supabase_client=supabase_client,
                reports_dir=reports_dir
            )

    async def generate_all_reports(
        self,
        research_data: List[Dict],
        new_keywords: List[str],
        summary: Dict,
        recommendations: str,
    ) -> str:
        """
        Genera todos los informes configurados (local, Notion, Supabase) de forma concurrente.
        """
        report_data = {
            "metadata": {
                "report_generated_at": datetime.now().isoformat(),
                "total_results": len(research_data),
            },
            "summary_and_recommendations": {
                "summary": summary,
                "recommendations": recommendations,
            },
            "newly_discovered_keywords": new_keywords,
            "research_data": research_data,
        }

        tasks = []
        
        local_report_task = self.local_reporter.create_local_report(report_data)
        tasks.append(local_report_task)

        if self.supabase_reporter:
            tasks.append(self.supabase_reporter.create_supabase_report(report_data))
        
        if self.notion_reporter:
            tasks.append(self.notion_reporter.create_notion_report(report_data))
            
        if not tasks:
            return ""

        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        local_path = ""
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"ðŸ”¥ OcurriÃ³ un error en una tarea de generaciÃ³n de informes: {result}")
            elif i == 0:
                 local_path = result if result else ""

        return local_path



---
File: /requirement.txt
---

# Biblioteca oficial para interactuar con los modelos de IA de Anthropic (Claude).
anthropic

# Biblioteca para el protocolo MCP (Model Context Protocol), que estandariza la comunicaciÃ³n con herramientas externas.
mcp

# Biblioteca estÃ¡ndar en Python para realizar peticiones HTTP a APIs y servicios web.
requests

# Biblioteca para cargar las variables de entorno definidas en el archivo .env en el script de Python.
python-dotenv

# --- Bibliotecas de IA aÃ±adidas ---

# Biblioteca oficial de Google para interactuar con los modelos de IA Gemini.
google-generativeai

# Biblioteca oficial para interactuar con la API de Groq, que ofrece inferencia rÃ¡pida de LLMs.
groq

# Biblioteca para interactuar con Ollama, que permite ejecutar modelos de IA localmente.
ollama

# Biblioteca oficial para interactuar con los modelos de IA de OpenAI (GPT).
openai



---
File: /requirements.txt
---

# Biblioteca oficial para interactuar con los modelos de IA de Anthropic (Claude).
anthropic

# Biblioteca para el protocolo MCP (Model Context Protocol), que estandariza la comunicaciÃ³n con herramientas externas.
mcp

# Biblioteca estÃ¡ndar en Python para realizar peticiones HTTP a APIs y servicios web.
requests

# Biblioteca para cargar las variables de entorno definidas en el archivo .env en el script de Python.
python-dotenv

# --- Bibliotecas de IA aÃ±adidas ---

# Biblioteca oficial de Google para interactuar con los modelos de IA Gemini.
google-generativeai

# Biblioteca oficial para interactuar con la API de Groq, que ofrece inferencia rÃ¡pida de LLMs.
groq

# Biblioteca para interactuar con Ollama, que permite ejecutar modelos de IA localmente.
ollama

# Biblioteca oficial para interactuar con los modelos de IA de OpenAI (GPT).
openai



---
File: /research_assistant_con_hackers_LLM.py
---

# research_assistant_con_hackers_LLM.py
# ============================================================
# Flujo de investigaciÃ³n con rust-research-mcp (MCP server)
# 1. Lee 'terminos.txt' y construye topics (MAX_TOPICS).
# 2. Busca papers (search_papers) para DOIs y metadatos.
# 3. Descarga PDFs (download_paper) con control de paralelismo.
# 4. Genera bibliografÃ­a (BibTeX) desde los metadatos recolectados.
# 5. Busca discusiones en Hacker News por cada topic y filtra con un LLM.
# ============================================================

from __future__ import annotations
import asyncio
import json
import os
import re
from datetime import datetime
from typing import Any, Iterable, List, Dict, Optional

from dotenv import load_dotenv
import aiofiles
import aiofiles.os

# MÃ³dulos del proyecto
from config_manager import ServerConfig, AppConfig
from mcp_client_manager import MCPClientManager, RemoteMCPClient
from ai_client_manager import AIClientManager

load_dotenv()

# ============================================================
# ðŸ”§ PARÃMETROS CONFIGURABLES (env-first)
# ============================================================

def env_bool(name: str, default: bool) -> bool:
    val = os.getenv(name, str(default)).strip().lower()
    return val in ("1", "true", "t", "yes", "y", "on")

def env_int(name: str, default: int) -> int:
    try:
        v = os.getenv(name)
        return int(v) if v is not None else default
    except ValueError:
        return default

# â€”â€”â€” Control de salidas â€”â€”â€”
SEPARATE_RUNS_IN_SUBFOLDER: bool = env_bool("SEPARATE_RUNS_IN_SUBFOLDER", True)
RUN_TAG: Optional[str] = os.getenv("RUN_TAG")

# â€”â€”â€” BÃºsqueda y filtrado â€”â€”â€”
MAX_TOPICS: int = env_int("MAX_TOPICS", 10)
MAX_RESULTS_PER_TOPIC: int = env_int("MAX_RESULTS_PER_TOPIC", 10)
MAX_HN_RESULTS_PER_TOPIC: int = env_int("MAX_HN_RESULTS_PER_TOPIC", 15)


# â€”â€”â€” Descarga de PDFs â€”â€”â€”
DOWNLOAD_ALL_PAPERS: bool = env_bool("DOWNLOAD_ALL_PAPERS", False)
SELECT_TOP_K: int = env_int("SELECT_TOP_K", 5)
MAX_PARALLEL_DOWNLOADS: int = env_int("MAX_PARALLEL_DOWNLOADS", 4)

# â€”â€”â€” Timeouts/reintentos MCP â€”â€”â€”
MCP_INIT_RETRIES: int = env_int("MCP_INIT_RETRIES", 1)
RESEARCH_HUB_INIT_TIMEOUT: int = env_int("RESEARCH_HUB_INIT_TIMEOUT", 45)
HACKERNEWS_INIT_TIMEOUT: int = env_int("HACKERNEWS_INIT_TIMEOUT", 15)

# ============================================================
# Constantes y Utilidades
# ============================================================
DOI_REGEX = re.compile(r'^10\.\d{4,9}/.+$')

def slugify(s: str) -> str:
    """Convierte un string en un formato seguro para nombres de archivo."""
    s = str(s).lower().strip()
    s = re.sub(r'[\s\W-]+', '-', s)
    s = s.strip('-')
    return s[:75] if len(s) > 75 else s

def now_str() -> str:
    """Devuelve la fecha y hora actual como un string formateado."""
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def extract_text(blobs: Optional[Iterable[Any]]) -> str:
    """Extrae y une el contenido de texto de una respuesta MCP."""
    if not blobs: 
        return ""
    parts: List[str] = []
    for b in blobs:
        if hasattr(b, "text"):
            parts.append(getattr(b, "text", "") or "")
        elif isinstance(b, str):
            parts.append(b)
    return "\n".join(parts).strip()

def parse_text_response_to_papers(raw_text: str, topic: str) -> List[Dict[str, Any]]:
    """Parsea la respuesta de texto del servidor Rust para la bÃºsqueda de papers."""
    papers = []
    if not raw_text:
        return papers

    content = raw_text.split('\n\n', 1)[-1]
    paper_blocks = re.split(r'\n\n(?=\d+\.\s)', content)

    for block in paper_blocks:
        if not block.strip():
            continue
        paper_data = {"topic": topic, "title": None, "doi": None, "source": None, "year": None, "authors": None, "journal": None}
        
        # ExtracciÃ³n de campos con expresiones regulares
        title_match = re.search(r'^\d+\.\s*(.*?)\s*\(Relevance:', block)
        if title_match: paper_data['title'] = title_match.group(1).strip()
        
        doi_match = re.search(r'ðŸ“–\s*DOI:\s*(.*)', block)
        if doi_match:
            doi_str = doi_match.group(1).strip()
            paper_data['doi'] = doi_str if doi_str and " " not in doi_str else None

        source_match = re.search(r'ðŸ”\s*Source:\s*(.*)', block)
        if source_match: paper_data['source'] = source_match.group(1).strip()
            
        year_match = re.search(r'ðŸ“…\s*Year:\s*(\d{4})', block)
        if year_match: paper_data['year'] = int(year_match.group(1).strip())

        authors_match = re.search(r'ðŸ‘¤\s*Authors:\s*(.*)', block)
        if authors_match: paper_data['authors'] = authors_match.group(1).strip()

        journal_match = re.search(r'Journal:\s*(.*)', block)
        if journal_match: paper_data['journal'] = journal_match.group(1).strip()
            
        if paper_data.get('title'):
            paper_data['url'] = f"https://doi.org/{paper_data['doi']}" if paper_data.get('doi') else None
            paper_data['is_valid_doi'] = bool(paper_data['doi'] and DOI_REGEX.match(paper_data['doi']))
            papers.append(paper_data)
            
    return papers

class AdvancedResearchAssistant:
    """Orquesta el flujo de investigaciÃ³n avanzada, gestionando clientes, archivos y lÃ³gica de negocio."""
    
    def __init__(self, topics: List[str]):
        """Inicializa el asistente con los temas de investigaciÃ³n."""
        self.topics = topics
        self.mcp_manager: MCPClientManager = None
        self.ai_client: Optional[AIClientManager] = None
        self.rh_client: Optional[RemoteMCPClient] = None
        self.hn_client: Optional[RemoteMCPClient] = None

        # Directorios de salida (inicializados como None)
        self.salidas_dir = self.csv_dir = self.logs_dir = self.bib_dir = self.json_dir = None
        self.downloads_dir = os.getenv("RESEARCH_PAPERS_DIR", os.path.join(os.getcwd(), "ResearchPapers"))

    async def setup_output_dirs(self):
        """Configura los directorios de salida para esta ejecuciÃ³n."""
        base = "salidas"
        if SEPARATE_RUNS_IN_SUBFOLDER:
            tag = RUN_TAG or datetime.now().strftime("%Y%m%d_%H%M%S")
            base = os.path.join(base, tag)
        
        self.salidas_dir = base
        self.csv_dir = os.path.join(base, "csv")
        self.logs_dir = os.path.join(base, "logs")
        self.bib_dir = os.path.join(base, "bib")
        self.json_dir = os.path.join(base, "json")
        
        for d in (self.salidas_dir, self.csv_dir, self.logs_dir, self.bib_dir, self.json_dir, self.downloads_dir):
            await aiofiles.os.makedirs(d, exist_ok=True)
        print(f"ðŸ“‚ Salidas se guardarÃ¡n en: {self.salidas_dir}")
        print(f"ðŸ“¥ PDFs se guardarÃ¡n en: {self.downloads_dir}")

    async def _save_json(self, name: str, data: Any):
        """Guarda datos en un archivo JSON en el directorio de salida."""
        path = os.path.join(self.json_dir, name)
        async with aiofiles.open(path, "w", encoding="utf-8") as f:
            await f.write(json.dumps(data, ensure_ascii=False, indent=2))
        print(f"ðŸ’¾ JSON guardado: {os.path.basename(path)}")
    
    async def _save_csv(self, name: str, rows: List[Dict[str, Any]]):
        """Guarda una lista de diccionarios en un archivo CSV."""
        if not rows: return
        path = os.path.join(self.csv_dir, name)
        async with aiofiles.open(path, "w", encoding="utf-8-sig", newline="") as f:
            fields = list(rows[0].keys())
            await f.write(",".join(fields) + "\n")
            for row in rows:
                values = [f"\"{str(row.get(k, '')).replace('\"', '\"\"')}\"" for k in fields]
                await f.write(",".join(values) + "\n")
        print(f"ðŸ’¾ CSV guardado: {os.path.basename(path)} ({len(rows)} filas)")

    async def _save_bib(self, name: str, content: str):
        """Guarda contenido en un archivo BibTeX."""
        path = os.path.join(self.bib_dir, name)
        async with aiofiles.open(path, "w", encoding="utf-8") as f:
            await f.write(content)
        print(f"ðŸ“š BibliografÃ­a guardada: {os.path.basename(path)}")

    async def _save_log(self, name: str, content: str):
        """Guarda texto en un archivo de registro."""
        path = os.path.join(self.logs_dir, name)
        async with aiofiles.open(path, "w", encoding="utf-8") as f:
            await f.write(content)
        print(f"ðŸ“œ Log de error guardado: {os.path.basename(path)}")

    async def _connect_with_retries(self, name: str, cfg: Dict[str, Any], timeout: int, retries: int) -> bool:
        """Intenta conectar a un servidor MCP con reintentos."""
        delay = 2.0
        for attempt in range(retries + 1):
            try:
                print(f"[{now_str()}] [MCP] Conectando a '{name}' (intento {attempt+1}/{retries+1}) | timeout={timeout}s")
                await asyncio.wait_for(self.mcp_manager._connect_single_server(name, cfg), timeout=timeout)
                if self.mcp_manager.is_platform_available(name):
                    print(f"  [Ã‰XITO] ConexiÃ³n establecida con '{name}'")
                    return True
            except Exception as e:
                err = f"Timeout en initialize() para '{name}'" if isinstance(e, asyncio.TimeoutError) else str(e)
            print(f"  [FALLO] Error conectando a '{name}': {err}")
            if attempt < retries:
                await asyncio.sleep(delay)
                delay *= 2
        print(f"  [FALLO] ConexiÃ³n definitiva fallida con '{name}'")
        return False

    async def connect_services(self) -> bool:
        """Inicializa y conecta a todos los servicios externos (MCP, IA)."""
        # 1. Conectar servidores MCP
        server_configs = ServerConfig.get_server_configs()
        self.mcp_manager = MCPClientManager(server_configs)
        
        print("\nðŸ”— Conectando a servidores MCPâ€¦")
        rh_cfg = server_configs.get("research_hub", {})
        hn_cfg = server_configs.get("hackernews", {})

        rh_ok, hn_ok = await asyncio.gather(
            self._connect_with_retries("research_hub", rh_cfg, RESEARCH_HUB_INIT_TIMEOUT, MCP_INIT_RETRIES),
            self._connect_with_retries("hackernews", hn_cfg, HACKERNEWS_INIT_TIMEOUT, MCP_INIT_RETRIES),
        )
        self.rh_client = self.mcp_manager.get_client("research_hub") if rh_ok else None
        self.hn_client = self.mcp_manager.get_client("hackernews") if hn_ok else None

        # 2. Inicializar cliente de IA
        try:
            ai_provider = AppConfig.get_ai_provider()
            api_key = AppConfig.get_api_key(ai_provider)
            ai_model = AppConfig.get_ai_model(ai_provider)
            if ai_provider != "ollama" and not api_key:
                print(f"âš ï¸  [OMITIDO] No hay API Key para {ai_provider}. Se omitirÃ¡ el filtrado con LLM.")
            else:
                self.ai_client = AIClientManager(provider=ai_provider, api_key=api_key, model=ai_model)
                print(f"âœ… Cliente IA inicializado: {ai_provider.upper()} (modelo: {ai_model or 'default'})")
        except Exception as e:
            print(f"âš ï¸  [FALLO] No se pudo inicializar el cliente de IA: {e}. Se omitirÃ¡ el filtrado con LLM.")
        
        return self.rh_client is not None or self.hn_client is not None

    async def _search_papers(self) -> List[Dict[str, Any]]:
        """Paso A: Busca papers para cada topic y los devuelve combinados y deduplicados."""
        print("\n--- PASO A: BÃºsqueda de Papers AcadÃ©micos ---")
        if not self.rh_client:
            print("  [OMITIDO] El cliente de Research Hub no estÃ¡ disponible.")
            return []
        
        all_papers: List[Dict[str, Any]] = []
        for topic in self.topics:
            print(f"  -> Buscando topic: '{topic}'...")
            try:
                res = await self.rh_client.call_tool("search_papers", {"query": topic, "limit": MAX_RESULTS_PER_TOPIC})
                papers = parse_text_response_to_papers(extract_text(res), topic)
                all_papers.extend(papers)
                print(f"     Encontrados {len(papers)} resultados.")
            except Exception as e:
                print(f"     [FALLO] Error buscando topic '{topic}': {e}")
        
        # Deduplicado por DOI
        seen_dois = set()
        unique_papers = [p for p in all_papers if p.get("doi") not in seen_dois and not seen_dois.add(p.get("doi"))]
        
        print(f"\nâœ¨ [Ã‰XITO] Total de papers Ãºnicos encontrados: {len(unique_papers)}")
        return unique_papers

    async def _select_and_download_papers(self, papers: List[Dict[str, Any]]):
        """Pasos B y C: Selecciona DOIs para descargar y ejecuta la descarga en paralelo."""
        print("\n--- PASOS B & C: SelecciÃ³n y Descarga de PDFs ---")
        if not self.rh_client:
            print("  [OMITIDO] El cliente de Research Hub no estÃ¡ disponible.")
            return

        # Paso B: SelecciÃ³n
        selected_dois = [p['doi'] for p in papers if p.get('is_valid_doi')]
        if not DOWNLOAD_ALL_PAPERS:
            selected_dois = selected_dois[:SELECT_TOP_K]
            print(f"  -> SelecciÃ³n: TOP {SELECT_TOP_K} papers ({len(selected_dois)} con DOI vÃ¡lido).")
        else:
            print(f"  -> SelecciÃ³n: TODOS los papers ({len(selected_dois)} con DOI vÃ¡lido).")
        
        if not selected_dois:
            print("  [AVISO] No hay DOIs vÃ¡lidos para descargar.")
            return

        await self._save_json("02_dois_seleccionados.json", {"dois": selected_dois})

        # Paso C: Descarga
        print(f"  -> Descargando {len(selected_dois)} papers en paralelo (max {MAX_PARALLEL_DOWNLOADS})...")
        semaphore = asyncio.Semaphore(MAX_PARALLEL_DOWNLOADS)
        doi_map = {p['doi']: p for p in papers if p.get('doi')}

        async def _download_one(doi: str):
            async with semaphore:
                title = doi_map.get(doi, {}).get('title', 'untitled')
                filename = f"{slugify(title)}_{slugify(doi)}.pdf"
                try:
                    res = await self.rh_client.call_tool("download_paper", {"doi": doi, "filename": filename})
                    raw_text = extract_text(res)
                    if "Download successful!" in raw_text or "File already exists" in raw_text:
                        print(f"     âœ“ Descargado (o ya existÃ­a): {doi}")
                        return doi, {"status": "ok", "title": title}
                    print(f"     âœ— Fallo en descarga: {doi}")
                    return doi, {"status": "failed", "reason": raw_text.strip()}
                except Exception as e:
                    print(f"     âœ— Error grave durante la descarga de {doi}: {e}")
                    return doi, {"status": "error", "reason": str(e)}

        tasks = [_download_one(doi) for doi in selected_dois]
        results = await asyncio.gather(*tasks)
        download_manifest = {doi: result for doi, result in results}
        await self._save_json("03_manifiesto_descarga.json", download_manifest)
        print("âœ¨ [Ã‰XITO] Proceso de descarga completado.")

    def _paper_to_bibtex(self, paper: Dict[str, Any]) -> str:
        """Convierte un diccionario de metadatos de un paper a una entrada BibTeX."""
        author_lastname = "unknown"
        if paper.get("authors"):
            try: author_lastname = slugify(paper["authors"].split(',')[0].split(' ')[-1])
            except: pass
        
        year_str = str(paper.get('year', 'nodate'))
        title_slug = slugify(paper.get('title', 'notitle')[:10])
        key = f"{author_lastname}{year_str}{title_slug}"

        entry = f"@article{{{key},\n"
        if paper.get('title'): entry += f"  title     = {{{{{paper['title']}}}}},\n"
        if paper.get('authors'): entry += f"  author    = {{{paper['authors']}}},\n"
        if paper.get('year'): entry += f"  year      = {{{paper['year']}}},\n"
        if paper.get('journal'): entry += f"  journal   = {{{paper['journal']}}},\n"
        if paper.get('doi'): entry += f"  doi       = {{{paper['doi']}}},\n"
        entry += "}"
        return entry
        
    async def _generate_bibliography(self, papers: List[Dict[str, Any]]):
        """Paso D: Genera una bibliografÃ­a completa a partir de los metadatos recolectados."""
        print("\n--- PASO D: GeneraciÃ³n de BibliografÃ­a ---")
        papers_with_doi = [p for p in papers if p.get('is_valid_doi')]
        if not papers_with_doi:
            print("  [OMITIDO] No se encontraron papers con DOI vÃ¡lido para generar bibliografÃ­a.")
            return

        print(f"  -> Usando metadatos de {len(papers_with_doi)} papers.")
        bib_entries = [self._paper_to_bibtex(p) for p in papers_with_doi]
        await self._save_bib("bibliografia_final.bib", "\n\n".join(bib_entries))
        print("âœ¨ [Ã‰XITO] BibliografÃ­a generada.")

    async def _search_hackernews(self) -> Dict[str, Any]:
        """
        Paso E: Busca en Hacker News para cada topic.
        MEJORA: Maneja explÃ­citamente errores de JSON y guarda la respuesta cruda.
        """
        print("\n--- PASO E: BÃºsqueda en Hacker News ---")
        if not self.hn_client:
            print("  [OMITIDO] El cliente de Hacker News no estÃ¡ disponible.")
            return {}

        all_results = {}
        for topic in self.topics:
            print(f"  -> Buscando en HN: '{topic}'...")
            raw_response_text = ""
            try:
                params = {"query": topic, "max_results": MAX_HN_RESULTS_PER_TOPIC}
                response = await self.hn_client.call_tool("getStories", params)
                
                stories = []
                # El problema puede estar aquÃ­: la respuesta puede ser un texto vacÃ­o o un error HTML
                raw_response_text = extract_text(response)
                if not raw_response_text.strip():
                     print(f"     [AVISO] Respuesta vacÃ­a del servidor para '{topic}'.")
                     all_results[topic] = []
                     continue

                # Intentamos decodificar el JSON
                response_json = json.loads(raw_response_text)
                
                # Asumimos que la respuesta es una lista de historias (o un dict con 'hits')
                story_items = response_json if isinstance(response_json, list) else response_json.get('hits', [])

                all_results[topic] = story_items
                print(f"     Encontrados {len(story_items)} resultados.")
            
            except json.JSONDecodeError as e:
                log_filename = f"hackernews_error_response_{slugify(topic)}.log"
                error_msg = f"     [FALLO] El servidor HN no devolviÃ³ un JSON vÃ¡lido para '{topic}'. Error: {e}"
                print(error_msg)
                await self._save_log(log_filename, f"{error_msg}\n\n--- RESPUESTA RECIBIDA ---\n{raw_response_text}")
                all_results[topic] = []
            except Exception as e:
                print(f"     [FALLO] Error inesperado buscando en HN para '{topic}': {e}")
                all_results[topic] = []
        
        print("âœ¨ [Ã‰XITO] BÃºsqueda en Hacker News completada.")
        return all_results

    async def _filter_with_llm(self, hn_results: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """Paso F: Usa un LLM para validar la relevancia de los resultados de Hacker News."""
        print("\n--- PASO F: Filtrado de HN con LLM ---")
        if not self.ai_client:
            print("  [OMITIDO] Cliente de IA no disponible.")
            return hn_results
        
        final_results = {}
        for topic, stories in hn_results.items():
            if not stories:
                final_results[topic] = []
                continue

            print(f"  -> Pidiendo al LLM que valide {len(stories)} historias para: '{topic}'...")
            titles_str = "\n".join([f"{i+1}. {s.get('title', 'N/A')}" for i, s in enumerate(stories)])
            
            prompt = (
                f"Identify which of the following Hacker News titles are relevant to the research topic: '{topic}'.\n\n"
                "A title is relevant if it discusses the topic directly, not if it just uses some of the same words in a different context.\n\n"
                f"Candidate Titles:\n{titles_str}\n\n"
                "Return a JSON object with a single key 'relevant_indices', a list of the numbers of relevant titles. Example: {\"relevant_indices\": [1, 4, 5]}.\n"
                "If none are relevant, return an empty list. Respond ONLY with the JSON object."
            )

            try:
                response_text = await self.ai_client.chat_completion(prompt)
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                response_json = json.loads(json_match.group(0)) if json_match else {}
                relevant_indices = response_json.get("relevant_indices", [])
                
                filtered = [stories[i-1] for i in relevant_indices if 0 < i <= len(stories)]
                final_results[topic] = filtered
                print(f"     LLM identificÃ³ {len(filtered)} historias relevantes.")
            except Exception as e:
                print(f"     [FALLO] Error con el LLM para '{topic}': {e}. Se mantienen resultados sin filtrar.")
                final_results[topic] = stories
        
        print("âœ¨ [Ã‰XITO] Filtrado con LLM completado.")
        return final_results

    async def run(self):
        """Ejecuta el flujo completo de investigaciÃ³n."""
        print(f"\n[{now_str()}] ðŸš€ Iniciando Asistente de InvestigaciÃ³nâ€¦")
        await self.setup_output_dirs()
        
        if not await self.connect_services():
            print("\nâŒ [ERROR] No se pudo conectar a los servicios necesarios. Abortando.")
            return

        try:
            # Flujo de Research Hub
            found_papers = await self._search_papers()
            if found_papers:
                await self._save_json("01_papers_encontrados.json", found_papers)
                await self._save_csv("01_papers_encontrados.csv", found_papers)
                await self._select_and_download_papers(found_papers)
                await self._generate_bibliography(found_papers)
            
            # Flujo de Hacker News
            hackernews_results = await self._search_hackernews()
            await self._save_json("04_hackernews_raw.json", hackernews_results)
            
            final_hn_results = await self._filter_with_llm(hackernews_results)
            await self._save_json("05_hackernews_filtrado_llm.json", final_hn_results)
            
            print(f"\nðŸŽ‰ [{now_str()}] Proceso completado con Ã©xito.")

        finally:
            print("\nðŸ Finalizando y cerrando conexionesâ€¦")
            if self.mcp_manager:
                await self.mcp_manager.close_all_clients()
                print("   [Ã‰XITO] Conexiones MCP cerradas.")

async def main():
    """Punto de entrada principal del script."""
    try:
        async with aiofiles.open("terminos.txt", "r", encoding="utf-8") as f:
            terminos = [t.strip() for t in (await f.read()).splitlines() if t.strip()]
        if not terminos:
            print("âš ï¸  'terminos.txt' estÃ¡ vacÃ­o. No hay nada que procesar.")
            return
    except FileNotFoundError:
        print("âŒ ERROR: No se encontrÃ³ 'terminos.txt'. Por favor, crea el archivo con los temas a investigar.")
        return
    
    topics = terminos[:MAX_TOPICS]
    print(f"âœ… Temas a investigar ({len(topics)}): {topics}")
    
    assistant = AdvancedResearchAssistant(topics)
    await assistant.run()

if __name__ == "__main__":
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())


---
File: /research_assistant.py
---

# research_assistant.py
# ============================================================
# Flujo de investigaciÃ³n con rust-research-mcp (MCP server)
# 1. Busca papers por temas para descubrir DOIs.
# 2. Descarga los papers seleccionados.
# 3. Vuelve a consultar cada DOI para obtener metadatos enriquecidos (autores, etc.).
# 4. Genera una bibliografÃ­a completa con los datos enriquecidos.
# ============================================================

import asyncio
import json
import os
import re
import csv
from datetime import datetime
from typing import Any, Iterable, List, Dict, Optional

from dotenv import load_dotenv
import aiofiles
import aiofiles.os

# MÃ³dulos del proyecto
from config_manager import ServerConfig
from mcp_client_manager import MCPClientManager, RemoteMCPClient

load_dotenv()

# ============================================================
# ðŸ”§ PARÃMETROS CONFIGURABLES
# ============================================================

def env_bool(name: str, default: bool) -> bool:
    val = os.getenv(name, str(default)).strip().lower()
    return val in ("1", "true", "t", "yes", "y", "on")

def env_int(name: str, default: int) -> int:
    val = os.getenv(name)
    return int(val) if val and val.isdigit() else default

# â€”â€”â€” Control de salidas â€”â€”â€”
SEPARATE_RUNS_IN_SUBFOLDER: bool = env_bool("SEPARATE_RUNS_IN_SUBFOLDER", True)
RUN_TAG: Optional[str] = os.getenv("RUN_TAG")

# â€”â€”â€” BÃºsqueda y filtrado â€”â€”â€”
MAX_TOPICS: int = env_int("MAX_TOPICS", 10)
MAX_RESULTS_PER_TOPIC: int = env_int("MAX_RESULTS_PER_TOPIC", 10)

# â€”â€”â€” Descarga de PDFs â€”â€”â€”
DOWNLOAD_ALL_PAPERS: bool = env_bool("DOWNLOAD_ALL_PAPERS", False)
SELECT_TOP_K: int = env_int("SELECT_TOP_K", 5)
MAX_PARALLEL_DOWNLOADS: int = env_int("MAX_PARALLEL_DOWNLOADS", 4)

# ============================================================
# Constantes y Utilidades
# ============================================================
DOI_REGEX = re.compile(r'^10\.\d{4,9}/.+$')

# Directorios de salida globales
SALIDAS_DIR, CSV_DIR, LOGS_DIR, BIB_DIR, JSON_DIR = "", "", "", "", ""
DOWNLOADS_DIR = ""

async def setup_output_dirs() -> None:
    """Configura los directorios de salida y descarga de forma asÃ­ncrona."""
    global SALIDAS_DIR, CSV_DIR, LOGS_DIR, BIB_DIR, JSON_DIR, DOWNLOADS_DIR
    
    DOWNLOADS_DIR = os.getenv("RESEARCH_PAPERS_DIR", os.path.join(os.getcwd(), "ResearchPapers"))
    
    base = "salidas"
    if SEPARATE_RUNS_IN_SUBFOLDER:
        tag = RUN_TAG or datetime.now().strftime("%Y%m%d_%H%M%S")
        base = os.path.join(base, tag)
    
    SALIDAS_DIR = base
    CSV_DIR, LOGS_DIR, BIB_DIR, JSON_DIR = (os.path.join(base, d) for d in ["csv", "logs", "bib", "json"])
    
    for d in (base, CSV_DIR, LOGS_DIR, BIB_DIR, JSON_DIR, DOWNLOADS_DIR):
        await aiofiles.os.makedirs(d, exist_ok=True)

def slugify(s: str) -> str:
    s = str(s).lower().strip()
    s = re.sub(r'[\s\W-]+', '-', s)
    s = s.strip('-')
    return s[:75] if len(s) > 75 else s

async def guardar_csv(nombre: str, rows: List[Dict[str, Any]]) -> None:
    path = os.path.join(CSV_DIR, nombre)
    if not rows: return
    async with aiofiles.open(path, "w", encoding="utf-8-sig", newline="") as f:
        fields = list(rows[0].keys())
        await f.write(",".join(fields) + "\n")
        for row in rows:
            values = [f'"{str(row.get(k, "")).replace("\"", "\"\"")}"' for k in fields]
            await f.write(",".join(values) + "\n")
    print(f"ðŸ’¾ CSV guardado: {path} ({len(rows)} filas)")

async def guardar_json(nombre: str, data: Any) -> None:
    path = os.path.join(JSON_DIR, nombre)
    async with aiofiles.open(path, "w", encoding="utf-8") as f:
        await f.write(json.dumps(data, ensure_ascii=False, indent=2))
    print(f"ðŸ’¾ JSON guardado: {path}")

async def guardar_bib(nombre: str, contenido: str) -> None:
    path = os.path.join(BIB_DIR, nombre)
    async with aiofiles.open(path, "w", encoding="utf-8") as f:
        await f.write(contenido)
    print(f"ðŸ“š BibliografÃ­a guardada: {path}")

def extract_text(blobs: Optional[Iterable[Any]]) -> str:
    parts: List[str] = []
    if not blobs: return ""
    for b in blobs:
        if hasattr(b, "text"): parts.append(getattr(b, "text", "") or "")
        elif isinstance(b, str): parts.append(b)
    return "\n".join(parts).strip()

def parse_text_response_to_papers(raw_text: str, topic: str) -> List[Dict[str, Any]]:
    """Parsea la respuesta de texto formateada del servidor Rust para la bÃºsqueda."""
    papers = []
    content = raw_text.split('\n\n', 1)[-1]
    paper_blocks = re.split(r'\n\n(?=\d+\.\s)', content)

    for block in paper_blocks:
        if not block.strip(): continue
        paper_data = {"topic": topic, "title": None, "doi": None, "source": None, "year": None}
        
        title_match = re.search(r'^\d+\.\s*(.*?)\s*\(Relevance:', block)
        if title_match: paper_data['title'] = title_match.group(1).strip()
        
        doi_match = re.search(r'ðŸ“–\s*DOI:\s*(.*)', block)
        if doi_match:
            doi_str = doi_match.group(1).strip()
            paper_data['doi'] = doi_str if doi_str and " " not in doi_str else None

        source_match = re.search(r'ðŸ”\s*Source:\s*(.*)', block)
        if source_match: paper_data['source'] = source_match.group(1).strip()
            
        year_match = re.search(r'ðŸ“…\s*Year:\s*(\d{4})', block)
        if year_match: paper_data['year'] = int(year_match.group(1).strip())
            
        if paper_data.get('title'):
            paper_data['url'] = f"https://doi.org/{paper_data['doi']}" if paper_data.get('doi') else None
            paper_data['is_valid_doi'] = bool(paper_data['doi'] and DOI_REGEX.match(paper_data['doi']))
            papers.append(paper_data)
            
    return papers

async def step_a_search_papers(rh_client: RemoteMCPClient, topics: List[str]) -> List[Dict[str, Any]]:
    """Busca papers para cada topic y los devuelve combinados y deduplicados."""
    all_papers = []
    for topic in topics:
        print(f"\n--- ðŸ”Ž Buscando topic: '{topic}' ---")
        try:
            res = await rh_client.call_tool("search_papers", {"query": topic, "limit": MAX_RESULTS_PER_TOPIC})
            raw_text_content = extract_text(res)
            papers = parse_text_response_to_papers(raw_text_content, topic)
            all_papers.extend(papers)
            print(f"  -> Encontrados {len(papers)} resultados para '{topic}'.")
        except Exception as e:
            print(f"  âœ— Error buscando topic '{topic}': {e}")
    
    seen_dois = set()
    unique_papers = []
    for p in all_papers:
        doi = p.get("doi")
        if doi and doi not in seen_dois:
            seen_dois.add(doi)
            unique_papers.append(p)
        elif not doi:
             unique_papers.append(p)
    
    print(f"\nâœ¨ Total de papers Ãºnicos encontrados: {len(unique_papers)}")
    return unique_papers

async def step_b_select_papers(papers: List[Dict[str, Any]]) -> List[str]:
    """Filtra y selecciona los DOIs vÃ¡lidos para descargar."""
    print("\n--- ðŸ§  Seleccionando papers para descarga ---")
    
    selected_dois = [p['doi'] for p in papers if p.get('is_valid_doi')]
    
    if DOWNLOAD_ALL_PAPERS:
        print(f"  -> SelecciÃ³n: TODOS ({len(selected_dois)} papers)")
        return selected_dois
    else:
        print(f"  -> SelecciÃ³n: TOP {SELECT_TOP_K} (usando los primeros encontrados)")
        return selected_dois[:SELECT_TOP_K]

async def step_c_download_papers(rh_client: RemoteMCPClient, dois: List[str], papers_metadata: List[Dict]) -> Dict[str, Dict]:
    """Descarga los papers seleccionados en paralelo."""
    if not dois: return {}
    print(f"\n--- ðŸ“¥ Descargando {len(dois)} papers en paralelo (max {MAX_PARALLEL_DOWNLOADS}) ---")
    semaphore = asyncio.Semaphore(MAX_PARALLEL_DOWNLOADS)
    
    doi_map = {p['doi']: p for p in papers_metadata if p.get('doi')}

    async def _download_one(doi: str):
        async with semaphore:
            title = doi_map.get(doi, {}).get('title', 'untitled')
            filename = f"{slugify(title)}_{slugify(doi)}.pdf"
            try:
                res = await rh_client.call_tool("download_paper", {"doi": doi, "filename": filename, "directory": DOWNLOADS_DIR})
                raw_response_text = extract_text(res)
                
                success_match = re.search(r'File:\s*(.*?)\n', raw_response_text)
                
                if ("Download successful!" in raw_response_text or "File already exists" in raw_response_text) and success_match:
                    file_path = success_match.group(1).strip()
                    print(f"  âœ“ Descargado (o ya existÃ­a): {doi}")
                    return doi, {"status": "ok", "path": file_path, "title": title}
                else:
                    print(f"  âœ— Fallo en descarga: {doi}")
                    return doi, {"status": "failed", "reason": raw_response_text.strip()}

            except Exception as e:
                print(f"  âœ— Error grave durante la descarga de {doi}: {e}")
                return doi, {"status": "error", "reason": str(e)}

    tasks = [_download_one(doi) for doi in dois]
    results = await asyncio.gather(*tasks)
    return {doi: result for doi, result in results}

def paper_dict_to_bibtex_entry(paper: Dict[str, Any]) -> str:
    """Convierte un diccionario de paper enriquecido en una entrada BibTeX string."""
    # Crea una clave Ãºnica a partir del primer autor y aÃ±o
    author_lastname = "unknown"
    if paper.get("authors"):
        try:
            author_lastname = slugify(paper["authors"].split(',')[0].split(' ')[-1])
        except: # noqa
            pass # Mantener 'unknown' si el formato del autor es inesperado
    
    year_str = str(paper.get('year', 'nodate'))
    key = f"{author_lastname}{year_str}"

    entry = f"@article{{{key},\n"
    if paper.get('title'):
        entry += f"  title     = {{{{{paper['title']}}}}},\n"
    if paper.get('authors'):
        entry += f"  author    = {{{paper['authors']}}},\n"
    if paper.get('year'):
        entry += f"  year      = {{{paper['year']}}},\n"
    if paper.get('journal'):
        entry += f"  journal   = {{{paper['journal']}}},\n"
    if paper.get('doi'):
        entry += f"  doi       = {{{paper['doi']}}},\n"
    entry += "}"
    return entry

async def step_d_generate_bibliography(rh_client: RemoteMCPClient, papers: List[Dict[str, Any]]) -> str:
    """Enriquece los metadatos de los papers y genera una bibliografÃ­a completa."""
    papers_with_doi = [p for p in papers if p.get('is_valid_doi')]
    if not papers_with_doi:
        return "% No se encontraron papers con DOI vÃ¡lido para generar la bibliografÃ­a."

    print(f"\n--- ðŸ“š Generando BibliografÃ­a ---")
    print(f"  -> Enriqueciendo metadatos para {len(papers_with_doi)} papers...")

    enriched_papers = []
    for paper in papers_with_doi:
        try:
            print(f"     - Obteniendo detalles para DOI: {paper['doi']}")
            # Llama a search_papers con el DOI para obtener metadatos ricos
            res = await rh_client.call_tool("search_papers", {"query": paper['doi'], "limit": 1})
            raw_text = extract_text(res)
            
            # Parsea la respuesta rica (puede tener mÃ¡s campos)
            # Usamos un parser simple aquÃ­, asumiendo un formato similar
            enriched_data = paper.copy() # Empezamos con los datos que ya tenemos
            
            authors_match = re.search(r'ðŸ‘¤\s*Authors:\s*(.*)', raw_text)
            if authors_match:
                enriched_data['authors'] = authors_match.group(1).strip()

            journal_match = re.search(r' L Journal:\s*(.*)', raw_text)
            if journal_match:
                enriched_data['journal'] = journal_match.group(1).strip()
            
            enriched_papers.append(enriched_data)
        except Exception as e:
            print(f"  âœ— Error enriqueciendo {paper.get('doi')}: {e}. Usando datos bÃ¡sicos.")
            enriched_papers.append(paper) # AÃ±adir con datos bÃ¡sicos si falla

    print(f"  -> Creando entradas BibTeX...")
    bib_entries = [paper_dict_to_bibtex_entry(p) for p in enriched_papers]
        
    print("  -> BibliografÃ­a generada con Ã©xito.")
    return "\n\n".join(bib_entries)

def construir_topics_desde_terminos(terminos: List[str], max_topics: int) -> List[str]:
    if not terminos: return ["model context protocol"]
    topics_a_buscar = terminos[:max_topics]
    print(f"âœ… Construidos {len(topics_a_buscar)} topics para la bÃºsqueda: {topics_a_buscar}")
    return topics_a_buscar

async def main():
    """Flujo principal que orquesta la investigaciÃ³n."""
    await setup_output_dirs()
    
    try:
        async with aiofiles.open("terminos.txt", "r", encoding="utf-8") as f:
            contenido = await f.read()
        terminos = [t.strip() for t in contenido.splitlines() if t.strip()]
        if not terminos:
            print("âš ï¸  El archivo 'terminos.txt' estÃ¡ vacÃ­o. No hay nada que procesar.")
            return
    except FileNotFoundError:
        print("âŒ ERROR: El archivo 'terminos.txt' no se encontrÃ³.")
        return

    topics = construir_topics_desde_terminos(terminos, MAX_TOPICS)
    
    print(f"\nIniciando Asistente de InvestigaciÃ³n...")
    print(f"ðŸ“‚ Salidas en: {SALIDAS_DIR}")
    print(f"ðŸ“¥ PDFs se guardarÃ¡n en: {DOWNLOADS_DIR}")
    
    server_configs = ServerConfig.get_server_configs()
    mcp_manager = MCPClientManager(server_configs)
    
    try:
        print("\nðŸ”— Conectando al servidor de Research Hub...")
        await mcp_manager._connect_single_server("research_hub", server_configs["research_hub"])
        rh_client = mcp_manager.get_client("research_hub")
        if not rh_client:
            print("âŒ No se pudo conectar al servidor de Research Hub. Abortando.")
            return
        print("âœ… Conectado.")

        found_papers = await step_a_search_papers(rh_client, topics)
        if not found_papers:
            print("\nâš ï¸ No se encontraron papers en ninguna de las bÃºsquedas. Terminando.")
            return
        
        await guardar_json("00_resultados_completos.json", found_papers)
        await guardar_csv("01_papers_encontrados.csv", found_papers)
        
        selected_dois = await step_b_select_papers(found_papers)
        if not selected_dois:
            print("\nâš ï¸ No se seleccionaron papers con DOI vÃ¡lido para descargar.")
        else:
            await guardar_json("02_dois_seleccionados.json", {"dois": selected_dois})
            download_manifest = await step_c_download_papers(rh_client, selected_dois, found_papers)
            await guardar_json("03_manifiesto_descarga.json", download_manifest)

        bib_content = await step_d_generate_bibliography(rh_client, found_papers)
        await guardar_bib("bibliografia_final.bib", bib_content)
        
        print("\nðŸŽ‰ Proceso completado con Ã©xito.")

    finally:
        print("\nðŸ Finalizando y cerrando conexiones...")
        await mcp_manager.close_all_clients()

if __name__ == "__main__":
    if os.name == 'nt':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())
