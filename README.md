# Motor de Investigaci√≥n de Tendencias de IA

![Python Version](https://img.shields.io/badge/python-3.9%2B-blue)![License](https://img.shields.io/badge/license-MIT-green)![Status](https://img.shields.io/badge/status-activo-brightgreen)

Un sistema automatizado y modular para la investigaci√≥n de tendencias en Inteligencia Artificial. Recopila y procesa datos de m√∫ltiples plataformas (GitHub, YouTube, ArXiv, etc.), utiliza modelos de lenguaje (LLMs) para extraer *insights*, descubrir nuevas palabras clave y genera informes completos en m√∫ltiples formatos (JSON, Notion, Supabase).

## üöÄ Caracter√≠sticas Principales

-   **Investigaci√≥n Multiplataforma**: Recopila datos de YouTube, GitHub, b√∫squedas web, ArXiv, HackerNews y un motor de investigaci√≥n de *papers* personalizado.
-   **Procesamiento con IA**: Utiliza LLMs (OpenAI, Gemini, Groq, Anthropic, Ollama) para tareas avanzadas como:
    -   Extracci√≥n inteligente de nuevas palabras clave.
    -   Generaci√≥n de recomendaciones din√°micas y contextuales.
    -   Traducci√≥n de t√©rminos de b√∫squeda para consultas multiling√ºes.
-   **Arquitectura As√≠ncrona**: Construido con `asyncio` para una alta concurrencia y eficiencia en operaciones de red y manejo de procesos.
-   **Generaci√≥n de Informes**: Crea autom√°ticamente informes detallados en:
    -   **JSON**: Archivos locales para archivo y an√°lisis.
    -   **Notion**: P√°ginas estructuradas y f√°ciles de leer en tu *workspace*.
    -   **Supabase**: Registros en una base de datos PostgreSQL para persistencia a largo plazo.
-   **Gesti√≥n de Keywords**: Mantiene un ciclo de vida para las palabras clave, con un cat√°logo maestro, una lista activa para investigar y un historial de ejecuciones.
-   **Altamente Configurable**: Toda la configuraci√≥n (API keys, selecci√≥n de modelos de IA, rutas de archivos) se gestiona a trav√©s de un archivo `.env` para facilitar la portabilidad y seguridad.
-   **Protocolo Est√°ndar**: Utiliza el **Model Context Protocol (MCP)** para comunicarse con los servidores de cada plataforma, asegurando una interfaz estandarizada y desacoplada.

## üèõÔ∏è Arquitectura del Proyecto

El sistema est√° dise√±ado con una arquitectura modular y desacoplada para facilitar su mantenimiento y extensi√≥n.

-   `ai_trend_researcher.py`: El orquestador principal que ejecuta el flujo de investigaci√≥n diario.
-   `config_manager.py`: Centraliza la carga y validaci√≥n de toda la configuraci√≥n desde el archivo `.env`.
-   `mcp_client_manager.py`: Gestiona el ciclo de vida (conexi√≥n, llamadas, cierre) de los clientes para los servidores MCP de cada plataforma.
-   `platform_handlers.py`: Contiene la l√≥gica espec√≠fica para interactuar con cada plataforma (ej. `YouTubeHandler`, `GitHubHandler`), procesar sus datos y estandarizarlos.
-   `data_processor.py`: Se encarga del an√°lisis de datos, la extracci√≥n de keywords y la generaci√≥n de recomendaciones utilizando tanto heur√≠sticas como LLMs.
-   `ai_client_manager.py`: Act√∫a como una f√°brica para interactuar de forma unificada con diferentes proveedores de modelos de lenguaje (OpenAI, Gemini, etc.).
-   `report_generator.py`: Genera los informes finales en todos los formatos soportados (JSON, Notion, Supabase).
-   `keyword_manager.py`: Administra la base de datos de palabras clave en archivos JSON.
-   `research_assistant.py`: Un script avanzado para realizar inmersiones profundas de investigaci√≥n utilizando el servidor personalizado `research_hub`.

## üõ†Ô∏è Instalaci√≥n y Configuraci√≥n

Sigue estos pasos para poner en marcha el proyecto en tu entorno local.

### 1. Prerrequisitos

-   **Python 3.9+**
-   **Node.js y npm** (necesario para `npx`, que ejecuta los servidores MCP de la comunidad).
-   **Git**
-   (Opcional) **Compilador de Rust**, si deseas compilar el ejecutable de `research_hub` desde el c√≥digo fuente.

### 2. Clonar el Repositorio

```bash
git clone https://github.com/tu_usuario/tu_repositorio.git
cd tu_repositorio
```

### 3. Instalar Dependencias de Python

Crea un entorno virtual (recomendado) e instala las bibliotecas necesarias.

```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 4. Configurar las Variables de Entorno

Este es el paso m√°s importante. El proyecto se controla mediante un archivo `.env`.

1.  Copia el archivo de ejemplo:
    ```bash
    cp .env.example .env
    ```
2.  Edita el archivo `.env` con un editor de texto y rellena todas las claves de API y rutas necesarias.

```env
# .env.example

# --- Claves de API (Obligatorias para las plataformas que uses) ---
YOUTUBE_API_KEY=AIzaSy...
GITHUB_PERSONAL_ACCESS_TOKEN=ghp_...
NOTION_API_KEY=secret_...
SUPABASE_ACCESS_TOKEN=eyJhbGciOi...
SILICONFLOW_API_KEY= # Opcional, para el servidor de ArXiv

# --- Configuraci√≥n de Notion (Obligatoria si se usa Notion) ---
NOTION_PARENT_PAGE_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# --- Configuraci√≥n de Rutas del Sistema (¬°IMPORTANTE!) ---
# Modifica estas rutas para que apunten a directorios en tu m√°quina.
RESEARCH_PAPERS_DIR="/home/user/documentos/research-papers"
RESEARCH_HUB_EXECUTABLE="/home/user/proyectos/rust-research-mcp/target/release/rust-research-mcp"

# --- Configuraci√≥n del Proveedor de IA ---
# Elige uno: "openai", "gemini", "groq", "anthropic", "ollama"
AI_PROVIDER="openai"

# Rellena la clave de API para el proveedor que hayas elegido.
OPENAI_API_KEY="sk-..."
GOOGLE_API_KEY="AIzaSy..."
GROQ_API_KEY="gsk_..."
ANTHROPIC_API_KEY="sk-ant-..."

# --- (Opcional) Modelos Espec√≠ficos de IA ---
# Si se dejan en blanco, se usar√°n los modelos por defecto.
AI_MODEL_OPENAI="gpt-4o"
AI_MODEL_GEMINI="gemini-1.5-flash"
AI_MODEL_GROQ="llama3-70b-8192"
AI_MODEL_ANTHROPIC="claude-3-haiku-20240307"
AI_MODEL_OLLAMA="llama3"
```

### 5. Configurar la Base de Datos (Supabase)

Si planeas usar la integraci√≥n con Supabase, aseg√∫rate de que tu tabla en la base de datos coincida con el esquema definido en `supabase_schema.sql`. Puedes ejecutar ese script en el editor SQL de tu proyecto de Supabase.

## ‚ñ∂Ô∏è Uso

Una vez configurado, puedes ejecutar los dos flujos de trabajo principales.

### Investigaci√≥n Diaria de Tendencias

Este es el flujo principal. Ejecutar√° la investigaci√≥n en todas las plataformas habilitadas, analizar√° los datos y generar√° los informes.

```bash
python ai_trend_researcher.py
```

El script imprimir√° su progreso en la consola. Al finalizar, encontrar√°s el informe JSON en el directorio `reports/` y, si est√° configurado, una nueva p√°gina en Notion y un nuevo registro en tu tabla de Supabase.

### Inmersi√≥n Profunda de Investigaci√≥n

Este script utiliza el servidor `research_hub` para realizar b√∫squedas avanzadas de *papers*, descargarlos, analizarlos y generar bibliograf√≠as.

1.  Aseg√∫rate de que el ejecutable `research_hub` est√© correctamente configurado en tu `.env`.
2.  (Opcional) A√±ade t√©rminos de b√∫squeda al archivo `terminos.txt`.

```bash
python research_assistant.py
```

Los resultados de esta ejecuci√≥n (CSVs, JSONs, archivos BibTeX y logs) se guardar√°n en subdirectorios dentro de `salidas/` para mantener cada ejecuci√≥n organizada.

## ü§ù Contribuciones

Las contribuciones son bienvenidas. Si tienes ideas para mejorar el proyecto, nuevas plataformas para integrar o encuentras alg√∫n error, por favor abre un *issue* o env√≠a un *pull request*.

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Consulta el archivo `LICENSE` para m√°s detalles.